{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#  Encapsulate the lightgbm Let it be in stacking It's called in \n",
    "class LGBregressor(object):\n",
    "    def __init__(self,params):\n",
    "        self.params = params\n",
    "\n",
    "    def fit(self, X, y, w):\n",
    "        y /= 10000000\n",
    "        # self.scaler = StandardScaler().fit(y)\n",
    "        # y = self.scaler.transform(y)\n",
    "        split = int(X.shape[0] * 0.8)\n",
    "        indices = np.random.permutation(X.shape[0])\n",
    "        train_id, test_id = indices[:split], indices[split:]\n",
    "        x_train, y_train, w_train, x_valid, y_valid,  w_valid = X[train_id], y[train_id], w[train_id], X[test_id], y[test_id], w[test_id],\n",
    "        d_train = lgb.Dataset(x_train, y_train, weight=w_train)\n",
    "        d_valid = lgb.Dataset(x_valid, y_valid, weight=w_valid)\n",
    "        partial_bst = lgb.train(self.params, d_train, 10000, valid_sets=d_valid, early_stopping_rounds=50)\n",
    "        num_round = partial_bst.best_iteration\n",
    "        d_all = lgb.Dataset(X, label = y, weight=w)\n",
    "        self.bst = lgb.train(self.params, d_all, num_round)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.bst.predict(X) * 10000000\n",
    "        # return self.scaler.inverse_transform(self.bst.predict(X))\n",
    "\n",
    "#  Encapsulate the xgboost Let it be in stacking It's called in \n",
    "class XGBregressor(object):\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "\n",
    "    def fit(self, X, y, w=None):\n",
    "        if w==None:\n",
    "            w = np.ones(X.shape[0])\n",
    "        split = int(X.shape[0] * 0.8)\n",
    "        indices = np.random.permutation(X.shape[0])\n",
    "        train_id, test_id = indices[:split], indices[split:]\n",
    "        x_train, y_train, w_train, x_valid, y_valid,  w_valid = X[train_id], y[train_id], w[train_id], X[test_id], y[test_id], w[test_id],\n",
    "        d_train = xgb.DMatrix(x_train, label=y_train, weight=w_train)\n",
    "        d_valid = xgb.DMatrix(x_valid, label=y_valid, weight=w_valid)\n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "        partial_bst = xgb.train(self.params, d_train, 10000, early_stopping_rounds=50, evals = watchlist, verbose_eval=100)\n",
    "        num_round = partial_bst.best_iteration\n",
    "        d_all = xgb.DMatrix(X, label = y, weight=w)\n",
    "        self.bst = xgb.train(self.params, d_all, num_round)\n",
    "\n",
    "    def predict(self, X):\n",
    "        test = xgb.DMatrix(X)\n",
    "        return self.bst.predict(test)\n",
    "\n",
    "# This object modified from Wille on https://dnc1994.com/2016/05/rank-10-percent-in-first-kaggle-competition-en/\n",
    "class Ensemble(object):\n",
    "    def __init__(self, n_folds, stacker, base_models):\n",
    "        self.n_folds = n_folds\n",
    "        self.stacker = stacker\n",
    "        self.base_models = base_models\n",
    "\n",
    "    def fit_predict(self, trainDf, testDf):\n",
    "        X = trainDf.drop(['price_doc', 'w'], 1).values\n",
    "        y = trainDf['price_doc'].values\n",
    "        w = trainDf['w'].values\n",
    "        T = testDf.values\n",
    "\n",
    "        X_fillna = trainDf.drop(['price_doc', 'w'], 1).fillna(-999).values\n",
    "        T_fillna = testDf.fillna(-999).values\n",
    "\n",
    "        folds = list(KFold(len(y), n_folds=self.n_folds, shuffle=True))\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        S_test = np.zeros((T.shape[0], len(self.base_models)))\n",
    "        for i, clf in enumerate(self.base_models):\n",
    "            print('Training base model ' + str(i+1) + '...')\n",
    "            S_test_i = np.zeros((T.shape[0], len(folds)))\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                print('Training round ' + str(j+1) + '...')\n",
    "                if clf not in [xgb1,lgb1]: # sklearn models cannot handle missing values.\n",
    "                    X = X_fillna\n",
    "                    T = T_fillna\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                w_train = w[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "                # w_holdout = w[test_idx]\n",
    "                # y_holdout = y[test_idx]\n",
    "                clf.fit(X_train, y_train, w_train)\n",
    "                y_pred = clf.predict(X_holdout)\n",
    "                S_train[test_idx, i] = y_pred\n",
    "                S_test_i[:, j] = clf.predict(T)\n",
    "            S_test[:, i] = S_test_i.mean(1)\n",
    "        self.S_train, self.S_test, self.y = S_train, S_test, y  # for diagnosis purpose\n",
    "        self.corr = pd.concat([pd.DataFrame(S_train),trainDf['price_doc']],1).corr() # correlation of predictions by different models.\n",
    "        # cv_stack = ShuffleSplit(n_splits=6, test_size=0.2)\n",
    "        # score_stacking = cross_val_score(self.stacker, S_train, y, cv=cv_stack, n_jobs=1, scoring='neg_mean_squared_error')\n",
    "        # print(np.sqrt(-score_stacking.mean())) # CV result of stacking\n",
    "        self.stacker.fit(S_train, y)\n",
    "        y_pred = self.stacker.predict(S_test)\n",
    "        return y_pred\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trainDf = pd.read_csv('train_featured.csv')\n",
    "    testDf = pd.read_csv('test_featured.csv')\n",
    "\n",
    "    params1 = {'eta':0.05, 'max_depth':5, 'subsample':0.8, 'colsample_bytree':0.8, 'min_child_weight':1,\n",
    "              'gamma':0, 'silent':1, 'objective':'reg:linear', 'eval_metric':'rmse'}\n",
    "    xgb1 = XGBregressor(params1)\n",
    "    params2 = {'booster':'gblinear', 'alpha':0,# for gblinear, delete this line if change back to gbtree\n",
    "               'eta':0.1, 'max_depth':2, 'subsample':1, 'colsample_bytree':1, 'min_child_weight':1,\n",
    "              'gamma':0, 'silent':1, 'objective':'reg:linear', 'eval_metric':'rmse'}\n",
    "    xgb2 = XGBregressor(params2)\n",
    "    RF = RandomForestRegressor(n_estimators=500, max_features=0.2)\n",
    "    ETR = ExtraTreesRegressor(n_estimators=500, max_features=0.3, max_depth=None)\n",
    "    Ada = AdaBoostRegressor(DecisionTreeRegressor(max_depth=15),n_estimators=200)\n",
    "    GBR = GradientBoostingRegressor(n_estimators=200,max_depth=5,max_features=0.5)\n",
    "    LR =LinearRegression()\n",
    "\n",
    "    params_lgb = {'objective':'regression','metric':'rmse',\n",
    "              'learning_rate':0.05,'max_depth':-1,'sub_feature':0.7,'sub_row':1,\n",
    "              'num_leaves':15,'min_data':30,'max_bin':20,\n",
    "              'bagging_fraction':0.9,'bagging_freq':40,'verbosity':0}\n",
    "    lgb1 = LGBregressor(params_lgb)\n",
    "\n",
    "    E = Ensemble(5, xgb2, [xgb1,lgb1,RF,ETR,Ada,GBR])\n",
    "    prediction = E.fit_predict(trainDf, testDf)\n",
    "    output = pd.read_csv('test.csv')\n",
    "    output = output[['id']]\n",
    "    output['price_doc'] = prediction\n",
    "    output.to_csv(r'Ensemble\\Submission_Stack.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
