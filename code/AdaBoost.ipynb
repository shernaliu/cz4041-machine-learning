{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "94d0a9cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kk43419t4dzT",
    "outputId": "2bf00b6f-d7f6-4a48-a78e-05bdb6b9d7d0"
   },
   "outputs": [],
   "source": [
    "# Google Drive mount for colab\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "# After mounting, /drive/MyDrive/ should appear on the left in Files tab\n",
    "# Go to your own Google Drive, create a /cz4041/ folder, and upload the zip and csv files there\n",
    "# It should appear in the files tab under /drive/MyDrive/cz4041/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d498c82",
   "metadata": {
    "id": "1b36853f-29b3-48d1-a364-19db616a4801"
   },
   "source": [
    "# Data Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2041f5e",
   "metadata": {
    "id": "aea06625-2d66-4ff1-92cc-6063222bb130"
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import pprint\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from IPython.display import display\n",
    "\n",
    "# pandas display options\n",
    "pd.set_option('display.max_columns', 150)\n",
    "pd.set_option('display.max_rows', 300)\n",
    "pd.set_option('float_format', '{:,.4f}'.format) # All float will be displayed in 4 d.p. with comma to separate thousands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c30a67fa",
   "metadata": {
    "id": "c9a97915-f943-48ad-9cda-30118a703fd0"
   },
   "outputs": [],
   "source": [
    "# datasets paths\n",
    "path = \"../data\"\n",
    "#path = \"/content/drive/MyDrive/cz4041/\" # path to Google Drive, for colab\n",
    "macro = os.path.join(path, \"macro.csv\")\n",
    "train = os.path.join(path, \"train.zip\")\n",
    "test = os.path.join(path,  \"test.zip\")\n",
    "\n",
    "# place all datasets paths in a datasets dict\n",
    "datasets = {}\n",
    "datasets['macro'] = macro\n",
    "datasets['train'] = train\n",
    "datasets['test'] = test\n",
    "\n",
    "# load dataframes into dfs dict\n",
    "dfs = {}\n",
    "for dataset_name, path in datasets.items():\n",
    "    df = pd.read_csv(path)\n",
    "    dfs[dataset_name] = df\n",
    "\n",
    "# assign to own df variables when you want to use them individually\n",
    "df_macro = dfs['macro']\n",
    "df_train = dfs['train']\n",
    "df_test = dfs['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547f69dd",
   "metadata": {
    "id": "c0c7eca0-bddb-4322-967b-837c1694db7d",
    "tags": []
   },
   "source": [
    "## Overview of datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e307c952",
   "metadata": {
    "id": "329adc68-c5ea-4656-a7dc-26dccbff516d"
   },
   "source": [
    "**Dataset size & Number of distinct datatypes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "677b7880",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "393d7704-805e-49e9-afd5-c17434b341ea",
    "outputId": "9029b006-6c55-4c4c-ad01-7d3ac56956af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Dataset size - macro: (2484, 100) ======\n",
      "Number of distinct datatypes: \n",
      "<class 'numpy.dtype[float64]'>    94\n",
      "<class 'numpy.dtype[object_]'>     4\n",
      "<class 'numpy.dtype[int64]'>       2\n",
      "dtype: int64\n",
      "====== Dataset size - train: (30471, 292) ======\n",
      "Number of distinct datatypes: \n",
      "<class 'numpy.dtype[int64]'>      157\n",
      "<class 'numpy.dtype[float64]'>    119\n",
      "<class 'numpy.dtype[object_]'>     16\n",
      "dtype: int64\n",
      "====== Dataset size - test: (7662, 291) ======\n",
      "Number of distinct datatypes: \n",
      "<class 'numpy.dtype[int64]'>      159\n",
      "<class 'numpy.dtype[float64]'>    116\n",
      "<class 'numpy.dtype[object_]'>     16\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, df in dfs.items():\n",
    "    print(\"====== Dataset size - {}: {} ======\".format(dataset_name , df.shape))\n",
    "    print(\"Number of distinct datatypes: \\n{}\".format(df.dtypes.map(type).value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc78108",
   "metadata": {},
   "source": [
    "**Prepare dataset for datacleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f108d2c",
   "metadata": {
    "id": "0d607c8a-6fa0-4aa6-9fb5-06b317a94dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38133, 291)\n"
     ]
    }
   ],
   "source": [
    "# Copy train price out to facilitate train & test split later\n",
    "trainPrice = dfs[\"train\"][[\"id\", \"price_doc\"]].copy()\n",
    "\n",
    "# Concat train dataset (minus price_doc) and test dataset for data cleaning\n",
    "trainNoPrice = dfs[\"train\"].drop(\"price_doc\", axis = 1)\n",
    "\n",
    "mergeData = pd.concat([trainNoPrice, dfs[\"test\"]])\n",
    "print(mergeData.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d67862",
   "metadata": {},
   "source": [
    "## Cleaning of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579e3492",
   "metadata": {},
   "source": [
    "**Since we are predicting price, we will want to drop columns that have low correlation value to 'price_doc'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d647e386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_year                     0.0022\n",
      "kitch_sq                       0.0287\n",
      "school_quota                   0.0140\n",
      "culture_objects_top_25_raion   0.0443\n",
      "full_all                       0.0253\n",
      "male_f                         0.0264\n",
      "female_f                       0.0243\n",
      "16_29_all                      0.0223\n",
      "16_29_male                     0.0231\n",
      "16_29_female                   0.0216\n",
      "build_count_block              0.0315\n",
      "build_count_wood               0.0425\n",
      "build_count_frame              0.0303\n",
      "build_count_panel              0.0201\n",
      "build_count_foam               0.0107\n",
      "build_count_slag               0.0240\n",
      "build_count_mix                0.0330\n",
      "build_count_1921-1945          0.0203\n",
      "build_count_1971-1995          0.0097\n",
      "build_count_after_1995         0.0259\n",
      "cemetery_km                    0.0249\n",
      "ID_railroad_station_walk       0.0218\n",
      "water_km                       0.0266\n",
      "mkad_km                        0.0206\n",
      "big_market_km                  0.0483\n",
      "prom_part_500                  0.0090\n",
      "trc_sqm_500                    0.0004\n",
      "cafe_sum_500_min_price_avg     0.0364\n",
      "cafe_sum_500_max_price_avg     0.0379\n",
      "cafe_avg_price_500             0.0374\n",
      "cafe_count_500_na_price        0.0492\n",
      "big_church_count_500           0.0262\n",
      "church_count_500               0.0147\n",
      "mosque_count_500               0.0185\n",
      "market_count_500               0.0404\n",
      "trc_sqm_1000                   0.0416\n",
      "cafe_count_1000_price_4000     0.0363\n",
      "prom_part_3000                 0.0227\n",
      "cafe_sum_3000_min_price_avg    0.0051\n",
      "cafe_sum_3000_max_price_avg    0.0022\n",
      "cafe_avg_price_3000            0.0033\n",
      "cafe_sum_5000_min_price_avg    0.0322\n",
      "cafe_sum_5000_max_price_avg    0.0333\n",
      "cafe_avg_price_5000            0.0329\n",
      "Name: price_doc, dtype: float64 , 44\n"
     ]
    }
   ],
   "source": [
    "# get the correlation to 'price_doc' in train dataset\n",
    "trainCorr = abs(dfs[\"train\"].corr()[\"price_doc\"])\n",
    "\n",
    "# pick out features that have less than 5% correlation to be dropped\n",
    "threshold = trainCorr <= 0.05\n",
    "lowCorrFeats = trainCorr[threshold]\n",
    "print(lowCorrFeats, \",\", len(lowCorrFeats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c220399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38133, 247)\n"
     ]
    }
   ],
   "source": [
    "# drop\n",
    "mergeData.drop(list(lowCorrFeats.index), axis = 1, inplace = True)\n",
    "print(mergeData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff66e04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38133, 241)\n"
     ]
    }
   ],
   "source": [
    "# Drop columns with 'ID' in name as they do not provide much value\n",
    "\n",
    "IDfeats = [feat for feat in mergeData.columns if \"ID\" in feat]\n",
    "mergeData.drop(IDfeats, axis = 1, inplace = True)\n",
    "print(mergeData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc8faa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace data in the following columns\n",
    "mergeData.state.replace({33:3},inplace=True)\n",
    "mergeData[\"material\"].replace(to_replace = 3, value = 1, inplace = True)\n",
    "mergeData[\"full_sq\"].replace(to_replace = 0, value = np.nan, inplace = True)\n",
    "mergeData[\"max_floor\"].replace(to_replace = 0, value = np.nan, inplace = True)\n",
    "mergeData[\"num_room\"].replace(to_replace = 0, value = np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d230eca",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b05b6d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38133, 251)\n"
     ]
    }
   ],
   "source": [
    "# create 'year' and 'year_month' features from 'timestamp'\n",
    "mergeData[\"year\"] = mergeData[\"timestamp\"].apply(lambda x: int(x[0:4]))\n",
    "mergeData[\"year_month\"] = mergeData[\"timestamp\"].apply(lambda x: x[0:7])\\\n",
    "\n",
    "# create 'living_area_ratio', 'non_living_area' and 'non_living_area_ratio' from 'life_sq' and 'full_sq'\n",
    "mergeData[\"living_area_ratio\"] = mergeData[\"life_sq\"] / mergeData[\"full_sq\"]\n",
    "mergeData[\"non_living_area\"] = mergeData[\"full_sq\"] - mergeData[\"life_sq\"]\n",
    "mergeData[\"non_living_area_ratio\"] = mergeData[\"non_living_area\"] / mergeData[\"full_sq\"]\n",
    "\n",
    "# create 'room_area_avg' from 'life_sq' and 'num_room'\n",
    "mergeData[\"room_area_avg\"] = mergeData[\"life_sq\"] / mergeData[\"num_room\"]\n",
    "\n",
    "# create 'relative_floor' from 'floor' and 'max_floor'\n",
    "mergeData[\"relative_floor\"] = mergeData[\"floor\"] / mergeData[\"max_floor\"]\n",
    "\n",
    "# create 'sub_area_building_height_avg' from 'sub_area' and 'max_floor'\n",
    "sub_area_building_avg = mergeData.groupby('sub_area').agg({'max_floor':np.mean}).reset_index().rename(columns={'max_floor':'sub_area_building_height_avg'})\n",
    "mergeData = pd.merge(mergeData, sub_area_building_avg, on = ['sub_area'], how = 'left')\n",
    "\n",
    "# create 'sub_area_kremlin_dist_avg' from 'sub_area' and 'kremlin_km'\n",
    "kremlin_dist = mergeData.groupby('sub_area').agg({'kremlin_km':np.nanmean}).reset_index().rename(columns={'kremlin_km':'sub_area_kremlin_dist_avg'})\n",
    "mergeData = pd.merge(mergeData, kremlin_dist, on = ['sub_area'], how = 'left')\n",
    "\n",
    "# create 'sales_year_month' from 'year_month'\n",
    "sales_year_month = mergeData.groupby('year_month').size().reset_index().rename(columns={0:'sales_year_month'})\n",
    "mergeData = pd.merge(mergeData, sales_year_month, on = ['year_month'], how = 'left')\n",
    "\n",
    "print(mergeData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5427b0",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5918d097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hospital_beds_raion                      17859\n",
      "room_area_avg                            14897\n",
      "state                                    14253\n",
      "max_floor                                10355\n",
      "relative_floor                           10355\n",
      "num_room                                  9586\n",
      "material                                  9572\n",
      "preschool_quota                           8284\n",
      "cafe_sum_1000_max_price_avg               7746\n",
      "cafe_sum_1000_min_price_avg               7746\n",
      "cafe_avg_price_1000                       7746\n",
      "living_area_ratio                         7562\n",
      "non_living_area_ratio                     7562\n",
      "non_living_area                           7562\n",
      "life_sq                                   7559\n",
      "build_count_brick                         6209\n",
      "raion_build_count_with_builddate_info     6209\n",
      "build_count_before_1920                   6209\n",
      "build_count_monolith                      6209\n",
      "build_count_1946-1970                     6209\n",
      "raion_build_count_with_material_info      6209\n",
      "cafe_sum_1500_min_price_avg               5020\n",
      "cafe_sum_1500_max_price_avg               5020\n",
      "cafe_avg_price_1500                       5020\n",
      "cafe_avg_price_2000                       2149\n",
      "cafe_sum_2000_max_price_avg               2149\n",
      "cafe_sum_2000_min_price_avg               2149\n",
      "prom_part_5000                             270\n",
      "floor                                      167\n",
      "metro_min_walk                              59\n",
      "metro_km_walk                               59\n",
      "railroad_station_walk_min                   59\n",
      "railroad_station_walk_km                    59\n",
      "product_type                                33\n",
      "green_part_2000                             19\n",
      "full_sq                                      3\n",
      "dtype: int64\n",
      "\r\n",
      "Missing Values Count: 36\n"
     ]
    }
   ],
   "source": [
    "# find out columns that have missing values\n",
    "missing_vals = ((mergeData.isna().sum()))\n",
    "missing_vals.sort_values(ascending = False, inplace = True)\n",
    "print(missing_vals[missing_vals > 0])\n",
    "print(\"\\r\\nMissing Values Count: \" + str(len(missing_vals[missing_vals > 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4adf2dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hospital_beds_raion                     0.4683\n",
      "room_area_avg                           0.3907\n",
      "state                                   0.3738\n",
      "max_floor                               0.2715\n",
      "relative_floor                          0.2715\n",
      "num_room                                0.2514\n",
      "material                                0.2510\n",
      "preschool_quota                         0.2172\n",
      "cafe_sum_1000_max_price_avg             0.2031\n",
      "cafe_sum_1000_min_price_avg             0.2031\n",
      "cafe_avg_price_1000                     0.2031\n",
      "living_area_ratio                       0.1983\n",
      "non_living_area_ratio                   0.1983\n",
      "non_living_area                         0.1983\n",
      "life_sq                                 0.1982\n",
      "build_count_brick                       0.1628\n",
      "raion_build_count_with_builddate_info   0.1628\n",
      "build_count_before_1920                 0.1628\n",
      "build_count_monolith                    0.1628\n",
      "build_count_1946-1970                   0.1628\n",
      "raion_build_count_with_material_info    0.1628\n",
      "cafe_sum_1500_min_price_avg             0.1316\n",
      "cafe_sum_1500_max_price_avg             0.1316\n",
      "cafe_avg_price_1500                     0.1316\n",
      "cafe_avg_price_2000                     0.0564\n",
      "cafe_sum_2000_max_price_avg             0.0564\n",
      "cafe_sum_2000_min_price_avg             0.0564\n",
      "prom_part_5000                          0.0071\n",
      "floor                                   0.0044\n",
      "metro_min_walk                          0.0015\n",
      "metro_km_walk                           0.0015\n",
      "railroad_station_walk_min               0.0015\n",
      "railroad_station_walk_km                0.0015\n",
      "product_type                            0.0009\n",
      "green_part_2000                         0.0005\n",
      "full_sq                                 0.0001\n",
      "dtype: float64\n",
      "\r\n",
      "Missing Values Count: 36\n"
     ]
    }
   ],
   "source": [
    "# get the missing values in percentage\n",
    "missing_vals_pct = ((mergeData.isna().sum()) / len(mergeData))\n",
    "missing_vals_pct.sort_values(ascending = False, inplace = True)\n",
    "print(missing_vals_pct[missing_vals_pct > 0])\n",
    "print(\"\\r\\nMissing Values Count: \" + str(len(missing_vals_pct[missing_vals_pct > 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e539fbd",
   "metadata": {},
   "source": [
    "**Median Imputation for missing values lesser or equal to 30%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c8b675c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['max_floor', 'relative_floor', 'num_room', 'material',\n",
      "       'preschool_quota', 'cafe_sum_1000_max_price_avg',\n",
      "       'cafe_sum_1000_min_price_avg', 'cafe_avg_price_1000',\n",
      "       'living_area_ratio', 'non_living_area_ratio', 'non_living_area',\n",
      "       'life_sq', 'build_count_brick', 'raion_build_count_with_builddate_info',\n",
      "       'build_count_before_1920', 'build_count_monolith',\n",
      "       'build_count_1946-1970', 'raion_build_count_with_material_info',\n",
      "       'cafe_sum_1500_min_price_avg', 'cafe_sum_1500_max_price_avg',\n",
      "       'cafe_avg_price_1500', 'cafe_avg_price_2000',\n",
      "       'cafe_sum_2000_max_price_avg', 'cafe_sum_2000_min_price_avg',\n",
      "       'prom_part_5000', 'floor', 'metro_min_walk', 'metro_km_walk',\n",
      "       'railroad_station_walk_min', 'railroad_station_walk_km', 'product_type',\n",
      "       'green_part_2000', 'full_sq'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(missing_vals_pct[(missing_vals_pct > 0) & (missing_vals_pct <= 0.3000)].index)\n",
    "\n",
    "for feat in missing_vals_pct[(missing_vals_pct > 0) & (missing_vals_pct <= 0.3000)].index:\n",
    "    try:\n",
    "        mergeData[feat].fillna(mergeData[feat].median(), inplace = True)\n",
    "    except:\n",
    "        mergeData[feat].fillna(mergeData[feat].mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e59e0480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hospital_beds_raion   0.4683\n",
      "room_area_avg         0.3907\n",
      "state                 0.3738\n",
      "dtype: float64\n",
      "\r\n",
      "Missing Values Count: 3\n"
     ]
    }
   ],
   "source": [
    "# check remaining missing values in percentage\n",
    "missing_vals_pct = ((mergeData.isna().sum()) / len(mergeData))\n",
    "missing_vals_pct.sort_values(ascending = False, inplace = True)\n",
    "print(missing_vals_pct[missing_vals_pct > 0])\n",
    "print(\"\\r\\nMissing Values Count: \" + str(len(missing_vals_pct[missing_vals_pct > 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21942b98",
   "metadata": {},
   "source": [
    "**KNN Imputer for remaining missing values greater than 30%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23eee080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1a6b2683a543f7b4c17345f1841340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sklearn KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "\n",
    "missingCol = list(missing_vals_pct[missing_vals_pct > 0].index)\n",
    "\n",
    "for i in tqdm_notebook(missingCol):\n",
    "    mergeData[i] = imputer.fit_transform(mergeData[[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a4acfbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: float64)\n",
      "\r\n",
      "Missing Values Count: 0\n",
      "\r\n",
      "(38133, 251)\n"
     ]
    }
   ],
   "source": [
    "# check for any remaining missing values\n",
    "missing_vals_pct = ((mergeData.isna().sum()) / len(mergeData))\n",
    "missing_vals_pct.sort_values(ascending = False, inplace = True)\n",
    "print(missing_vals_pct[missing_vals_pct > 0])\n",
    "print(\"\\r\\nMissing Values Count: \" + str(len(missing_vals_pct[missing_vals_pct > 0])))\n",
    "print(\"\\r\\n\" + str(mergeData.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a91a053",
   "metadata": {},
   "source": [
    "## Prepare Train and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2591c7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 'trainPrice' dataset from earlier to split 'mergeData' back into train and test datasets\n",
    "xTrain = mergeData[mergeData[\"id\"].isin(trainPrice[\"id\"])]\n",
    "xTrain = pd.merge(xTrain, trainPrice, on = [\"id\"], how = \"inner\")\n",
    "\n",
    "xTest = mergeData[~mergeData[\"id\"].isin(trainPrice[\"id\"])]\n",
    "\n",
    "yTrain = xTrain[\"price_doc\"].apply(lambda j: np.log1p(j))\n",
    "xTrain.drop(columns = [\"id\", \"timestamp\", \"price_doc\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5bc5c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and cross-validation\n",
    "x_tr, x_cv, y_tr, y_cv = train_test_split(xTrain, yTrain, test_size=0.15,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "07071280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wehweh\\AppData\\Local\\Temp/ipykernel_11448/1337914271.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_cv[cat] = x_cv[cat].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
      "C:\\Users\\wehweh\\AppData\\Local\\Temp/ipykernel_11448/1337914271.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_tr[cat] = le.transform(x_tr[cat])\n",
      "C:\\Users\\wehweh\\AppData\\Local\\Temp/ipykernel_11448/1337914271.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_cv[cat] = le.transform(x_cv[cat])\n",
      "C:\\Users\\wehweh\\AppData\\Local\\Temp/ipykernel_11448/1337914271.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_tr[num] = (x_tr[num] - min)/(max-min)\n",
      "C:\\Users\\wehweh\\AppData\\Local\\Temp/ipykernel_11448/1337914271.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_cv[num] = (x_cv[num] - min)/(max-min)\n"
     ]
    }
   ],
   "source": [
    "# process categorical ('object') and numerical (non 'object') type data using label encoder\n",
    "\n",
    "categoricals = xTrain.select_dtypes(include = [\"object\"]).copy()\n",
    "numericals = xTrain.select_dtypes(exclude = [\"object\"])\n",
    "\n",
    "# categoricals = xTrain.dtypes[(xTrain.dtypes == object)].copy()\n",
    "# numericals = xTrain.dtypes[(xTrain.dtypes != object)].copy()\n",
    "\n",
    "# categoricals\n",
    "for cat in categoricals:\n",
    "  le = preprocessing.LabelEncoder()\n",
    "  le.fit(x_tr[cat])\n",
    "\n",
    "  x_cv[cat] = x_cv[cat].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
    "  le.classes_ = np.append(le.classes_, '<unknown>')\n",
    "\n",
    "  x_tr[cat] = le.transform(x_tr[cat])\n",
    "  x_cv[cat] = le.transform(x_cv[cat])\n",
    "\n",
    "# numericals\n",
    "for num in numericals:\n",
    "  min = x_tr[num].min()\n",
    "  max = x_tr[num].max()\n",
    "  x_tr[num] = (x_tr[num] - min)/(max-min)\n",
    "  x_cv[num] = (x_cv[num] - min)/(max-min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5de73124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x_tr.isnull().values.any())\n",
    "print(x_cv.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85215eb2",
   "metadata": {},
   "source": [
    "## AdaBoost (Adaptive Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c344f5b9",
   "metadata": {},
   "source": [
    "**Tune 'n_estimators' parameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d0a7db5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 6 is smaller than n_iter=15. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=AdaBoostRegressor(), n_iter=15, n_jobs=-1,\n",
       "                   param_distributions={'n_estimators': [50, 100, 150, 200, 250,\n",
       "                                                         300]},\n",
       "                   verbose=10)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adbModel = AdaBoostRegressor()\n",
    "\n",
    "# define 'n_estimators' distributions to use for tuning\n",
    "paramDist = { \"n_estimators\": [50, 100, 150, 200, 250, 300] }\n",
    "\n",
    "# use RandomizedSearchCV to determine best value for 'n_estimators'\n",
    "randomSearchModel = RandomizedSearchCV(adbModel, param_distributions = paramDist, verbose = 10, n_jobs = -1, cv = 3, n_iter = 15)\n",
    "randomSearchModel.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8219309b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best 'n_estimators' value: \n",
      "50\n",
      "\r\n",
      "Best Score: \n",
      "-0.489144277905456\n"
     ]
    }
   ],
   "source": [
    "print(\"Best 'n_estimators' value: \")\n",
    "print(randomSearchModel.best_params_[\"n_estimators\"])\n",
    "print(\"\\r\\nBest Score: \")\n",
    "print(randomSearchModel.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6592b13d",
   "metadata": {},
   "source": [
    "**Use best 'n_estimators' value in model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8b9c3d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostRegressor()"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tune Model\n",
    "adbModel = AdaBoostRegressor(n_estimators = randomSearchModel.best_params_[\"n_estimators\"])\n",
    "# fit model\n",
    "adbModel.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8de1ae5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Train\n",
      "MSE : 0.5457\n",
      "RMSE: 0.7387\n"
     ]
    }
   ],
   "source": [
    "# predict on train data\n",
    "yPred = adbModel.predict(x_tr)\n",
    "mse = mean_squared_error(y_tr, yPred)\n",
    "print(\"Predict Train\")\n",
    "print(\"MSE : %.4f\" % mse)\n",
    "print(\"RMSE: %.4f\" % sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ffcb4a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Cross Validation\n",
      "MSE : 0.5419\n",
      "RMSE: 0.7362\n"
     ]
    }
   ],
   "source": [
    "# predict on cross validation data\n",
    "yPred = adbModel.predict(x_cv)\n",
    "mse = mean_squared_error(y_cv, yPred)\n",
    "print(\"Predict Cross Validation\")\n",
    "print(\"MSE : %.4f\" % mse)\n",
    "print(\"RMSE: %.4f\" % sqrt(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69486961",
   "metadata": {},
   "source": [
    "### Predict on test.csv test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "41f8bcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n",
      "C:\\Users\\wehweh\\AppData\\Local\\Temp/ipykernel_11448/629287186.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xTest[c] = xTest[c].map(lambda s: \"<unknown>\" if s not in le.classes_ else s)\n",
      "C:\\Users\\wehweh\\AppData\\Local\\Temp/ipykernel_11448/629287186.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xTest[c] = le.transform(xTest[c])\n",
      "C:\\Users\\wehweh\\AppData\\Local\\Temp/ipykernel_11448/629287186.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xTest[n] = (xTest[n] - min) / (max - min)\n"
     ]
    }
   ],
   "source": [
    "# do the same prep: process.. run pred\n",
    "testId = xTest[\"id\"]\n",
    "xTest.drop([\"id\", \"timestamp\"], axis = 1, inplace = True)\n",
    "\n",
    "# process datatype\n",
    "categoricals = xTrain.select_dtypes(include = [\"object\"]).copy()\n",
    "numericals = xTrain.select_dtypes(exclude = [\"object\"])\n",
    "\n",
    "for c in categoricals:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(xTrain[c])\n",
    "\n",
    "    xTest[c] = xTest[c].map(lambda s: \"<unknown>\" if s not in le.classes_ else s)\n",
    "    le.classes_ = np.append(le.classes_, \"<unknown>\")\n",
    "\n",
    "    xTest[c] = le.transform(xTest[c])\n",
    "\n",
    "for n in numericals:\n",
    "    min = xTrain[n].min()\n",
    "    max = xTrain[n].max()\n",
    "\n",
    "    xTest[n] = (xTest[n] - min) / (max - min)\n",
    "\n",
    "# predict using adb model\n",
    "testPred = adbModel.predict(xTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddba68fa",
   "metadata": {},
   "source": [
    "**Output prediction to csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b0361cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wehweh\\AppData\\Local\\Temp/ipykernel_11448/120303509.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xTest[\"price_doc\"] = np.expm1(testPred)\n",
      "C:\\Users\\wehweh\\AppData\\Local\\Temp/ipykernel_11448/120303509.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xTest[\"id\"] = testId\n"
     ]
    }
   ],
   "source": [
    "# populate price_doc column with predicted prices\n",
    "xTest[\"price_doc\"] = np.expm1(testPred)\n",
    "xTest[\"id\"] = testId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0fb04165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check output dir\n",
    "if not os.path.exists(\"./output_models\"):\n",
    "    os.mkdir(\"./output_models\")\n",
    "\n",
    "# write id and predicted price_doc columns to csv\n",
    "xTest[[\"id\", \"price_doc\"]].to_csv(\"./output_models/output_adaboost.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "1d649c4a-48b1-4ddf-9c6c-61154fc3eb69",
    "3fc7bc7b-a329-4d0a-8378-ecd214a94a5c",
    "f43233fd-d92c-45c3-a983-6760d17f8af2",
    "3db5652b-0e4c-4483-9ae4-c12cfc02cad9",
    "eacd3859-1644-49e7-b823-ecaa1a1e1309",
    "9fd74c3a-7d03-4ef1-88f5-512543ea2743",
    "ad7d8de9-ce0e-49ff-9182-d43884be8de9",
    "76333052-72ea-49d9-9bae-dfc14cb16a97",
    "b2c02a57-93c2-48a1-811f-a0a098007051",
    "cb448266-8c1d-43f4-a546-cd1da680cc5d",
    "e7611d91-b2bb-463f-afec-d61387c96aa9",
    "f6467dc8-edbf-42f4-a5da-2b943280d570",
    "a249ef17-e4c0-4dc6-9c37-d99bddfe4397",
    "dc7fd1ae-6b64-477d-a07d-2c39691ee4c9",
    "dfcbad98-798b-4726-9945-65f0621e9a6c",
    "13a68d3a-924b-418f-aa40-7e2d781d7cd4",
    "12015471-eacf-408c-823a-99a86a89a3f5",
    "81987137-ad1d-4c38-b4fd-5afa0d179f45",
    "5a556eaa-ce64-42cd-bc3b-012dd80e6abb"
   ],
   "name": "ExploratoryDataAnalysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
