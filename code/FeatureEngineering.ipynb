{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d59556",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kk43419t4dzT",
    "outputId": "2bf00b6f-d7f6-4a48-a78e-05bdb6b9d7d0"
   },
   "outputs": [],
   "source": [
    "# Google Drive mount for colab\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "# After mounting, /drive/MyDrive/ should appear on the left in Files tab\n",
    "# Go to your own Google Drive, create a /cz4041/ folder, and upload the zip and csv files there\n",
    "# It should appear in the files tab under /drive/MyDrive/cz4041/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92339ead",
   "metadata": {
    "id": "1b36853f-29b3-48d1-a364-19db616a4801"
   },
   "source": [
    "# Data Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca005703",
   "metadata": {
    "id": "aea06625-2d66-4ff1-92cc-6063222bb130"
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import pprint\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostRegressor, RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, ShuffleSplit, KFold,TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import pdb\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "\n",
    "# pandas display options\n",
    "pd.set_option('display.max_columns', 150)\n",
    "pd.set_option('display.max_rows', 300)\n",
    "pd.set_option('float_format', '{:,.4f}'.format) # All float will be displayed in 4 d.p. with comma to separate thousands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38d7cd4a",
   "metadata": {
    "id": "c9a97915-f943-48ad-9cda-30118a703fd0"
   },
   "outputs": [],
   "source": [
    "# datasets paths\n",
    "path = \"../data\"\n",
    "#path = \"/content/drive/MyDrive/cz4041/\" # path to Google Drive, for colab\n",
    "macro = os.path.join(path, \"macro.csv\")\n",
    "train = os.path.join(path, \"train.zip\")\n",
    "test = os.path.join(path,  \"test.zip\")\n",
    "\n",
    "# place all datasets paths in a datasets dict\n",
    "datasets = {}\n",
    "datasets['macro'] = macro\n",
    "datasets['train'] = train\n",
    "datasets['test'] = test\n",
    "\n",
    "# load dataframes into dfs dict\n",
    "dfs = {}\n",
    "for dataset_name, path in datasets.items():\n",
    "    df = pd.read_csv(path)\n",
    "    dfs[dataset_name] = df\n",
    "\n",
    "# assign to own df variables when you want to use them individually\n",
    "df_macro = dfs['macro']\n",
    "df_train = dfs['train']\n",
    "df_test = dfs['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d50406",
   "metadata": {
    "id": "c0c7eca0-bddb-4322-967b-837c1694db7d",
    "tags": []
   },
   "source": [
    "## Overview of datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220c4bfb",
   "metadata": {
    "id": "329adc68-c5ea-4656-a7dc-26dccbff516d"
   },
   "source": [
    "**Dataset size & Number of distinct datatypes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "790b9c00",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "393d7704-805e-49e9-afd5-c17434b341ea",
    "outputId": "9029b006-6c55-4c4c-ad01-7d3ac56956af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Dataset size - macro: (2484, 100) ======\n",
      "Number of distinct datatypes: \n",
      "<class 'numpy.dtype[float64]'>    94\n",
      "<class 'numpy.dtype[object_]'>     4\n",
      "<class 'numpy.dtype[int64]'>       2\n",
      "dtype: int64\n",
      "====== Dataset size - train: (30471, 292) ======\n",
      "Number of distinct datatypes: \n",
      "<class 'numpy.dtype[int64]'>      157\n",
      "<class 'numpy.dtype[float64]'>    119\n",
      "<class 'numpy.dtype[object_]'>     16\n",
      "dtype: int64\n",
      "====== Dataset size - test: (7662, 291) ======\n",
      "Number of distinct datatypes: \n",
      "<class 'numpy.dtype[int64]'>      159\n",
      "<class 'numpy.dtype[float64]'>    116\n",
      "<class 'numpy.dtype[object_]'>     16\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, df in dfs.items():\n",
    "    print(\"====== Dataset size - {}: {} ======\".format(dataset_name , df.shape))\n",
    "    print(\"Number of distinct datatypes: \\n{}\".format(df.dtypes.map(type).value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f67e97",
   "metadata": {},
   "source": [
    "**Prepare dataset for datacleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbf8dbc9",
   "metadata": {
    "id": "0d607c8a-6fa0-4aa6-9fb5-06b317a94dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38133, 291)\n"
     ]
    }
   ],
   "source": [
    "# Copy train price out to facilitate train & test split later\n",
    "trainPrice = dfs[\"train\"][[\"id\", \"price_doc\"]].copy()\n",
    "\n",
    "# Concat train dataset (minus price_doc) and test dataset for data cleaning\n",
    "trainNoPrice = dfs[\"train\"].drop(\"price_doc\", axis = 1)\n",
    "\n",
    "mergeData = pd.concat([trainNoPrice, dfs[\"test\"]])\n",
    "print(mergeData.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ee068",
   "metadata": {},
   "source": [
    "## Cleaning of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a85601",
   "metadata": {},
   "source": [
    "**Since we are predicting price, we will want to drop columns that have low correlation value to 'price_doc'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbb7e3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_year                     0.0022\n",
      "kitch_sq                       0.0287\n",
      "school_quota                   0.0140\n",
      "culture_objects_top_25_raion   0.0443\n",
      "full_all                       0.0253\n",
      "male_f                         0.0264\n",
      "female_f                       0.0243\n",
      "16_29_all                      0.0223\n",
      "16_29_male                     0.0231\n",
      "16_29_female                   0.0216\n",
      "build_count_block              0.0315\n",
      "build_count_wood               0.0425\n",
      "build_count_frame              0.0303\n",
      "build_count_panel              0.0201\n",
      "build_count_foam               0.0107\n",
      "build_count_slag               0.0240\n",
      "build_count_mix                0.0330\n",
      "build_count_1921-1945          0.0203\n",
      "build_count_1971-1995          0.0097\n",
      "build_count_after_1995         0.0259\n",
      "cemetery_km                    0.0249\n",
      "ID_railroad_station_walk       0.0218\n",
      "water_km                       0.0266\n",
      "mkad_km                        0.0206\n",
      "big_market_km                  0.0483\n",
      "prom_part_500                  0.0090\n",
      "trc_sqm_500                    0.0004\n",
      "cafe_sum_500_min_price_avg     0.0364\n",
      "cafe_sum_500_max_price_avg     0.0379\n",
      "cafe_avg_price_500             0.0374\n",
      "cafe_count_500_na_price        0.0492\n",
      "big_church_count_500           0.0262\n",
      "church_count_500               0.0147\n",
      "mosque_count_500               0.0185\n",
      "market_count_500               0.0404\n",
      "trc_sqm_1000                   0.0416\n",
      "cafe_count_1000_price_4000     0.0363\n",
      "prom_part_3000                 0.0227\n",
      "cafe_sum_3000_min_price_avg    0.0051\n",
      "cafe_sum_3000_max_price_avg    0.0022\n",
      "cafe_avg_price_3000            0.0033\n",
      "cafe_sum_5000_min_price_avg    0.0322\n",
      "cafe_sum_5000_max_price_avg    0.0333\n",
      "cafe_avg_price_5000            0.0329\n",
      "Name: price_doc, dtype: float64 , 44\n"
     ]
    }
   ],
   "source": [
    "# get the correlation to 'price_doc' in train dataset\n",
    "trainCorr = abs(dfs[\"train\"].corr()[\"price_doc\"])\n",
    "\n",
    "# pick out features that have less than 5% correlation to be dropped\n",
    "threshold = trainCorr <= 0.05\n",
    "lowCorrFeats = trainCorr[threshold]\n",
    "print(lowCorrFeats, \",\", len(lowCorrFeats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5937487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38133, 247)\n"
     ]
    }
   ],
   "source": [
    "# drop\n",
    "mergeData.drop(list(lowCorrFeats.index), axis = 1, inplace = True)\n",
    "print(mergeData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7435590f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38133, 241)\n"
     ]
    }
   ],
   "source": [
    "# Drop columns with 'ID' in name as they do not provide much value\n",
    "\n",
    "IDfeats = [feat for feat in mergeData.columns if \"ID\" in feat]\n",
    "mergeData.drop(IDfeats, axis = 1, inplace = True)\n",
    "print(mergeData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a361d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace data in the following columns\n",
    "mergeData.state.replace({33:3},inplace=True)\n",
    "mergeData[\"material\"].replace(to_replace = 3, value = 1, inplace = True)\n",
    "mergeData[\"full_sq\"].replace(to_replace = 0, value = np.nan, inplace = True)\n",
    "mergeData[\"max_floor\"].replace(to_replace = 0, value = np.nan, inplace = True)\n",
    "mergeData[\"num_room\"].replace(to_replace = 0, value = np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5060939",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33541f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38133, 251)\n"
     ]
    }
   ],
   "source": [
    "# create 'year' and 'year_month' features from 'timestamp'\n",
    "mergeData[\"year\"] = mergeData[\"timestamp\"].apply(lambda x: int(x[0:4]))\n",
    "mergeData[\"year_month\"] = mergeData[\"timestamp\"].apply(lambda x: x[0:7])\\\n",
    "\n",
    "# create 'living_area_ratio', 'non_living_area' and 'non_living_area_ratio' from 'life_sq' and 'full_sq'\n",
    "mergeData[\"living_area_ratio\"] = mergeData[\"life_sq\"] / mergeData[\"full_sq\"]\n",
    "mergeData[\"non_living_area\"] = mergeData[\"full_sq\"] - mergeData[\"life_sq\"]\n",
    "mergeData[\"non_living_area_ratio\"] = mergeData[\"non_living_area\"] / mergeData[\"full_sq\"]\n",
    "\n",
    "# create 'room_area_avg' from 'life_sq' and 'num_room'\n",
    "mergeData[\"room_area_avg\"] = mergeData[\"life_sq\"] / mergeData[\"num_room\"]\n",
    "\n",
    "# create 'relative_floor' from 'floor' and 'max_floor'\n",
    "mergeData[\"relative_floor\"] = mergeData[\"floor\"] / mergeData[\"max_floor\"]\n",
    "\n",
    "# create 'sub_area_building_height_avg' from 'sub_area' and 'max_floor'\n",
    "sub_area_building_avg = mergeData.groupby('sub_area').agg({'max_floor':np.mean}).reset_index().rename(columns={'max_floor':'sub_area_building_height_avg'})\n",
    "mergeData = pd.merge(mergeData, sub_area_building_avg, on = ['sub_area'], how = 'left')\n",
    "\n",
    "# create 'sub_area_kremlin_dist_avg' from 'sub_area' and 'kremlin_km'\n",
    "kremlin_dist = mergeData.groupby('sub_area').agg({'kremlin_km':np.nanmean}).reset_index().rename(columns={'kremlin_km':'sub_area_kremlin_dist_avg'})\n",
    "mergeData = pd.merge(mergeData, kremlin_dist, on = ['sub_area'], how = 'left')\n",
    "\n",
    "# create 'sales_year_month' from 'year_month'\n",
    "sales_year_month = mergeData.groupby('year_month').size().reset_index().rename(columns={0:'sales_year_month'})\n",
    "mergeData = pd.merge(mergeData, sales_year_month, on = ['year_month'], how = 'left')\n",
    "\n",
    "print(mergeData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9d14c7",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f9222e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hospital_beds_raion                      17859\n",
      "room_area_avg                            14897\n",
      "state                                    14253\n",
      "max_floor                                10355\n",
      "relative_floor                           10355\n",
      "num_room                                  9586\n",
      "material                                  9572\n",
      "preschool_quota                           8284\n",
      "cafe_sum_1000_max_price_avg               7746\n",
      "cafe_sum_1000_min_price_avg               7746\n",
      "cafe_avg_price_1000                       7746\n",
      "living_area_ratio                         7562\n",
      "non_living_area_ratio                     7562\n",
      "non_living_area                           7562\n",
      "life_sq                                   7559\n",
      "build_count_brick                         6209\n",
      "raion_build_count_with_builddate_info     6209\n",
      "build_count_before_1920                   6209\n",
      "build_count_monolith                      6209\n",
      "build_count_1946-1970                     6209\n",
      "raion_build_count_with_material_info      6209\n",
      "cafe_sum_1500_min_price_avg               5020\n",
      "cafe_sum_1500_max_price_avg               5020\n",
      "cafe_avg_price_1500                       5020\n",
      "cafe_avg_price_2000                       2149\n",
      "cafe_sum_2000_max_price_avg               2149\n",
      "cafe_sum_2000_min_price_avg               2149\n",
      "prom_part_5000                             270\n",
      "floor                                      167\n",
      "metro_min_walk                              59\n",
      "metro_km_walk                               59\n",
      "railroad_station_walk_min                   59\n",
      "railroad_station_walk_km                    59\n",
      "product_type                                33\n",
      "green_part_2000                             19\n",
      "full_sq                                      3\n",
      "dtype: int64\n",
      "\r\n",
      "Missing Values Count: 36\n"
     ]
    }
   ],
   "source": [
    "# find out columns that have missing values\n",
    "missing_vals = ((mergeData.isna().sum()))\n",
    "missing_vals.sort_values(ascending = False, inplace = True)\n",
    "print(missing_vals[missing_vals > 0])\n",
    "print(\"\\r\\nMissing Values Count: \" + str(len(missing_vals[missing_vals > 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6d3790f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hospital_beds_raion                     0.4683\n",
      "room_area_avg                           0.3907\n",
      "state                                   0.3738\n",
      "max_floor                               0.2715\n",
      "relative_floor                          0.2715\n",
      "num_room                                0.2514\n",
      "material                                0.2510\n",
      "preschool_quota                         0.2172\n",
      "cafe_sum_1000_max_price_avg             0.2031\n",
      "cafe_sum_1000_min_price_avg             0.2031\n",
      "cafe_avg_price_1000                     0.2031\n",
      "living_area_ratio                       0.1983\n",
      "non_living_area_ratio                   0.1983\n",
      "non_living_area                         0.1983\n",
      "life_sq                                 0.1982\n",
      "build_count_brick                       0.1628\n",
      "raion_build_count_with_builddate_info   0.1628\n",
      "build_count_before_1920                 0.1628\n",
      "build_count_monolith                    0.1628\n",
      "build_count_1946-1970                   0.1628\n",
      "raion_build_count_with_material_info    0.1628\n",
      "cafe_sum_1500_min_price_avg             0.1316\n",
      "cafe_sum_1500_max_price_avg             0.1316\n",
      "cafe_avg_price_1500                     0.1316\n",
      "cafe_avg_price_2000                     0.0564\n",
      "cafe_sum_2000_max_price_avg             0.0564\n",
      "cafe_sum_2000_min_price_avg             0.0564\n",
      "prom_part_5000                          0.0071\n",
      "floor                                   0.0044\n",
      "metro_min_walk                          0.0015\n",
      "metro_km_walk                           0.0015\n",
      "railroad_station_walk_min               0.0015\n",
      "railroad_station_walk_km                0.0015\n",
      "product_type                            0.0009\n",
      "green_part_2000                         0.0005\n",
      "full_sq                                 0.0001\n",
      "dtype: float64\n",
      "\r\n",
      "Missing Values Count: 36\n"
     ]
    }
   ],
   "source": [
    "# get the missing values in percentage\n",
    "missing_vals_pct = ((mergeData.isna().sum()) / len(mergeData))\n",
    "missing_vals_pct.sort_values(ascending = False, inplace = True)\n",
    "print(missing_vals_pct[missing_vals_pct > 0])\n",
    "print(\"\\r\\nMissing Values Count: \" + str(len(missing_vals_pct[missing_vals_pct > 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1868cbf6",
   "metadata": {},
   "source": [
    "**Median Imputation for missing values lesser or equal to 30%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aca0ce83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['max_floor', 'relative_floor', 'num_room', 'material',\n",
      "       'preschool_quota', 'cafe_sum_1000_max_price_avg',\n",
      "       'cafe_sum_1000_min_price_avg', 'cafe_avg_price_1000',\n",
      "       'living_area_ratio', 'non_living_area_ratio', 'non_living_area',\n",
      "       'life_sq', 'build_count_brick', 'raion_build_count_with_builddate_info',\n",
      "       'build_count_before_1920', 'build_count_monolith',\n",
      "       'build_count_1946-1970', 'raion_build_count_with_material_info',\n",
      "       'cafe_sum_1500_min_price_avg', 'cafe_sum_1500_max_price_avg',\n",
      "       'cafe_avg_price_1500', 'cafe_avg_price_2000',\n",
      "       'cafe_sum_2000_max_price_avg', 'cafe_sum_2000_min_price_avg',\n",
      "       'prom_part_5000', 'floor', 'metro_min_walk', 'metro_km_walk',\n",
      "       'railroad_station_walk_min', 'railroad_station_walk_km', 'product_type',\n",
      "       'green_part_2000', 'full_sq'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(missing_vals_pct[(missing_vals_pct > 0) & (missing_vals_pct <= 0.3000)].index)\n",
    "\n",
    "for feat in missing_vals_pct[(missing_vals_pct > 0) & (missing_vals_pct <= 0.3000)].index:\n",
    "    try:\n",
    "        mergeData[feat].fillna(mergeData[feat].median(), inplace = True)\n",
    "    except:\n",
    "        mergeData[feat].fillna(mergeData[feat].mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "067c3c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hospital_beds_raion   0.4683\n",
      "room_area_avg         0.3907\n",
      "state                 0.3738\n",
      "dtype: float64\n",
      "\r\n",
      "Missing Values Count: 3\n"
     ]
    }
   ],
   "source": [
    "# check remaining missing values in percentage\n",
    "missing_vals_pct = ((mergeData.isna().sum()) / len(mergeData))\n",
    "missing_vals_pct.sort_values(ascending = False, inplace = True)\n",
    "print(missing_vals_pct[missing_vals_pct > 0])\n",
    "print(\"\\r\\nMissing Values Count: \" + str(len(missing_vals_pct[missing_vals_pct > 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c65474",
   "metadata": {},
   "source": [
    "**KNN Imputer for remaining missing values greater than 30%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93e49669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be14e7118d1c466cbbc74e99b5602033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sklearn KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "\n",
    "missingCol = list(missing_vals_pct[missing_vals_pct > 0].index)\n",
    "\n",
    "for i in tqdm_notebook(missingCol):\n",
    "    mergeData[i] = imputer.fit_transform(mergeData[[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17aafe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: float64)\n",
      "\r\n",
      "Missing Values Count: 0\n",
      "\r\n",
      "(38133, 251)\n"
     ]
    }
   ],
   "source": [
    "# check for any remaining missing values\n",
    "missing_vals_pct = ((mergeData.isna().sum()) / len(mergeData))\n",
    "missing_vals_pct.sort_values(ascending = False, inplace = True)\n",
    "print(missing_vals_pct[missing_vals_pct > 0])\n",
    "print(\"\\r\\nMissing Values Count: \" + str(len(missing_vals_pct[missing_vals_pct > 0])))\n",
    "print(\"\\r\\n\" + str(mergeData.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacba8ba",
   "metadata": {},
   "source": [
    "## Prepare Train and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "152c7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 'trainPrice' dataset from earlier to split 'mergeData' back into train and test datasets\n",
    "xTrain = mergeData[mergeData[\"id\"].isin(trainPrice[\"id\"])]\n",
    "xTrain = pd.merge(xTrain, trainPrice, on = [\"id\"], how = \"inner\")\n",
    "\n",
    "xTest = mergeData[~mergeData[\"id\"].isin(trainPrice[\"id\"])]\n",
    "\n",
    "yTrain = xTrain[\"price_doc\"].apply(lambda j: np.log1p(j))\n",
    "train_id = xTrain['id']\n",
    "xTrain.drop(columns = [\"id\", \"timestamp\", \"price_doc\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3267ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and cross-validation\n",
    "x_tr, x_cv, y_tr, y_cv = train_test_split(xTrain, yTrain, test_size=0.15,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2580f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wehweh\\AppData\\Local\\Temp/ipykernel_8956/3056538181.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_cv[cat] = x_cv[cat].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
      "C:\\Users\\wehweh\\AppData\\Local\\Temp/ipykernel_8956/3056538181.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_tr[cat] = le.transform(x_tr[cat])\n",
      "C:\\Users\\wehweh\\AppData\\Local\\Temp/ipykernel_8956/3056538181.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_cv[cat] = le.transform(x_cv[cat])\n",
      "C:\\Users\\wehweh\\AppData\\Local\\Temp/ipykernel_8956/3056538181.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_tr[num] = (x_tr[num] - min)/(max-min)\n",
      "C:\\Users\\wehweh\\AppData\\Local\\Temp/ipykernel_8956/3056538181.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_cv[num] = (x_cv[num] - min)/(max-min)\n"
     ]
    }
   ],
   "source": [
    "# process categorical ('object') and numerical (non 'object') type data using label encoder\n",
    "\n",
    "categoricals = xTrain.select_dtypes(include = [\"object\"]).copy()\n",
    "numericals = xTrain.select_dtypes(exclude = [\"object\"])\n",
    "\n",
    "# categoricals\n",
    "for cat in categoricals:\n",
    "  le = preprocessing.LabelEncoder()\n",
    "  le.fit(x_tr[cat])\n",
    "\n",
    "  x_cv[cat] = x_cv[cat].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
    "  le.classes_ = np.append(le.classes_, '<unknown>')\n",
    "\n",
    "  x_tr[cat] = le.transform(x_tr[cat])\n",
    "  x_cv[cat] = le.transform(x_cv[cat])\n",
    "\n",
    "# numericals\n",
    "for num in numericals:\n",
    "  min = x_tr[num].min()\n",
    "  max = x_tr[num].max()\n",
    "  x_tr[num] = (x_tr[num] - min)/(max-min)\n",
    "  x_cv[num] = (x_cv[num] - min)/(max-min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25d1100f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# check any null values\n",
    "print(x_tr.isnull().values.any())\n",
    "print(x_cv.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327dd4c5",
   "metadata": {},
   "source": [
    "# Testing the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1738f156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result array\n",
    "resArr = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184d73b0",
   "metadata": {},
   "source": [
    "## 1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d5ddec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (after feature selection): (25900, 30)\n",
      "Test (after feature selection) : (4571, 30)\n"
     ]
    }
   ],
   "source": [
    "# Select best features\n",
    "sel = SelectFromModel(RandomForestRegressor(n_jobs = -1, max_depth = 10))\n",
    "sel.fit(x_tr, y_tr)\n",
    "\n",
    "trainFiltered = sel.transform(x_tr)\n",
    "testFiltered = sel.transform(x_cv)\n",
    "\n",
    "print(\"Train (after feature selection): \" + str(trainFiltered.shape))\n",
    "print(\"Test (after feature selection) : \" + str(testFiltered.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3407b7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 15 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=2, estimator=RandomForestRegressor(), n_iter=15,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'max_depth': [10, 11, 12, 13, 14, 15],\n",
       "                                        'n_estimators': [50, 100, 150, 200, 250,\n",
       "                                                         300]},\n",
       "                   verbose=10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define parameter distributions to use for tuning\n",
    "paramDist = {\n",
    "    'n_estimators': [50, 100, 150, 200, 250, 300],\n",
    "    'max_depth': [10, 11, 12, 13, 14, 15]\n",
    "}\n",
    "\n",
    "# use RandomizedSearchCV to determine best values for parameters\n",
    "randomSearchModel = RandomizedSearchCV(RandomForestRegressor(), param_distributions = paramDist, verbose = 10, n_jobs = -1, cv = 2, n_iter = 15)\n",
    "randomSearchModel.fit(trainFiltered, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e467659d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameter values: \n",
      "{'n_estimators': 250, 'max_depth': 12}\n",
      "\r\n",
      "Best Score: \n",
      "0.3905168753704433\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameter values: \")\n",
    "print(randomSearchModel.best_params_)\n",
    "print(\"\\r\\nBest Score: \")\n",
    "print(randomSearchModel.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dcd4191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=12, n_estimators=250, n_jobs=-1,\n",
       "                      random_state=42)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tune the Model with the best hyperparameters\n",
    "rfrModel = RandomForestRegressor(\n",
    "    n_estimators = randomSearchModel.best_params_['n_estimators'],\n",
    "    max_depth = randomSearchModel.best_params_['max_depth'],\n",
    "    random_state = 42,\n",
    "    n_jobs = -1\n",
    ")\n",
    "rfrModel.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3ae97fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Train\n",
      "MSE : 0.1225\n"
     ]
    }
   ],
   "source": [
    "# predict on train data\n",
    "yPred = rfrModel.predict(x_tr)\n",
    "trainMse = mean_squared_error(y_tr, yPred)\n",
    "print(\"Predict Train\")\n",
    "print(\"MSE : %.4f\" % trainMse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e1ffe42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Test\n",
      "MSE : 0.2243\n"
     ]
    }
   ],
   "source": [
    "# predict on test data\n",
    "yPred = rfrModel.predict(x_cv)\n",
    "testMse = mean_squared_error(y_cv, yPred)\n",
    "print(\"Predict Test\")\n",
    "print(\"MSE : %.4f\" % testMse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06fb1aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to result array\n",
    "resArr.append([\"Random Forest\", str(\"%.4f\" % trainMse), str(\"%.4f\" % testMse), \"-\", \"-\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae8021a",
   "metadata": {},
   "source": [
    "###  Predict on test.csv test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "938af5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do prep: process.. run pred\n",
    "testRF = xTest.copy()\n",
    "test_id = testRF['id']\n",
    "testRF.drop(['id', 'timestamp'], axis = 1, inplace = True)\n",
    "\n",
    "# process datatype\n",
    "categoricals = xTrain.select_dtypes(include = [\"object\"]).copy()\n",
    "numericals = xTrain.select_dtypes(exclude = [\"object\"])\n",
    "\n",
    "for c in categoricals:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(xTrain[c])\n",
    "\n",
    "    testRF[c] = testRF[c].map(lambda s: \"<unknown>\" if s not in le.classes_ else s)\n",
    "    le.classes_ = np.append(le.classes_, \"<unknown>\")\n",
    "\n",
    "    testRF[c] = le.transform(testRF[c])\n",
    "\n",
    "for n in numericals:\n",
    "    min = xTrain[n].min()\n",
    "    max = xTrain[n].max()\n",
    "\n",
    "    testRF[n] = (testRF[n] - min) / (max - min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed5c871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using random forest model\n",
    "test_pred = rfrModel.predict(testRF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba4fb84",
   "metadata": {},
   "source": [
    "### Output File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e921a48",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_sq</th>\n",
       "      <th>life_sq</th>\n",
       "      <th>floor</th>\n",
       "      <th>max_floor</th>\n",
       "      <th>material</th>\n",
       "      <th>num_room</th>\n",
       "      <th>state</th>\n",
       "      <th>product_type</th>\n",
       "      <th>sub_area</th>\n",
       "      <th>area_m</th>\n",
       "      <th>raion_popul</th>\n",
       "      <th>green_zone_part</th>\n",
       "      <th>indust_part</th>\n",
       "      <th>children_preschool</th>\n",
       "      <th>preschool_quota</th>\n",
       "      <th>preschool_education_centers_raion</th>\n",
       "      <th>children_school</th>\n",
       "      <th>school_education_centers_raion</th>\n",
       "      <th>school_education_centers_top_20_raion</th>\n",
       "      <th>hospital_beds_raion</th>\n",
       "      <th>healthcare_centers_raion</th>\n",
       "      <th>university_top_20_raion</th>\n",
       "      <th>sport_objects_raion</th>\n",
       "      <th>additional_education_raion</th>\n",
       "      <th>culture_objects_top_25</th>\n",
       "      <th>shopping_centers_raion</th>\n",
       "      <th>office_raion</th>\n",
       "      <th>thermal_power_plant_raion</th>\n",
       "      <th>incineration_raion</th>\n",
       "      <th>oil_chemistry_raion</th>\n",
       "      <th>radiation_raion</th>\n",
       "      <th>railroad_terminal_raion</th>\n",
       "      <th>big_market_raion</th>\n",
       "      <th>nuclear_reactor_raion</th>\n",
       "      <th>detention_facility_raion</th>\n",
       "      <th>young_all</th>\n",
       "      <th>young_male</th>\n",
       "      <th>young_female</th>\n",
       "      <th>work_all</th>\n",
       "      <th>work_male</th>\n",
       "      <th>work_female</th>\n",
       "      <th>ekder_all</th>\n",
       "      <th>ekder_male</th>\n",
       "      <th>ekder_female</th>\n",
       "      <th>0_6_all</th>\n",
       "      <th>0_6_male</th>\n",
       "      <th>0_6_female</th>\n",
       "      <th>7_14_all</th>\n",
       "      <th>7_14_male</th>\n",
       "      <th>7_14_female</th>\n",
       "      <th>0_17_all</th>\n",
       "      <th>0_17_male</th>\n",
       "      <th>0_17_female</th>\n",
       "      <th>0_13_all</th>\n",
       "      <th>0_13_male</th>\n",
       "      <th>0_13_female</th>\n",
       "      <th>raion_build_count_with_material_info</th>\n",
       "      <th>build_count_brick</th>\n",
       "      <th>build_count_monolith</th>\n",
       "      <th>raion_build_count_with_builddate_info</th>\n",
       "      <th>build_count_before_1920</th>\n",
       "      <th>build_count_1946-1970</th>\n",
       "      <th>metro_min_avto</th>\n",
       "      <th>metro_km_avto</th>\n",
       "      <th>metro_min_walk</th>\n",
       "      <th>metro_km_walk</th>\n",
       "      <th>kindergarten_km</th>\n",
       "      <th>school_km</th>\n",
       "      <th>park_km</th>\n",
       "      <th>green_zone_km</th>\n",
       "      <th>industrial_km</th>\n",
       "      <th>water_treatment_km</th>\n",
       "      <th>incineration_km</th>\n",
       "      <th>railroad_station_walk_km</th>\n",
       "      <th>railroad_station_walk_min</th>\n",
       "      <th>...</th>\n",
       "      <th>sport_count_1500</th>\n",
       "      <th>market_count_1500</th>\n",
       "      <th>green_part_2000</th>\n",
       "      <th>prom_part_2000</th>\n",
       "      <th>office_count_2000</th>\n",
       "      <th>office_sqm_2000</th>\n",
       "      <th>trc_count_2000</th>\n",
       "      <th>trc_sqm_2000</th>\n",
       "      <th>cafe_count_2000</th>\n",
       "      <th>cafe_sum_2000_min_price_avg</th>\n",
       "      <th>cafe_sum_2000_max_price_avg</th>\n",
       "      <th>cafe_avg_price_2000</th>\n",
       "      <th>cafe_count_2000_na_price</th>\n",
       "      <th>cafe_count_2000_price_500</th>\n",
       "      <th>cafe_count_2000_price_1000</th>\n",
       "      <th>cafe_count_2000_price_1500</th>\n",
       "      <th>cafe_count_2000_price_2500</th>\n",
       "      <th>cafe_count_2000_price_4000</th>\n",
       "      <th>cafe_count_2000_price_high</th>\n",
       "      <th>big_church_count_2000</th>\n",
       "      <th>church_count_2000</th>\n",
       "      <th>mosque_count_2000</th>\n",
       "      <th>leisure_count_2000</th>\n",
       "      <th>sport_count_2000</th>\n",
       "      <th>market_count_2000</th>\n",
       "      <th>green_part_3000</th>\n",
       "      <th>office_count_3000</th>\n",
       "      <th>office_sqm_3000</th>\n",
       "      <th>trc_count_3000</th>\n",
       "      <th>trc_sqm_3000</th>\n",
       "      <th>cafe_count_3000</th>\n",
       "      <th>cafe_count_3000_na_price</th>\n",
       "      <th>cafe_count_3000_price_500</th>\n",
       "      <th>cafe_count_3000_price_1000</th>\n",
       "      <th>cafe_count_3000_price_1500</th>\n",
       "      <th>cafe_count_3000_price_2500</th>\n",
       "      <th>cafe_count_3000_price_4000</th>\n",
       "      <th>cafe_count_3000_price_high</th>\n",
       "      <th>big_church_count_3000</th>\n",
       "      <th>church_count_3000</th>\n",
       "      <th>mosque_count_3000</th>\n",
       "      <th>leisure_count_3000</th>\n",
       "      <th>sport_count_3000</th>\n",
       "      <th>market_count_3000</th>\n",
       "      <th>green_part_5000</th>\n",
       "      <th>prom_part_5000</th>\n",
       "      <th>office_count_5000</th>\n",
       "      <th>office_sqm_5000</th>\n",
       "      <th>trc_count_5000</th>\n",
       "      <th>trc_sqm_5000</th>\n",
       "      <th>cafe_count_5000</th>\n",
       "      <th>cafe_count_5000_na_price</th>\n",
       "      <th>cafe_count_5000_price_500</th>\n",
       "      <th>cafe_count_5000_price_1000</th>\n",
       "      <th>cafe_count_5000_price_1500</th>\n",
       "      <th>cafe_count_5000_price_2500</th>\n",
       "      <th>cafe_count_5000_price_4000</th>\n",
       "      <th>cafe_count_5000_price_high</th>\n",
       "      <th>big_church_count_5000</th>\n",
       "      <th>church_count_5000</th>\n",
       "      <th>mosque_count_5000</th>\n",
       "      <th>leisure_count_5000</th>\n",
       "      <th>sport_count_5000</th>\n",
       "      <th>market_count_5000</th>\n",
       "      <th>year</th>\n",
       "      <th>year_month</th>\n",
       "      <th>living_area_ratio</th>\n",
       "      <th>non_living_area</th>\n",
       "      <th>non_living_area_ratio</th>\n",
       "      <th>room_area_avg</th>\n",
       "      <th>relative_floor</th>\n",
       "      <th>sub_area_building_height_avg</th>\n",
       "      <th>sub_area_kremlin_dist_avg</th>\n",
       "      <th>sales_year_month</th>\n",
       "      <th>price_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30471</th>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0690</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0.1180</td>\n",
       "      <td>0.7174</td>\n",
       "      <td>0.1598</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.7300</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8462</td>\n",
       "      <td>0.7784</td>\n",
       "      <td>0.9286</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.2462</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4483</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1739</td>\n",
       "      <td>0.0284</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7549</td>\n",
       "      <td>0.7727</td>\n",
       "      <td>0.7360</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.7399</td>\n",
       "      <td>0.7597</td>\n",
       "      <td>0.4517</td>\n",
       "      <td>0.3794</td>\n",
       "      <td>0.4886</td>\n",
       "      <td>0.7300</td>\n",
       "      <td>0.7443</td>\n",
       "      <td>0.7145</td>\n",
       "      <td>0.7784</td>\n",
       "      <td>0.8013</td>\n",
       "      <td>0.7544</td>\n",
       "      <td>0.7581</td>\n",
       "      <td>0.7767</td>\n",
       "      <td>0.7383</td>\n",
       "      <td>0.7505</td>\n",
       "      <td>0.7700</td>\n",
       "      <td>0.7297</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3690</td>\n",
       "      <td>0.9134</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0916</td>\n",
       "      <td>0.5195</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0124</td>\n",
       "      <td>0.0124</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0158</td>\n",
       "      <td>0.0432</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0858</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>0.1774</td>\n",
       "      <td>0.1943</td>\n",
       "      <td>0.1943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1081</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2014</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.3493</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0926</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1951</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>0.0275</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2510</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0667</td>\n",
       "      <td>0.0652</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0642</td>\n",
       "      <td>0.0476</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.5839</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.5248</td>\n",
       "      <td>0.3324</td>\n",
       "      <td>0.2309</td>\n",
       "      <td>5,112,861.2160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30472</th>\n",
       "      <td>0.0147</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.1039</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1111</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>0.1150</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2462</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0435</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.1673</td>\n",
       "      <td>0.1024</td>\n",
       "      <td>0.0472</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1645</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.0460</td>\n",
       "      <td>0.0581</td>\n",
       "      <td>0.0581</td>\n",
       "      <td>0.0410</td>\n",
       "      <td>0.0281</td>\n",
       "      <td>0.0929</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.3334</td>\n",
       "      <td>0.3132</td>\n",
       "      <td>0.2205</td>\n",
       "      <td>0.2205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6541</td>\n",
       "      <td>0.0724</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.2449</td>\n",
       "      <td>0.2619</td>\n",
       "      <td>0.2554</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5317</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0305</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4946</td>\n",
       "      <td>0.2642</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.0504</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0440</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>0.0476</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.5839</td>\n",
       "      <td>0.9937</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>0.7023</td>\n",
       "      <td>0.3214</td>\n",
       "      <td>0.2309</td>\n",
       "      <td>8,251,718.1752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30473</th>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0390</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.0556</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>0.5584</td>\n",
       "      <td>0.0746</td>\n",
       "      <td>0.4327</td>\n",
       "      <td>0.3268</td>\n",
       "      <td>0.1872</td>\n",
       "      <td>0.5385</td>\n",
       "      <td>0.3378</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3192</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4483</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0870</td>\n",
       "      <td>0.0496</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3331</td>\n",
       "      <td>0.3246</td>\n",
       "      <td>0.3422</td>\n",
       "      <td>0.5647</td>\n",
       "      <td>0.5570</td>\n",
       "      <td>0.5722</td>\n",
       "      <td>0.5869</td>\n",
       "      <td>0.4967</td>\n",
       "      <td>0.6329</td>\n",
       "      <td>0.3268</td>\n",
       "      <td>0.3151</td>\n",
       "      <td>0.3394</td>\n",
       "      <td>0.3378</td>\n",
       "      <td>0.3339</td>\n",
       "      <td>0.3419</td>\n",
       "      <td>0.3374</td>\n",
       "      <td>0.3301</td>\n",
       "      <td>0.3451</td>\n",
       "      <td>0.3315</td>\n",
       "      <td>0.3233</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.3825</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.3335</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5172</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0530</td>\n",
       "      <td>0.2929</td>\n",
       "      <td>0.0641</td>\n",
       "      <td>0.2425</td>\n",
       "      <td>0.1711</td>\n",
       "      <td>0.1462</td>\n",
       "      <td>0.1462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3243</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.5431</td>\n",
       "      <td>0.1873</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0811</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0251</td>\n",
       "      <td>0.1786</td>\n",
       "      <td>0.1975</td>\n",
       "      <td>0.1903</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0727</td>\n",
       "      <td>0.2593</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.6180</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0352</td>\n",
       "      <td>0.0909</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>0.0204</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.0431</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0588</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.3072</td>\n",
       "      <td>0.4720</td>\n",
       "      <td>0.0342</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.2167</td>\n",
       "      <td>0.2234</td>\n",
       "      <td>0.0677</td>\n",
       "      <td>0.0287</td>\n",
       "      <td>0.0815</td>\n",
       "      <td>0.0988</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0662</td>\n",
       "      <td>0.0840</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0943</td>\n",
       "      <td>0.3257</td>\n",
       "      <td>0.5238</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.5837</td>\n",
       "      <td>0.9935</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.2586</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>0.2309</td>\n",
       "      <td>5,319,710.9437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30474</th>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.2208</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0556</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>1</td>\n",
       "      <td>105</td>\n",
       "      <td>0.0952</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.3062</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2462</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0163</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0184</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0158</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0163</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>0.1673</td>\n",
       "      <td>0.1024</td>\n",
       "      <td>0.0472</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1645</td>\n",
       "      <td>0.1291</td>\n",
       "      <td>0.0806</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>0.1096</td>\n",
       "      <td>0.0747</td>\n",
       "      <td>0.1185</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>0.1012</td>\n",
       "      <td>0.2470</td>\n",
       "      <td>0.4165</td>\n",
       "      <td>0.4165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4163</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0278</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4689</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2882</td>\n",
       "      <td>0.0511</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.5846</td>\n",
       "      <td>0.9939</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.3331</td>\n",
       "      <td>0.3632</td>\n",
       "      <td>0.2309</td>\n",
       "      <td>6,045,199.6659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30475</th>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.2208</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>0.1150</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2462</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0435</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.1673</td>\n",
       "      <td>0.1024</td>\n",
       "      <td>0.0472</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1645</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0291</td>\n",
       "      <td>0.0291</td>\n",
       "      <td>0.0309</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.3489</td>\n",
       "      <td>0.3243</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5127</td>\n",
       "      <td>0.0556</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.2449</td>\n",
       "      <td>0.2619</td>\n",
       "      <td>0.2554</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0556</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5607</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4462</td>\n",
       "      <td>0.2381</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0439</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0480</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0505</td>\n",
       "      <td>0.0476</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.5825</td>\n",
       "      <td>0.9894</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.7023</td>\n",
       "      <td>0.3214</td>\n",
       "      <td>0.2309</td>\n",
       "      <td>5,063,400.4229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30476</th>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.3549</td>\n",
       "      <td>0.4017</td>\n",
       "      <td>0.4572</td>\n",
       "      <td>0.1957</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.3846</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>0.3571</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6207</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2609</td>\n",
       "      <td>0.0426</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1966</td>\n",
       "      <td>0.1947</td>\n",
       "      <td>0.1986</td>\n",
       "      <td>0.3976</td>\n",
       "      <td>0.4815</td>\n",
       "      <td>0.3160</td>\n",
       "      <td>0.2743</td>\n",
       "      <td>0.2373</td>\n",
       "      <td>0.2933</td>\n",
       "      <td>0.1957</td>\n",
       "      <td>0.1918</td>\n",
       "      <td>0.1999</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>0.1947</td>\n",
       "      <td>0.1987</td>\n",
       "      <td>0.1975</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.2006</td>\n",
       "      <td>0.1963</td>\n",
       "      <td>0.1928</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1750</td>\n",
       "      <td>0.2560</td>\n",
       "      <td>0.0551</td>\n",
       "      <td>0.1751</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.2118</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>0.0283</td>\n",
       "      <td>0.0352</td>\n",
       "      <td>0.0352</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.0226</td>\n",
       "      <td>0.0362</td>\n",
       "      <td>0.1994</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>0.2104</td>\n",
       "      <td>0.1154</td>\n",
       "      <td>0.1154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1892</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1578</td>\n",
       "      <td>0.6733</td>\n",
       "      <td>0.1120</td>\n",
       "      <td>0.5540</td>\n",
       "      <td>0.2703</td>\n",
       "      <td>0.2629</td>\n",
       "      <td>0.0404</td>\n",
       "      <td>0.3189</td>\n",
       "      <td>0.3294</td>\n",
       "      <td>0.3253</td>\n",
       "      <td>0.0429</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0573</td>\n",
       "      <td>0.0421</td>\n",
       "      <td>0.0412</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0182</td>\n",
       "      <td>0.2963</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.1055</td>\n",
       "      <td>0.5254</td>\n",
       "      <td>0.2424</td>\n",
       "      <td>0.4152</td>\n",
       "      <td>0.0865</td>\n",
       "      <td>0.1092</td>\n",
       "      <td>0.0757</td>\n",
       "      <td>0.0884</td>\n",
       "      <td>0.0650</td>\n",
       "      <td>0.1165</td>\n",
       "      <td>0.0885</td>\n",
       "      <td>0.0435</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0549</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0471</td>\n",
       "      <td>0.3600</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1685</td>\n",
       "      <td>0.4102</td>\n",
       "      <td>0.2142</td>\n",
       "      <td>0.3911</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.4965</td>\n",
       "      <td>0.2291</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>0.1785</td>\n",
       "      <td>0.2083</td>\n",
       "      <td>0.2231</td>\n",
       "      <td>0.2626</td>\n",
       "      <td>0.3878</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1523</td>\n",
       "      <td>0.1680</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.1226</td>\n",
       "      <td>0.5642</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.5839</td>\n",
       "      <td>0.9937</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.5676</td>\n",
       "      <td>0.4846</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>0.2309</td>\n",
       "      <td>7,999,563.0845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30477</th>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.1948</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.4546</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.7112</td>\n",
       "      <td>0.4287</td>\n",
       "      <td>0.3315</td>\n",
       "      <td>0.3846</td>\n",
       "      <td>0.3646</td>\n",
       "      <td>0.3571</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1547</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2174</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3948</td>\n",
       "      <td>0.3776</td>\n",
       "      <td>0.4131</td>\n",
       "      <td>0.4616</td>\n",
       "      <td>0.4733</td>\n",
       "      <td>0.4503</td>\n",
       "      <td>0.3843</td>\n",
       "      <td>0.3151</td>\n",
       "      <td>0.4197</td>\n",
       "      <td>0.4287</td>\n",
       "      <td>0.3999</td>\n",
       "      <td>0.4597</td>\n",
       "      <td>0.3646</td>\n",
       "      <td>0.3560</td>\n",
       "      <td>0.3736</td>\n",
       "      <td>0.3909</td>\n",
       "      <td>0.3755</td>\n",
       "      <td>0.4072</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.3798</td>\n",
       "      <td>0.4185</td>\n",
       "      <td>0.4762</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>0.1260</td>\n",
       "      <td>0.4759</td>\n",
       "      <td>0.0620</td>\n",
       "      <td>0.3290</td>\n",
       "      <td>0.0544</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0344</td>\n",
       "      <td>0.0344</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.1818</td>\n",
       "      <td>0.0449</td>\n",
       "      <td>0.3345</td>\n",
       "      <td>0.2631</td>\n",
       "      <td>0.0920</td>\n",
       "      <td>0.0920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0811</td>\n",
       "      <td>0.2857</td>\n",
       "      <td>0.2884</td>\n",
       "      <td>0.4772</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>0.1351</td>\n",
       "      <td>0.0471</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.1929</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1973</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0182</td>\n",
       "      <td>0.0741</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.3412</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>0.1061</td>\n",
       "      <td>0.1109</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.3140</td>\n",
       "      <td>0.4268</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>0.0307</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.1032</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.0169</td>\n",
       "      <td>0.0216</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0186</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0331</td>\n",
       "      <td>0.0560</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0283</td>\n",
       "      <td>0.0780</td>\n",
       "      <td>0.0952</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.5839</td>\n",
       "      <td>0.9937</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.6719</td>\n",
       "      <td>0.2395</td>\n",
       "      <td>0.2309</td>\n",
       "      <td>4,340,339.2936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30478</th>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1121</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3568</td>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "      <td>0.3171</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.3928</td>\n",
       "      <td>0.1383</td>\n",
       "      <td>0.0253</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2462</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0249</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.0248</td>\n",
       "      <td>0.0281</td>\n",
       "      <td>0.0301</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0267</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.0253</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0246</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.0253</td>\n",
       "      <td>0.0255</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.0248</td>\n",
       "      <td>0.0248</td>\n",
       "      <td>0.0247</td>\n",
       "      <td>0.1673</td>\n",
       "      <td>0.1024</td>\n",
       "      <td>0.0472</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1645</td>\n",
       "      <td>0.1008</td>\n",
       "      <td>0.0687</td>\n",
       "      <td>0.0868</td>\n",
       "      <td>0.0868</td>\n",
       "      <td>0.1025</td>\n",
       "      <td>0.0658</td>\n",
       "      <td>0.0680</td>\n",
       "      <td>0.0565</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1435</td>\n",
       "      <td>0.1695</td>\n",
       "      <td>0.4931</td>\n",
       "      <td>0.4931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4942</td>\n",
       "      <td>0.1061</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.1071</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.1438</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3860</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4051</td>\n",
       "      <td>0.3929</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0732</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0480</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0275</td>\n",
       "      <td>0.1429</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.5839</td>\n",
       "      <td>0.9937</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.4895</td>\n",
       "      <td>0.2961</td>\n",
       "      <td>0.2309</td>\n",
       "      <td>4,259,547.0554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30479</th>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.1169</td>\n",
       "      <td>0.0948</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.0556</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.0755</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0695</td>\n",
       "      <td>0.0691</td>\n",
       "      <td>0.0927</td>\n",
       "      <td>0.0769</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0714</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0412</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0435</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.0654</td>\n",
       "      <td>0.0642</td>\n",
       "      <td>0.0741</td>\n",
       "      <td>0.0771</td>\n",
       "      <td>0.0711</td>\n",
       "      <td>0.0717</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.0795</td>\n",
       "      <td>0.0691</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>0.0686</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0618</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>0.0647</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.0650</td>\n",
       "      <td>0.0659</td>\n",
       "      <td>0.0641</td>\n",
       "      <td>0.7333</td>\n",
       "      <td>0.3825</td>\n",
       "      <td>0.1417</td>\n",
       "      <td>0.7338</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.1032</td>\n",
       "      <td>0.0699</td>\n",
       "      <td>0.0883</td>\n",
       "      <td>0.0883</td>\n",
       "      <td>0.0442</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.1502</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.1121</td>\n",
       "      <td>0.3460</td>\n",
       "      <td>0.3474</td>\n",
       "      <td>0.0932</td>\n",
       "      <td>0.0932</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4290</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.1747</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5162</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0122</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5666</td>\n",
       "      <td>0.1249</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.0304</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0280</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0321</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.5838</td>\n",
       "      <td>0.9934</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.3672</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>0.2309</td>\n",
       "      <td>3,619,208.6025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30480</th>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0909</td>\n",
       "      <td>0.1810</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>0.1150</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2462</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0435</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.1673</td>\n",
       "      <td>0.1024</td>\n",
       "      <td>0.0472</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1645</td>\n",
       "      <td>0.1679</td>\n",
       "      <td>0.1228</td>\n",
       "      <td>0.0718</td>\n",
       "      <td>0.0718</td>\n",
       "      <td>0.0847</td>\n",
       "      <td>0.0456</td>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1166</td>\n",
       "      <td>0.3946</td>\n",
       "      <td>0.3706</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6516</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2073</td>\n",
       "      <td>0.2207</td>\n",
       "      <td>0.2151</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5238</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3716</td>\n",
       "      <td>0.0480</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0321</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.5825</td>\n",
       "      <td>0.9894</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.7023</td>\n",
       "      <td>0.3214</td>\n",
       "      <td>0.2309</td>\n",
       "      <td>4,425,364.4423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       full_sq  life_sq  floor  max_floor  material  num_room  state  \\\n",
       "30471   0.0071   0.0028 0.0260     0.0690    0.0000    0.0000 0.6667   \n",
       "30472   0.0147   0.0040 0.1039     0.1379    0.0000    0.1111 0.0000   \n",
       "30473   0.0074   0.0034 0.0390     0.0345    0.2000    0.0556 0.3333   \n",
       "30474   0.0116   0.0048 0.2208     0.1379    0.0000    0.0556 0.6667   \n",
       "30475   0.0073   0.0053 0.2208     0.1379    0.0000    0.0000 0.0000   \n",
       "30476   0.0089   0.0040 0.2727     0.0000    0.0000    0.0000 0.0000   \n",
       "30477   0.0071   0.0040 0.1948     0.1379    0.0000    0.0000 0.0000   \n",
       "30478   0.0079   0.0040 0.0649     0.1121    0.0000    0.0000 0.3568   \n",
       "30479   0.0083   0.0038 0.1169     0.0948    0.8000    0.0556 0.3333   \n",
       "30480   0.0079   0.0058 0.0909     0.1810    0.0000    0.0000 0.0000   \n",
       "\n",
       "       product_type  sub_area  area_m  raion_popul  green_zone_part  \\\n",
       "30471             0        38  0.1180       0.7174           0.1598   \n",
       "30472             1       103  0.1150       0.0059           0.5810   \n",
       "30473             0        84  0.0386       0.5584           0.0746   \n",
       "30474             1       105  0.0952       0.0187           0.3062   \n",
       "30475             1       103  0.1150       0.0059           0.5810   \n",
       "30476             1        24  0.0370       0.3549           0.4017   \n",
       "30477             1       124  0.0453       0.4546           0.0963   \n",
       "30478             1       102  0.3171       0.0286           0.3928   \n",
       "30479             0       136  0.0649       0.0755           0.0584   \n",
       "30480             1       103  0.1150       0.0059           0.5810   \n",
       "\n",
       "       indust_part  children_preschool  preschool_quota  \\\n",
       "30471       0.0788              0.7300           1.0000   \n",
       "30472       0.0136              0.0052           0.2416   \n",
       "30473       0.4327              0.3268           0.1872   \n",
       "30474       0.0338              0.0165           0.2416   \n",
       "30475       0.0136              0.0052           0.2416   \n",
       "30476       0.4572              0.1957           0.1276   \n",
       "30477       0.7112              0.4287           0.3315   \n",
       "30478       0.1383              0.0253           0.2416   \n",
       "30479       0.0695              0.0691           0.0927   \n",
       "30480       0.0136              0.0052           0.2416   \n",
       "\n",
       "       preschool_education_centers_raion  children_school  \\\n",
       "30471                             0.8462           0.7784   \n",
       "30472                             0.0000           0.0051   \n",
       "30473                             0.5385           0.3378   \n",
       "30474                             0.0000           0.0159   \n",
       "30475                             0.0000           0.0051   \n",
       "30476                             0.3846           0.1967   \n",
       "30477                             0.3846           0.3646   \n",
       "30478                             0.0000           0.0244   \n",
       "30479                             0.0769           0.0608   \n",
       "30480                             0.0000           0.0051   \n",
       "\n",
       "       school_education_centers_raion  school_education_centers_top_20_raion  \\\n",
       "30471                          0.9286                                 0.5000   \n",
       "30472                          0.0000                                 0.0000   \n",
       "30473                          0.5000                                 0.0000   \n",
       "30474                          0.0000                                 0.0000   \n",
       "30475                          0.0000                                 0.0000   \n",
       "30476                          0.3571                                 0.0000   \n",
       "30477                          0.3571                                 0.0000   \n",
       "30478                          0.0000                                 0.0000   \n",
       "30479                          0.0714                                 0.0000   \n",
       "30480                          0.0000                                 0.0000   \n",
       "\n",
       "       hospital_beds_raion  healthcare_centers_raion  university_top_20_raion  \\\n",
       "30471               0.2462                    0.1667                   0.0000   \n",
       "30472               0.2462                    0.0000                   0.0000   \n",
       "30473               0.3192                    0.5000                   0.0000   \n",
       "30474               0.2462                    0.0000                   0.0000   \n",
       "30475               0.2462                    0.0000                   0.0000   \n",
       "30476               0.1454                    0.5000                   0.0000   \n",
       "30477               0.1547                    0.3333                   0.0000   \n",
       "30478               0.2462                    0.0000                   0.0000   \n",
       "30479               0.0412                    0.1667                   0.0000   \n",
       "30480               0.2462                    0.0000                   0.0000   \n",
       "\n",
       "       sport_objects_raion  additional_education_raion  \\\n",
       "30471               0.4483                      0.2500   \n",
       "30472               0.0000                      0.0000   \n",
       "30473               0.4483                      0.0000   \n",
       "30474               0.0000                      0.1250   \n",
       "30475               0.0000                      0.0000   \n",
       "30476               0.6207                      0.0625   \n",
       "30477               0.1379                      0.1875   \n",
       "30478               0.0345                      0.0000   \n",
       "30479               0.0000                      0.0000   \n",
       "30480               0.0000                      0.0000   \n",
       "\n",
       "       culture_objects_top_25  shopping_centers_raion  office_raion  \\\n",
       "30471                       0                  0.1739        0.0284   \n",
       "30472                       0                  0.0435        0.0000   \n",
       "30473                       0                  0.0870        0.0496   \n",
       "30474                       0                  0.0000        0.0000   \n",
       "30475                       0                  0.0435        0.0000   \n",
       "30476                       0                  0.2609        0.0426   \n",
       "30477                       0                  0.2174        0.0071   \n",
       "30478                       0                  0.0000        0.0071   \n",
       "30479                       0                  0.0435        0.0071   \n",
       "30480                       0                  0.0435        0.0000   \n",
       "\n",
       "       thermal_power_plant_raion  incineration_raion  oil_chemistry_raion  \\\n",
       "30471                          0                   0                    0   \n",
       "30472                          0                   0                    0   \n",
       "30473                          1                   0                    1   \n",
       "30474                          0                   0                    0   \n",
       "30475                          0                   0                    0   \n",
       "30476                          0                   0                    0   \n",
       "30477                          0                   0                    0   \n",
       "30478                          0                   0                    0   \n",
       "30479                          0                   0                    0   \n",
       "30480                          0                   0                    0   \n",
       "\n",
       "       radiation_raion  railroad_terminal_raion  big_market_raion  \\\n",
       "30471                0                        0                 0   \n",
       "30472                0                        0                 0   \n",
       "30473                1                        0                 0   \n",
       "30474                0                        0                 0   \n",
       "30475                0                        0                 0   \n",
       "30476                1                        0                 0   \n",
       "30477                0                        0                 0   \n",
       "30478                0                        0                 1   \n",
       "30479                0                        0                 0   \n",
       "30480                0                        0                 0   \n",
       "\n",
       "       nuclear_reactor_raion  detention_facility_raion  young_all  young_male  \\\n",
       "30471                      0                         0     0.7549      0.7727   \n",
       "30472                      0                         0     0.0052      0.0052   \n",
       "30473                      0                         0     0.3331      0.3246   \n",
       "30474                      0                         0     0.0163      0.0164   \n",
       "30475                      0                         0     0.0052      0.0052   \n",
       "30476                      0                         0     0.1966      0.1947   \n",
       "30477                      0                         0     0.3948      0.3776   \n",
       "30478                      0                         0     0.0249      0.0250   \n",
       "30479                      0                         0     0.0649      0.0654   \n",
       "30480                      0                         0     0.0052      0.0052   \n",
       "\n",
       "       young_female  work_all  work_male  work_female  ekder_all  ekder_male  \\\n",
       "30471        0.7360    0.7500     0.7399       0.7597     0.4517      0.3794   \n",
       "30472        0.0051    0.0058     0.0063       0.0054     0.0055      0.0046   \n",
       "30473        0.3422    0.5647     0.5570       0.5722     0.5869      0.4967   \n",
       "30474        0.0162    0.0184     0.0197       0.0171     0.0174      0.0146   \n",
       "30475        0.0051    0.0058     0.0063       0.0054     0.0055      0.0046   \n",
       "30476        0.1986    0.3976     0.4815       0.3160     0.2743      0.2373   \n",
       "30477        0.4131    0.4616     0.4733       0.4503     0.3843      0.3151   \n",
       "30478        0.0248    0.0281     0.0301       0.0262     0.0267      0.0223   \n",
       "30479        0.0642    0.0741     0.0771       0.0711     0.0717      0.0564   \n",
       "30480        0.0051    0.0058     0.0063       0.0054     0.0055      0.0046   \n",
       "\n",
       "       ekder_female  0_6_all  0_6_male  0_6_female  7_14_all  7_14_male  \\\n",
       "30471        0.4886   0.7300    0.7443      0.7145    0.7784     0.8013   \n",
       "30472        0.0060   0.0052    0.0053      0.0052    0.0051     0.0051   \n",
       "30473        0.6329   0.3268    0.3151      0.3394    0.3378     0.3339   \n",
       "30474        0.0189   0.0165    0.0165      0.0165    0.0159     0.0160   \n",
       "30475        0.0060   0.0052    0.0053      0.0052    0.0051     0.0051   \n",
       "30476        0.2933   0.1957    0.1918      0.1999    0.1967     0.1947   \n",
       "30477        0.4197   0.4287    0.3999      0.4597    0.3646     0.3560   \n",
       "30478        0.0289   0.0253    0.0252      0.0252    0.0244     0.0246   \n",
       "30479        0.0795   0.0691    0.0694      0.0686    0.0608     0.0618   \n",
       "30480        0.0060   0.0052    0.0053      0.0052    0.0051     0.0051   \n",
       "\n",
       "       7_14_female  0_17_all  0_17_male  0_17_female  0_13_all  0_13_male  \\\n",
       "30471       0.7544    0.7581     0.7767       0.7383    0.7505     0.7700   \n",
       "30472       0.0050    0.0053     0.0053       0.0052    0.0052     0.0052   \n",
       "30473       0.3419    0.3374     0.3301       0.3451    0.3315     0.3233   \n",
       "30474       0.0158    0.0165     0.0166       0.0163    0.0162     0.0162   \n",
       "30475       0.0050    0.0053     0.0053       0.0052    0.0052     0.0052   \n",
       "30476       0.1987    0.1975     0.1945       0.2006    0.1963     0.1928   \n",
       "30477       0.3736    0.3909     0.3755       0.4072    0.3985     0.3798   \n",
       "30478       0.0241    0.0253     0.0255       0.0250    0.0248     0.0248   \n",
       "30479       0.0596    0.0647     0.0645       0.0649    0.0650     0.0659   \n",
       "30480       0.0050    0.0053     0.0053       0.0052    0.0052     0.0052   \n",
       "\n",
       "       0_13_female  raion_build_count_with_material_info  build_count_brick  \\\n",
       "30471       0.7297                                1.0000             0.3690   \n",
       "30472       0.0051                                0.1673             0.1024   \n",
       "30473       0.3403                                0.3333             0.3825   \n",
       "30474       0.0161                                0.1673             0.1024   \n",
       "30475       0.0051                                0.1673             0.1024   \n",
       "30476       0.2000                                0.1750             0.2560   \n",
       "30477       0.4185                                0.4762             0.1898   \n",
       "30478       0.0247                                0.1673             0.1024   \n",
       "30479       0.0641                                0.7333             0.3825   \n",
       "30480       0.0051                                0.1673             0.1024   \n",
       "\n",
       "       build_count_monolith  raion_build_count_with_builddate_info  \\\n",
       "30471                0.9134                                 1.0000   \n",
       "30472                0.0472                                 0.1674   \n",
       "30473                0.0236                                 0.3335   \n",
       "30474                0.0472                                 0.1674   \n",
       "30475                0.0472                                 0.1674   \n",
       "30476                0.0551                                 0.1751   \n",
       "30477                0.1260                                 0.4759   \n",
       "30478                0.0472                                 0.1674   \n",
       "30479                0.1417                                 0.7338   \n",
       "30480                0.0472                                 0.1674   \n",
       "\n",
       "       build_count_before_1920  build_count_1946-1970  metro_min_avto  \\\n",
       "30471                   0.0916                 0.5195          0.0205   \n",
       "30472                   0.0000                 0.1645          0.0689   \n",
       "30473                   0.0000                 0.5172          0.0258   \n",
       "30474                   0.0000                 0.1645          0.1291   \n",
       "30475                   0.0000                 0.1645          0.0350   \n",
       "30476                   0.0027                 0.2118          0.0550   \n",
       "30477                   0.0620                 0.3290          0.0544   \n",
       "30478                   0.0000                 0.1645          0.1008   \n",
       "30479                   0.0027                 1.0000          0.1032   \n",
       "30480                   0.0000                 0.1645          0.1679   \n",
       "\n",
       "       metro_km_avto  metro_min_walk  metro_km_walk  kindergarten_km  \\\n",
       "30471         0.0098          0.0124         0.0124           0.0027   \n",
       "30472         0.0460          0.0581         0.0581           0.0410   \n",
       "30473         0.0150          0.0189         0.0189           0.0022   \n",
       "30474         0.0806          0.0964         0.0964           0.1096   \n",
       "30475         0.0230          0.0291         0.0291           0.0309   \n",
       "30476         0.0283          0.0352         0.0352           0.0250   \n",
       "30477         0.0272          0.0344         0.0344           0.0079   \n",
       "30478         0.0687          0.0868         0.0868           0.1025   \n",
       "30479         0.0699          0.0883         0.0883           0.0442   \n",
       "30480         0.1228          0.0718         0.0718           0.0847   \n",
       "\n",
       "       school_km  park_km  green_zone_km  industrial_km  water_treatment_km  \\\n",
       "30471     0.0158   0.0432         0.0310         0.0858              0.0147   \n",
       "30472     0.0281   0.0929         0.0000         0.0528              0.3334   \n",
       "30473     0.0041   0.0530         0.2929         0.0641              0.2425   \n",
       "30474     0.0747   0.1185         0.0128         0.0332              0.1012   \n",
       "30475     0.0260   0.0964         0.2155         0.0252              0.3489   \n",
       "30476     0.0226   0.0362         0.1994         0.0000              0.3048   \n",
       "30477     0.0009   0.0131         0.1818         0.0449              0.3345   \n",
       "30478     0.0658   0.0680         0.0565         0.0060              0.1435   \n",
       "30479     0.0178   0.1502         0.0197         0.1121              0.3460   \n",
       "30480     0.0456   0.1453         0.0000         0.1166              0.3946   \n",
       "\n",
       "       incineration_km  railroad_station_walk_km  railroad_station_walk_min  \\\n",
       "30471           0.1774                    0.1943                     0.1943   \n",
       "30472           0.3132                    0.2205                     0.2205   \n",
       "30473           0.1711                    0.1462                     0.1462   \n",
       "30474           0.2470                    0.4165                     0.4165   \n",
       "30475           0.3243                    0.1506                     0.1506   \n",
       "30476           0.2104                    0.1154                     0.1154   \n",
       "30477           0.2631                    0.0920                     0.0920   \n",
       "30478           0.1695                    0.4931                     0.4931   \n",
       "30479           0.3474                    0.0932                     0.0932   \n",
       "30480           0.3706                    0.0995                     0.0995   \n",
       "\n",
       "       ...  sport_count_1500  market_count_1500  green_part_2000  \\\n",
       "30471  ...            0.1081             0.0000           0.2014   \n",
       "30472  ...            0.0270             0.0000           0.6541   \n",
       "30473  ...            0.3243             0.4286           0.5431   \n",
       "30474  ...            0.0000             0.0000           0.4163   \n",
       "30475  ...            0.0000             0.0000           0.5127   \n",
       "30476  ...            0.1892             0.0000           0.1578   \n",
       "30477  ...            0.0811             0.2857           0.2884   \n",
       "30478  ...            0.0270             0.0000           0.4942   \n",
       "30479  ...            0.0000             0.0000           0.4290   \n",
       "30480  ...            0.0000             0.0000           0.6516   \n",
       "\n",
       "       prom_part_2000  office_count_2000  office_sqm_2000  trc_count_2000  \\\n",
       "30471          0.0210             0.0000           0.0000          0.0000   \n",
       "30472          0.0724             0.0000           0.0000          0.0270   \n",
       "30473          0.1873             0.0240           0.0223          0.0811   \n",
       "30474          0.0533             0.0000           0.0000          0.0000   \n",
       "30475          0.0556             0.0000           0.0000          0.0541   \n",
       "30476          0.6733             0.1120           0.5540          0.2703   \n",
       "30477          0.4772             0.0080           0.0219          0.1351   \n",
       "30478          0.1061             0.0000           0.0000          0.0000   \n",
       "30479          0.0025             0.0000           0.0000          0.0270   \n",
       "30480          0.0036             0.0000           0.0000          0.0270   \n",
       "\n",
       "       trc_sqm_2000  cafe_count_2000  cafe_sum_2000_min_price_avg  \\\n",
       "30471        0.0000           0.0027                       0.3750   \n",
       "30472        0.0020           0.0063                       0.2449   \n",
       "30473        0.0058           0.0251                       0.1786   \n",
       "30474        0.0000           0.0009                       0.0000   \n",
       "30475        0.0090           0.0063                       0.2449   \n",
       "30476        0.2629           0.0404                       0.3189   \n",
       "30477        0.0471           0.0063                       0.1929   \n",
       "30478        0.0000           0.0009                       0.1071   \n",
       "30479        0.0069           0.0018                       0.1875   \n",
       "30480        0.0069           0.0000                       0.2073   \n",
       "\n",
       "       cafe_sum_2000_max_price_avg  cafe_avg_price_2000  \\\n",
       "30471                       0.3333               0.3493   \n",
       "30472                       0.2619               0.2554   \n",
       "30473                       0.1975               0.1903   \n",
       "30474                       0.0000               0.0000   \n",
       "30475                       0.2619               0.2554   \n",
       "30476                       0.3294               0.3253   \n",
       "30477                       0.2000               0.1973   \n",
       "30478                       0.1667               0.1438   \n",
       "30479                       0.1667               0.1747   \n",
       "30480                       0.2207               0.2151   \n",
       "\n",
       "       cafe_count_2000_na_price  cafe_count_2000_price_500  \\\n",
       "30471                    0.0000                     0.0000   \n",
       "30472                    0.0000                     0.0036   \n",
       "30473                    0.0143                     0.0252   \n",
       "30474                    0.0000                     0.0036   \n",
       "30475                    0.0000                     0.0036   \n",
       "30476                    0.0429                     0.0252   \n",
       "30477                    0.0286                     0.0036   \n",
       "30478                    0.0000                     0.0000   \n",
       "30479                    0.0000                     0.0036   \n",
       "30480                    0.0000                     0.0000   \n",
       "\n",
       "       cafe_count_2000_price_1000  cafe_count_2000_price_1500  \\\n",
       "30471                      0.0000                      0.0115   \n",
       "30472                      0.0115                      0.0077   \n",
       "30473                      0.0458                      0.0230   \n",
       "30474                      0.0000                      0.0000   \n",
       "30475                      0.0115                      0.0077   \n",
       "30476                      0.0573                      0.0421   \n",
       "30477                      0.0076                      0.0077   \n",
       "30478                      0.0038                      0.0000   \n",
       "30479                      0.0000                      0.0038   \n",
       "30480                      0.0000                      0.0000   \n",
       "\n",
       "       cafe_count_2000_price_2500  cafe_count_2000_price_4000  \\\n",
       "30471                      0.0000                      0.0000   \n",
       "30472                      0.0059                      0.0000   \n",
       "30473                      0.0118                      0.0000   \n",
       "30474                      0.0000                      0.0000   \n",
       "30475                      0.0059                      0.0000   \n",
       "30476                      0.0412                      0.0123   \n",
       "30477                      0.0000                      0.0000   \n",
       "30478                      0.0000                      0.0000   \n",
       "30479                      0.0000                      0.0000   \n",
       "30480                      0.0000                      0.0000   \n",
       "\n",
       "       cafe_count_2000_price_high  big_church_count_2000  church_count_2000  \\\n",
       "30471                      0.0000                 0.0143             0.0185   \n",
       "30472                      0.0000                 0.0143             0.0185   \n",
       "30473                      0.0000                 0.0286             0.0185   \n",
       "30474                      0.0000                 0.0000             0.0278   \n",
       "30475                      0.0000                 0.0143             0.0185   \n",
       "30476                      0.0625                 0.0143             0.0185   \n",
       "30477                      0.0000                 0.0000             0.0000   \n",
       "30478                      0.0000                 0.0000             0.0185   \n",
       "30479                      0.0000                 0.0000             0.0093   \n",
       "30480                      0.0000                 0.0000             0.0185   \n",
       "\n",
       "       mosque_count_2000  leisure_count_2000  sport_count_2000  \\\n",
       "30471             1.0000              0.0000            0.0926   \n",
       "30472             0.0000              0.0000            0.0185   \n",
       "30473             0.0000              0.0727            0.2593   \n",
       "30474             0.0000              0.0000            0.0000   \n",
       "30475             0.0000              0.0000            0.0556   \n",
       "30476             0.0000              0.0182            0.2963   \n",
       "30477             0.0000              0.0182            0.0741   \n",
       "30478             0.0000              0.0000            0.0185   \n",
       "30479             0.0000              0.0000            0.0000   \n",
       "30480             0.0000              0.0000            0.0000   \n",
       "\n",
       "       market_count_2000  green_part_3000  office_count_3000  office_sqm_3000  \\\n",
       "30471             0.0000           0.1951             0.0000           0.0000   \n",
       "30472             0.0000           0.5317             0.0000           0.0000   \n",
       "30473             0.5000           0.6180             0.0162           0.0352   \n",
       "30474             0.0000           0.4689             0.0000           0.0000   \n",
       "30475             0.0000           0.5607             0.0000           0.0000   \n",
       "30476             0.0000           0.1811             0.1055           0.5254   \n",
       "30477             0.2500           0.3412             0.0061           0.0300   \n",
       "30478             0.0000           0.3860             0.0020           0.0139   \n",
       "30479             0.0000           0.5162             0.0000           0.0000   \n",
       "30480             0.0000           0.5238             0.0000           0.0000   \n",
       "\n",
       "       trc_count_3000  trc_sqm_3000  cafe_count_3000  \\\n",
       "30471          0.0455        0.0275           0.0066   \n",
       "30472          0.0303        0.0083           0.0055   \n",
       "30473          0.0909        0.0147           0.0204   \n",
       "30474          0.0000        0.0000           0.0006   \n",
       "30475          0.0303        0.0083           0.0050   \n",
       "30476          0.2424        0.4152           0.0865   \n",
       "30477          0.1061        0.1109           0.0066   \n",
       "30478          0.0000        0.0000           0.0022   \n",
       "30479          0.0152        0.0064           0.0028   \n",
       "30480          0.0152        0.0064           0.0022   \n",
       "\n",
       "       cafe_count_3000_na_price  cafe_count_3000_price_500  \\\n",
       "30471                    0.0084                     0.0045   \n",
       "30472                    0.0000                     0.0022   \n",
       "30473                    0.0084                     0.0178   \n",
       "30474                    0.0000                     0.0022   \n",
       "30475                    0.0000                     0.0022   \n",
       "30476                    0.1092                     0.0757   \n",
       "30477                    0.0252                     0.0045   \n",
       "30478                    0.0000                     0.0000   \n",
       "30479                    0.0000                     0.0022   \n",
       "30480                    0.0000                     0.0022   \n",
       "\n",
       "       cafe_count_3000_price_1000  cafe_count_3000_price_1500  \\\n",
       "30471                      0.0045                      0.0157   \n",
       "30472                      0.0136                      0.0045   \n",
       "30473                      0.0431                      0.0157   \n",
       "30474                      0.0000                      0.0000   \n",
       "30475                      0.0113                      0.0045   \n",
       "30476                      0.0884                      0.0650   \n",
       "30477                      0.0113                      0.0045   \n",
       "30478                      0.0023                      0.0022   \n",
       "30479                      0.0045                      0.0022   \n",
       "30480                      0.0023                      0.0022   \n",
       "\n",
       "       cafe_count_3000_price_2500  cafe_count_3000_price_4000  \\\n",
       "30471                      0.0000                      0.0000   \n",
       "30472                      0.0038                      0.0000   \n",
       "30473                      0.0075                      0.0000   \n",
       "30474                      0.0000                      0.0000   \n",
       "30475                      0.0038                      0.0000   \n",
       "30476                      0.1165                      0.0885   \n",
       "30477                      0.0000                      0.0000   \n",
       "30478                      0.0038                      0.0088   \n",
       "30479                      0.0038                      0.0000   \n",
       "30480                      0.0038                      0.0000   \n",
       "\n",
       "       cafe_count_3000_price_high  big_church_count_3000  church_count_3000  \\\n",
       "30471                      0.0000                 0.0098             0.0183   \n",
       "30472                      0.0000                 0.0098             0.0305   \n",
       "30473                      0.0000                 0.0196             0.0183   \n",
       "30474                      0.0000                 0.0000             0.0244   \n",
       "30475                      0.0000                 0.0098             0.0244   \n",
       "30476                      0.0435                 0.0294             0.0549   \n",
       "30477                      0.0000                 0.0000             0.0183   \n",
       "30478                      0.0000                 0.0000             0.0244   \n",
       "30479                      0.0000                 0.0000             0.0122   \n",
       "30480                      0.0000                 0.0000             0.0183   \n",
       "\n",
       "       mosque_count_3000  leisure_count_3000  sport_count_3000  \\\n",
       "30471             0.5000              0.0000            0.0700   \n",
       "30472             0.0000              0.0000            0.0700   \n",
       "30473             0.0000              0.0588            0.2200   \n",
       "30474             0.0000              0.0000            0.0000   \n",
       "30475             0.0000              0.0000            0.0600   \n",
       "30476             0.0000              0.0471            0.3600   \n",
       "30477             0.0000              0.0118            0.0500   \n",
       "30478             0.0000              0.0000            0.0100   \n",
       "30479             0.0000              0.0000            0.0000   \n",
       "30480             0.0000              0.0000            0.0000   \n",
       "\n",
       "       market_count_3000  green_part_5000  prom_part_5000  office_count_5000  \\\n",
       "30471             0.0000           0.2510          0.1580             0.0013   \n",
       "30472             0.0000           0.4946          0.2642             0.0025   \n",
       "30473             0.4000           0.3072          0.4720             0.0342   \n",
       "30474             0.0000           0.2882          0.0511             0.0000   \n",
       "30475             0.0000           0.4462          0.2381             0.0013   \n",
       "30476             0.2000           0.1685          0.4102             0.2142   \n",
       "30477             0.2000           0.3140          0.4268             0.0139   \n",
       "30478             0.0000           0.4051          0.3929             0.0038   \n",
       "30479             0.0000           0.5666          0.1249             0.0013   \n",
       "30480             0.0000           0.3716          0.0480             0.0000   \n",
       "\n",
       "       office_sqm_5000  trc_count_5000  trc_sqm_5000  cafe_count_5000  \\\n",
       "30471           0.0030          0.0667        0.0652           0.0072   \n",
       "30472           0.0140          0.0500        0.0504           0.0076   \n",
       "30473           0.0337          0.2167        0.2234           0.0677   \n",
       "30474           0.0000          0.0000        0.0000           0.0019   \n",
       "30475           0.0092          0.0333        0.0439           0.0076   \n",
       "30476           0.3911          0.3333        0.4965           0.2291   \n",
       "30477           0.0307          0.1250        0.1032           0.0197   \n",
       "30478           0.0086          0.0333        0.0732           0.0098   \n",
       "30479           0.0092          0.0250        0.0304           0.0049   \n",
       "30480           0.0000          0.0167        0.0048           0.0068   \n",
       "\n",
       "       cafe_count_5000_na_price  cafe_count_5000_price_500  \\\n",
       "30471                    0.0115                     0.0077   \n",
       "30472                    0.0115                     0.0062   \n",
       "30473                    0.0287                     0.0815   \n",
       "30474                    0.0000                     0.0015   \n",
       "30475                    0.0057                     0.0062   \n",
       "30476                    0.2529                     0.1785   \n",
       "30477                    0.0345                     0.0169   \n",
       "30478                    0.0057                     0.0154   \n",
       "30479                    0.0057                     0.0031   \n",
       "30480                    0.0000                     0.0031   \n",
       "\n",
       "       cafe_count_5000_price_1000  cafe_count_5000_price_1500  \\\n",
       "30471                      0.0062                      0.0125   \n",
       "30472                      0.0123                      0.0062   \n",
       "30473                      0.0988                      0.0655   \n",
       "30474                      0.0000                      0.0016   \n",
       "30475                      0.0123                      0.0078   \n",
       "30476                      0.2083                      0.2231   \n",
       "30477                      0.0216                      0.0187   \n",
       "30478                      0.0093                      0.0078   \n",
       "30479                      0.0093                      0.0047   \n",
       "30480                      0.0123                      0.0078   \n",
       "\n",
       "       cafe_count_5000_price_2500  cafe_count_5000_price_4000  \\\n",
       "30471                      0.0000                      0.0000   \n",
       "30472                      0.0027                      0.0068   \n",
       "30473                      0.0292                      0.0272   \n",
       "30474                      0.0027                      0.0136   \n",
       "30475                      0.0027                      0.0068   \n",
       "30476                      0.2626                      0.3878   \n",
       "30477                      0.0186                      0.0136   \n",
       "30478                      0.0053                      0.0136   \n",
       "30479                      0.0027                      0.0000   \n",
       "30480                      0.0053                      0.0068   \n",
       "\n",
       "       cafe_count_5000_price_high  big_church_count_5000  church_count_5000  \\\n",
       "30471                      0.0000                 0.0066             0.0400   \n",
       "30472                      0.0000                 0.0132             0.0440   \n",
       "30473                      0.0000                 0.0662             0.0840   \n",
       "30474                      0.0000                 0.0000             0.0400   \n",
       "30475                      0.0000                 0.0132             0.0480   \n",
       "30476                      0.4000                 0.1523             0.1680   \n",
       "30477                      0.0000                 0.0331             0.0560   \n",
       "30478                      0.0000                 0.0199             0.0480   \n",
       "30479                      0.0000                 0.0066             0.0280   \n",
       "30480                      0.0000                 0.0132             0.0360   \n",
       "\n",
       "       mosque_count_5000  leisure_count_5000  sport_count_5000  \\\n",
       "30471             0.5000              0.0000            0.0642   \n",
       "30472             0.0000              0.0094            0.0550   \n",
       "30473             0.0000              0.0943            0.3257   \n",
       "30474             0.0000              0.0000            0.0092   \n",
       "30475             0.0000              0.0094            0.0505   \n",
       "30476             0.5000              0.1226            0.5642   \n",
       "30477             0.0000              0.0283            0.0780   \n",
       "30478             0.0000              0.0000            0.0275   \n",
       "30479             0.0000              0.0000            0.0321   \n",
       "30480             0.0000              0.0000            0.0321   \n",
       "\n",
       "       market_count_5000   year  year_month  living_area_ratio  \\\n",
       "30471             0.0476 1.0000          47             0.0056   \n",
       "30472             0.0476 1.0000          47             0.0063   \n",
       "30473             0.5238 1.0000          47             0.0065   \n",
       "30474             0.0000 1.0000          47             0.0061   \n",
       "30475             0.0476 1.0000          47             0.0106   \n",
       "30476             0.3333 1.0000          47             0.0063   \n",
       "30477             0.0952 1.0000          47             0.0063   \n",
       "30478             0.1429 1.0000          47             0.0063   \n",
       "30479             0.0000 1.0000          47             0.0066   \n",
       "30480             0.0000 1.0000          47             0.0106   \n",
       "\n",
       "       non_living_area  non_living_area_ratio  room_area_avg  relative_floor  \\\n",
       "30471           0.5839                 0.9944         0.0083          0.0060   \n",
       "30472           0.5839                 0.9937         0.0074          0.0127   \n",
       "30473           0.5837                 0.9935         0.0050          0.0162   \n",
       "30474           0.5846                 0.9939         0.0072          0.0270   \n",
       "30475           0.5825                 0.9894         0.0160          0.0270   \n",
       "30476           0.5839                 0.9937         0.0074          0.5676   \n",
       "30477           0.5839                 0.9937         0.0074          0.0238   \n",
       "30478           0.5839                 0.9937         0.0074          0.0160   \n",
       "30479           0.5838                 0.9934         0.0057          0.0203   \n",
       "30480           0.5825                 0.9894         0.0174          0.0086   \n",
       "\n",
       "       sub_area_building_height_avg  sub_area_kremlin_dist_avg  \\\n",
       "30471                        0.5248                     0.3324   \n",
       "30472                        0.7023                     0.3214   \n",
       "30473                        0.2586                     0.1357   \n",
       "30474                        0.3331                     0.3632   \n",
       "30475                        0.7023                     0.3214   \n",
       "30476                        0.4846                     0.0964   \n",
       "30477                        0.6719                     0.2395   \n",
       "30478                        0.4895                     0.2961   \n",
       "30479                        0.3672                     0.3590   \n",
       "30480                        0.7023                     0.3214   \n",
       "\n",
       "       sales_year_month      price_doc  \n",
       "30471            0.2309 5,112,861.2160  \n",
       "30472            0.2309 8,251,718.1752  \n",
       "30473            0.2309 5,319,710.9437  \n",
       "30474            0.2309 6,045,199.6659  \n",
       "30475            0.2309 5,063,400.4229  \n",
       "30476            0.2309 7,999,563.0845  \n",
       "30477            0.2309 4,340,339.2936  \n",
       "30478            0.2309 4,259,547.0554  \n",
       "30479            0.2309 3,619,208.6025  \n",
       "30480            0.2309 4,425,364.4423  \n",
       "\n",
       "[10 rows x 250 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# populate price_doc column with predicted prices\n",
    "testRF['price_doc'] = np.expm1(test_pred)\n",
    "testRF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdc11157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check output dir\n",
    "if not os.path.exists(\"./output_models\"):\n",
    "    os.mkdir(\"./output_models\")\n",
    "\n",
    "# write id and predicted price_doc columns to csv\n",
    "testRF['id'] = test_id\n",
    "testRF[['id', 'price_doc']].to_csv('./output_models/output_random_forest.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1aee11",
   "metadata": {},
   "source": [
    "## 2. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f77b10e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 14 candidates, totalling 42 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=DecisionTreeRegressor(), n_jobs=-1,\n",
       "             param_grid={'max_depth': [5, 10, 11, 12, 13, 14, 15, 20, 25, 30,\n",
       "                                       35, 40, 45, 50]},\n",
       "             verbose=10)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtModel = DecisionTreeRegressor()\n",
    "\n",
    "# Define 'max_depth' params to use for tuning GridSearchCV\n",
    "paramDepth ={'max_depth' : [5, 10, 11, 12, 13, 14, 15, 20, 25, 30, 35, 40, 45, 50]}\n",
    "\n",
    "gridSearchModel = GridSearchCV(dtModel, param_grid = paramDepth, verbose = 10, cv = 3, n_jobs = -1)\n",
    "gridSearchModel.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c43280e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameter values: \n",
      "{'max_depth': 5}\n",
      "\r\n",
      "Best Score: \n",
      "0.33285913552258484\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameter values: \")\n",
    "print(gridSearchModel.best_params_)\n",
    "print(\"\\r\\nBest Score: \")\n",
    "print(gridSearchModel.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca10fc56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=5, random_state=42)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tune the Model with the best hyperparameters\n",
    "dtModel = DecisionTreeRegressor(\n",
    "    max_depth = gridSearchModel.best_params_['max_depth'],\n",
    "    random_state = 42\n",
    ")\n",
    "dtModel.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25a872f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Train\n",
      "MSE : 0.2341\n",
      "RMSE: 0.4838\n"
     ]
    }
   ],
   "source": [
    "# predict on train data\n",
    "yPred = dtModel.predict(x_tr)\n",
    "trainMse = mean_squared_error(y_tr, yPred)\n",
    "print(\"Predict Train\")\n",
    "print(\"MSE : %.4f\" % trainMse)\n",
    "print(\"RMSE: %.4f\" % sqrt(trainMse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9853e7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Cross Validation\n",
      "MSE : 0.2449\n",
      "RMSE: 0.4949\n"
     ]
    }
   ],
   "source": [
    "# predict on cross validation data\n",
    "yPred = dtModel.predict(x_cv)\n",
    "testMse = mean_squared_error(y_cv, yPred)\n",
    "print(\"Predict Cross Validation\")\n",
    "print(\"MSE : %.4f\" % testMse)\n",
    "print(\"RMSE: %.4f\" % sqrt(testMse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35e1d39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to result array\n",
    "resArr.append([\"Decision Tree\", str(\"%.4f\" % trainMse), str(\"%.4f\" % testMse), str(\"%.4f\" % sqrt(trainMse)), str(\"%.4f\" % sqrt(testMse))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc22cc7",
   "metadata": {},
   "source": [
    "### Predict on test.csv test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ebf3f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same prep: process.. run pred\n",
    "testDT = xTest.copy()\n",
    "testId = testDT[\"id\"]\n",
    "testDT.drop([\"id\", \"timestamp\"], axis = 1, inplace = True)\n",
    "\n",
    "# process datatype\n",
    "categoricals = xTrain.select_dtypes(include = [\"object\"]).copy()\n",
    "numericals = xTrain.select_dtypes(exclude = [\"object\"])\n",
    "\n",
    "for c in categoricals:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(xTrain[c])\n",
    "\n",
    "    testDT[c] = testDT[c].map(lambda s: \"<unknown>\" if s not in le.classes_ else s)\n",
    "    le.classes_ = np.append(le.classes_, \"<unknown>\")\n",
    "\n",
    "    testDT[c] = le.transform(testDT[c])\n",
    "\n",
    "for n in numericals:\n",
    "    min = xTrain[n].min()\n",
    "    max = xTrain[n].max()\n",
    "\n",
    "    testDT[n] = (testDT[n] - min) / (max - min)\n",
    "\n",
    "# predict using decision tree model\n",
    "testPred = dtModel.predict(testDT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4328ecec",
   "metadata": {},
   "source": [
    "**Output prediction to csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3cec5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate price_doc column with predicted prices\n",
    "testDT[\"price_doc\"] = np.expm1(testPred)\n",
    "testDT[\"id\"] = testId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e05f639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check output dir\n",
    "if not os.path.exists(\"./output_models\"):\n",
    "    os.mkdir(\"./output_models\")\n",
    "\n",
    "# write id and predicted price_doc columns to csv\n",
    "testDT[[\"id\", \"price_doc\"]].to_csv(\"./output_models/output_decisiontree.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba925d",
   "metadata": {},
   "source": [
    "## 3. XGBoost (eXtreme Gradient Boost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1d7628",
   "metadata": {},
   "source": [
    "**Tune Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "126c649c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 15 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=2,\n",
       "                   estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                          colsample_bylevel=None,\n",
       "                                          colsample_bynode=None,\n",
       "                                          colsample_bytree=None,\n",
       "                                          enable_categorical=False, gamma=None,\n",
       "                                          gpu_id=None, importance_type=None,\n",
       "                                          interaction_constraints=None,\n",
       "                                          learning_rate=None,\n",
       "                                          max_delta_step=None, max_depth=None,\n",
       "                                          min_child_weight=None, missing=nan,\n",
       "                                          monotone_constraints=...\n",
       "                                          reg_alpha=None, reg_lambda=None,\n",
       "                                          scale_pos_weight=None, subsample=None,\n",
       "                                          tree_method=None,\n",
       "                                          validate_parameters=None,\n",
       "                                          verbosity=None),\n",
       "                   n_iter=15, n_jobs=-1,\n",
       "                   param_distributions={'colsample_bytree': [0.1, 0.25, 0.3,\n",
       "                                                             0.5, 0.75, 0.8,\n",
       "                                                             1],\n",
       "                                        'learning_rate': [0.01, 0.03, 0.05,\n",
       "                                                          0.07, 0.1, 0.13,\n",
       "                                                          0.15],\n",
       "                                        'max_depth': [10, 11, 12, 13, 14, 15],\n",
       "                                        'n_estimators': [50, 100, 150, 200],\n",
       "                                        'subsample': [0.1, 0.3, 0.5, 0.7, 1]},\n",
       "                   verbose=10)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbModel = XGBRegressor(nthread = -1)\n",
    "\n",
    "# define parameter distributions to use for tuning\n",
    "paramDist = {\n",
    "    \"colsample_bytree\": [0.1, 0.25, 0.3, 0.5, 0.75, 0.8, 1],\n",
    "    \"learning_rate\": [0.01, 0.03, 0.05, 0.07, 0.1, 0.13, 0.15],\n",
    "    \"max_depth\": [10, 11, 12, 13, 14, 15],\n",
    "    \"n_estimators\": [50, 100, 150, 200],\n",
    "    \"subsample\": [0.1, 0.3, 0.5, 0.7, 1]\n",
    "}\n",
    "\n",
    "# use RandomizedSearchCV to determine best values for hyperparameters\n",
    "randomSearchModel = RandomizedSearchCV(xgbModel, param_distributions = paramDist, verbose = 10, n_jobs = -1, cv = 2, n_iter = 15)\n",
    "randomSearchModel.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce73e23b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparamter values: \n",
      "{'subsample': 1, 'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.07, 'colsample_bytree': 0.75}\n",
      "\r\n",
      "Best Score: \n",
      "0.38276085596262455\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Hyperparamter values: \")\n",
    "print(randomSearchModel.best_params_)\n",
    "print(\"\\r\\nBest Score: \")\n",
    "print(randomSearchModel.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb979c6",
   "metadata": {},
   "source": [
    "**Use best Hyperparameters in model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "03e49f20",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.75,\n",
       "             enable_categorical=False, gamma=0, gpu_id=-1, importance_type=None,\n",
       "             interaction_constraints='', learning_rate=0.07, max_delta_step=0,\n",
       "             max_depth=10, min_child_weight=1, missing=nan,\n",
       "             monotone_constraints='()', n_estimators=100, n_jobs=4,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tune Model\n",
    "xgbModel = XGBRegressor(\n",
    "    colsample_bytree = randomSearchModel.best_params_[\"colsample_bytree\"],\n",
    "    learning_rate = randomSearchModel.best_params_[\"learning_rate\"],\n",
    "    max_depth = randomSearchModel.best_params_[\"max_depth\"],\n",
    "    n_estimators = randomSearchModel.best_params_[\"n_estimators\"],\n",
    "    subsample = randomSearchModel.best_params_[\"subsample\"]\n",
    ")\n",
    "# fit model\n",
    "xgbModel.fit(x_tr, y_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a45fac05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Train\n",
      "MSE : 0.0815\n",
      "RMSE: 0.2855\n"
     ]
    }
   ],
   "source": [
    "# predict on train data\n",
    "yPred = xgbModel.predict(x_tr)\n",
    "trainMse = mean_squared_error(y_tr, yPred)\n",
    "print(\"Predict Train\")\n",
    "print(\"MSE : %.4f\" % trainMse)\n",
    "print(\"RMSE: %.4f\" % sqrt(trainMse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9cf6b7a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Cross Validation\n",
      "MSE : 0.2212\n",
      "RMSE: 0.4703\n"
     ]
    }
   ],
   "source": [
    "# predict on cross validation data\n",
    "yPred = xgbModel.predict(x_cv)\n",
    "testMse = mean_squared_error(y_cv, yPred)\n",
    "print(\"Predict Cross Validation\")\n",
    "print(\"MSE : %.4f\" % testMse)\n",
    "print(\"RMSE: %.4f\" % sqrt(testMse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ddfc29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to result array\n",
    "resArr.append([\"XGBoost\", str(\"%.4f\" % trainMse), str(\"%.4f\" % testMse), str(\"%.4f\" % sqrt(trainMse)), str(\"%.4f\" % sqrt(testMse))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c77a73a",
   "metadata": {},
   "source": [
    "### Predict on test.csv test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "30cd62b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same prep: process.. run pred\n",
    "testXGB = xTest.copy()\n",
    "testId = testXGB[\"id\"]\n",
    "testXGB.drop([\"id\", \"timestamp\"], axis = 1, inplace = True)\n",
    "\n",
    "# process datatype\n",
    "categoricals = xTrain.select_dtypes(include = [\"object\"]).copy()\n",
    "numericals = xTrain.select_dtypes(exclude = [\"object\"])\n",
    "\n",
    "for c in categoricals:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(xTrain[c])\n",
    "\n",
    "    testXGB[c] = testXGB[c].map(lambda s: \"<unknown>\" if s not in le.classes_ else s)\n",
    "    le.classes_ = np.append(le.classes_, \"<unknown>\")\n",
    "\n",
    "    testXGB[c] = le.transform(testXGB[c])\n",
    "\n",
    "for n in numericals:\n",
    "    min = xTrain[n].min()\n",
    "    max = xTrain[n].max()\n",
    "\n",
    "    testXGB[n] = (testXGB[n] - min) / (max - min)\n",
    "\n",
    "# predict using xgb model\n",
    "testPred = xgbModel.predict(testXGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040cacb5",
   "metadata": {},
   "source": [
    "**Output prediction to csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "877c721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate price_doc column with predicted prices\n",
    "testXGB[\"price_doc\"] = np.expm1(testPred)\n",
    "testXGB[\"id\"] = testId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "80dc0afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check output dir\n",
    "if not os.path.exists(\"./output_models\"):\n",
    "    os.mkdir(\"./output_models\")\n",
    "\n",
    "# write id and predicted price_doc columns to csv\n",
    "testXGB[[\"id\", \"price_doc\"]].to_csv(\"./output_models/output_xgboost.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ff8c01",
   "metadata": {},
   "source": [
    "## 4. SGD (Stochastic Gradient Descent) Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe6f6dd",
   "metadata": {},
   "source": [
    "**Tune Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4be9cc68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=SGDRegressor(), n_iter=15, n_jobs=-1,\n",
       "                   param_distributions={'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0,\n",
       "                                                  10.0, 100.0, 1000.0,\n",
       "                                                  10000.0],\n",
       "                                        'learning_rate': ['optimal'],\n",
       "                                        'loss': ['squared_loss'],\n",
       "                                        'max_iter': [500, 1000, 1500, 2000],\n",
       "                                        'penalty': ['l2']},\n",
       "                   verbose=10)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgdModel = SGDRegressor()\n",
    "\n",
    "# define parameter distributions to use for tuning\n",
    "paramDist = {\n",
    "    \"alpha\": [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4],\n",
    "    \"learning_rate\": ['optimal'],\n",
    "    \"loss\": [\"squared_loss\"],\n",
    "    \"max_iter\": [500, 1000, 1500, 2000],\n",
    "    \"penalty\": [\"l2\"]\n",
    "}\n",
    "\n",
    "# use RandomizedSearchCV to determine best values for hyperparameters\n",
    "randomSearchModel = RandomizedSearchCV(sgdModel, param_distributions = paramDist, verbose = 10, n_jobs = -1, cv=3, n_iter = 15)\n",
    "randomSearchModel.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "252d7c00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparamter values: \n",
      "{'penalty': 'l2', 'max_iter': 1500, 'loss': 'squared_loss', 'learning_rate': 'optimal', 'alpha': 10000.0}\n",
      "\r\n",
      "Best Score: \n",
      "-244.8534832031947\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Hyperparamter values: \")\n",
    "print(randomSearchModel.best_params_)\n",
    "print(\"\\r\\nBest Score: \")\n",
    "print(randomSearchModel.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c465e",
   "metadata": {},
   "source": [
    "**Use best Hyperparameters in model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e48b0519",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=10000.0, learning_rate='optimal', loss='squared_loss',\n",
       "             max_iter=1500)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tune Model\n",
    "sgdModel = SGDRegressor(\n",
    "    alpha = randomSearchModel.best_params_[\"alpha\"],\n",
    "    learning_rate = \"optimal\",\n",
    "    loss = \"squared_loss\",\n",
    "    max_iter = randomSearchModel.best_params_[\"max_iter\"],\n",
    "    penalty = \"l2\",\n",
    ")\n",
    "# fit model\n",
    "sgdModel.fit(x_tr, y_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f43745a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Train\n",
      "MSE : 84.5541\n",
      "RMSE: 9.1953\n"
     ]
    }
   ],
   "source": [
    "# predict on train data\n",
    "yPred = sgdModel.predict(x_tr)\n",
    "trainMse = mean_squared_error(y_tr, yPred)\n",
    "print(\"Predict Train\")\n",
    "print(\"MSE : %.4f\" % trainMse)\n",
    "print(\"RMSE: %.4f\" % sqrt(trainMse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b7d9ace",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Cross Validation\n",
      "MSE : 83.8895\n",
      "RMSE: 9.1591\n"
     ]
    }
   ],
   "source": [
    "# predict on cross validation data\n",
    "yPred = sgdModel.predict(x_cv)\n",
    "testMse = mean_squared_error(y_cv, yPred)\n",
    "print(\"Predict Cross Validation\")\n",
    "print(\"MSE : %.4f\" % testMse)\n",
    "print(\"RMSE: %.4f\" % sqrt(testMse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e3d179cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to result array\n",
    "resArr.append([\"SGD\", str(\"%.4f\" % trainMse), str(\"%.4f\" % testMse), str(\"%.4f\" % sqrt(trainMse)), str(\"%.4f\" % sqrt(testMse))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f729604f",
   "metadata": {},
   "source": [
    "### Predict on test.csv test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "55f165f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same prep: process.. run pred\n",
    "testSGD = xTest.copy()\n",
    "testId = testSGD[\"id\"]\n",
    "testSGD.drop([\"id\", \"timestamp\"], axis = 1, inplace = True)\n",
    "\n",
    "# process datatype\n",
    "categoricals = xTrain.select_dtypes(include = [\"object\"]).copy()\n",
    "numericals = xTrain.select_dtypes(exclude = [\"object\"])\n",
    "\n",
    "for c in categoricals:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(xTrain[c])\n",
    "\n",
    "    testSGD[c] = testSGD[c].map(lambda s: \"<unknown>\" if s not in le.classes_ else s)\n",
    "    le.classes_ = np.append(le.classes_, \"<unknown>\")\n",
    "\n",
    "    testSGD[c] = le.transform(testSGD[c])\n",
    "\n",
    "for n in numericals:\n",
    "    min = xTrain[n].min()\n",
    "    max = xTrain[n].max()\n",
    "\n",
    "    testSGD[n] = (testSGD[n] - min) / (max - min)\n",
    "\n",
    "# predict using sgd model\n",
    "testPred = sgdModel.predict(testSGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ae5e24",
   "metadata": {},
   "source": [
    "**Output prediction to csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "22f4e66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate price_doc column with predicted prices\n",
    "testSGD[\"price_doc\"] = np.expm1(testPred)\n",
    "testSGD[\"id\"] = testId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4f029173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check output dir\n",
    "if not os.path.exists(\"./output_models\"):\n",
    "    os.mkdir(\"./output_models\")\n",
    "\n",
    "# write id and predicted price_doc columns to csv\n",
    "testSGD[[\"id\", \"price_doc\"]].to_csv(\"./output_models/output_sgd.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f778aeb",
   "metadata": {},
   "source": [
    "## 5. AdaBoost (Adaptive Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04b2a41",
   "metadata": {},
   "source": [
    "**Tune 'n_estimators' parameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a01e10c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 6 is smaller than n_iter=10. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=AdaBoostRegressor(), n_jobs=-1,\n",
       "                   param_distributions={'n_estimators': [50, 100, 150, 200, 250,\n",
       "                                                         300]},\n",
       "                   verbose=10)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adbModel = AdaBoostRegressor()\n",
    "\n",
    "# define 'n_estimators' distributions to use for tuning\n",
    "paramDist = { \"n_estimators\": [50, 100, 150, 200, 250, 300] }\n",
    "\n",
    "# use RandomizedSearchCV to determine best value for 'n_estimators'\n",
    "randomSearchModel = RandomizedSearchCV(adbModel, param_distributions = paramDist, verbose = 10, n_jobs = -1, cv = 3)\n",
    "randomSearchModel.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "de0a2cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best 'n_estimators' value: \n",
      "50\n",
      "\r\n",
      "Best Score: \n",
      "-0.4955513640903855\n"
     ]
    }
   ],
   "source": [
    "print(\"Best 'n_estimators' value: \")\n",
    "print(randomSearchModel.best_params_[\"n_estimators\"])\n",
    "print(\"\\r\\nBest Score: \")\n",
    "print(randomSearchModel.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c588c28",
   "metadata": {},
   "source": [
    "**Use best 'n_estimators' value in model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "26608d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostRegressor()"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tune Model\n",
    "adbModel = AdaBoostRegressor(n_estimators = randomSearchModel.best_params_[\"n_estimators\"])\n",
    "# fit model\n",
    "adbModel.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "18afeea2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Train\n",
      "MSE : 0.5344\n",
      "RMSE: 0.7310\n"
     ]
    }
   ],
   "source": [
    "# predict on train data\n",
    "yPred = adbModel.predict(x_tr)\n",
    "trainMse = mean_squared_error(y_tr, yPred)\n",
    "print(\"Predict Train\")\n",
    "print(\"MSE : %.4f\" % trainMse)\n",
    "print(\"RMSE: %.4f\" % sqrt(trainMse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8352d49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Cross Validation\n",
      "MSE : 0.5288\n",
      "RMSE: 0.7272\n"
     ]
    }
   ],
   "source": [
    "# predict on cross validation data\n",
    "yPred = adbModel.predict(x_cv)\n",
    "testMse = mean_squared_error(y_cv, yPred)\n",
    "print(\"Predict Cross Validation\")\n",
    "print(\"MSE : %.4f\" % testMse)\n",
    "print(\"RMSE: %.4f\" % sqrt(testMse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5b9c6368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25900"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d132d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to result array\n",
    "resArr.append([\"AdaBoost\", str(\"%.4f\" % trainMse), str(\"%.4f\" % testMse), str(\"%.4f\" % sqrt(trainMse)), str(\"%.4f\" % sqrt(testMse))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d39544",
   "metadata": {},
   "source": [
    "### Predict on test.csv test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2508d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same prep: process.. run pred\n",
    "testAdB = xTest.copy()\n",
    "testId = testAdB[\"id\"]\n",
    "testAdB.drop([\"id\", \"timestamp\"], axis = 1, inplace = True)\n",
    "\n",
    "# process datatype\n",
    "categoricals = xTrain.select_dtypes(include = [\"object\"]).copy()\n",
    "numericals = xTrain.select_dtypes(exclude = [\"object\"])\n",
    "\n",
    "for c in categoricals:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(xTrain[c])\n",
    "\n",
    "    testAdB[c] = testAdB[c].map(lambda s: \"<unknown>\" if s not in le.classes_ else s)\n",
    "    le.classes_ = np.append(le.classes_, \"<unknown>\")\n",
    "\n",
    "    testAdB[c] = le.transform(testAdB[c])\n",
    "\n",
    "for n in numericals:\n",
    "    min = xTrain[n].min()\n",
    "    max = xTrain[n].max()\n",
    "\n",
    "    testAdB[n] = (testAdB[n] - min) / (max - min)\n",
    "\n",
    "# predict using adb model\n",
    "testPred = adbModel.predict(testAdB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8729c664",
   "metadata": {},
   "source": [
    "**Output prediction to csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e7b63d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate price_doc column with predicted prices\n",
    "testAdB[\"price_doc\"] = np.expm1(testPred)\n",
    "testAdB[\"id\"] = testId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fcf7484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check output dir\n",
    "if not os.path.exists(\"./output_models\"):\n",
    "    os.mkdir(\"./output_models\")\n",
    "\n",
    "# write id and predicted price_doc columns to csv\n",
    "testAdB[[\"id\", \"price_doc\"]].to_csv(\"./output_models/output_adaboost.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59fa221",
   "metadata": {},
   "source": [
    "# 6. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b47cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_predict(train, test, y_train):\n",
    "    RS=1\n",
    "    np.random.seed(RS)\n",
    "    ROUNDS = 1500 # 1300,1400 all works fine\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'boosting': 'gbdt',\n",
    "            'learning_rate': 0.01 , #small learn rate, large number of iterations\n",
    "            'verbose': -1,\n",
    "            'num_leaves': 2 ** 5,\n",
    "            'bagging_fraction': 0.95,\n",
    "            'bagging_freq': 1,\n",
    "            'bagging_seed': RS,\n",
    "            'feature_fraction': 0.7,\n",
    "            'feature_fraction_seed': RS,\n",
    "            'max_bin': 100,\n",
    "            'max_depth': 7,\n",
    "            'num_rounds': ROUNDS,\n",
    "    }\n",
    "    train_lgb=lgb.Dataset(train,y_train)\n",
    "    model=lgb.train(params,train_lgb,num_boost_round=ROUNDS)\n",
    "    predict=model.predict(test)\n",
    "#     predict=np.exp(predict)\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a563a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do prep: process.. run pred\n",
    "testLgbm = xTest.copy()\n",
    "test_id = testLgbm['id']\n",
    "testLgbm.drop(['id', 'timestamp'], axis = 1, inplace = True)\n",
    "\n",
    "# process datatype\n",
    "categoricals = xTrain.select_dtypes(include = [\"object\"]).copy()\n",
    "numericals = xTrain.select_dtypes(exclude = [\"object\"])\n",
    "\n",
    "for c in categoricals:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(xTrain[c])\n",
    "\n",
    "    testLgbm[c] = testLgbm[c].map(lambda s: \"<unknown>\" if s not in le.classes_ else s)\n",
    "    le.classes_ = np.append(le.classes_, \"<unknown>\")\n",
    "\n",
    "    testLgbm[c] = le.transform(testLgbm[c])\n",
    "\n",
    "for n in numericals:\n",
    "    min = xTrain[n].min()\n",
    "    max = xTrain[n].max()\n",
    "\n",
    "    testLgbm[n] = (testLgbm[n] - min) / (max - min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a22c2103",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Train\n",
      "MSE : 0.1414\n",
      "RMSE: 0.3760\n"
     ]
    }
   ],
   "source": [
    "# predict on train data\n",
    "lgbm_pred = lgbm_predict(x_tr, x_tr, y_tr)\n",
    "trainMse = mean_squared_error(y_tr, lgbm_pred)\n",
    "print(\"Predict Train\")\n",
    "print(\"MSE : %.4f\" % trainMse)\n",
    "print(\"RMSE: %.4f\" % sqrt(trainMse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2bbb2bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Cross Validation\n",
      "MSE : 0.2117\n",
      "RMSE: 0.4601\n"
     ]
    }
   ],
   "source": [
    "# predict on cross validation data\n",
    "lgbm_pred = lgbm_predict(x_tr, x_cv, y_tr)\n",
    "testMse = mean_squared_error(y_cv, lgbm_pred)\n",
    "print(\"Predict Cross Validation\")\n",
    "print(\"MSE : %.4f\" % testMse)\n",
    "print(\"RMSE: %.4f\" % sqrt(testMse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2df1fb67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16.37145868, 15.52221449, 15.21597236, ..., 15.19071261,\n",
       "       15.74439144, 15.82490189])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5f0650",
   "metadata": {},
   "source": [
    "### Predict on test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "343ad38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    }
   ],
   "source": [
    "lgbm_pred = lgbm_predict(x_tr, testLgbm, y_tr)\n",
    "lgbm_output=pd.DataFrame({'id':test_id,'price_doc':np.expm1(lgbm_pred)})\n",
    "lgbm_output.to_csv('./output_models/output_lgbm.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeb6339",
   "metadata": {},
   "source": [
    "## Test raw data on naive xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2d750cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrainnxgb = dfs[\"train\"].drop([\"id\", \"timestamp\", \"price_doc\"], axis=1).select_dtypes(include=['number','bool','category'])\n",
    "ytrainnxgb = dfs['train']['price_doc'].copy()\n",
    "testnxgb = dfs['test'].drop([\"id\", \"timestamp\"], axis=1).select_dtypes(include=['number','bool','category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e111be9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 20 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "6 fits failed out of a total of 40.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "6 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\xgboost\\core.py\", line 506, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\xgboost\\sklearn.py\", line 789, in fit\n",
      "    self._Booster = train(\n",
      "  File \"D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\xgboost\\training.py\", line 188, in train\n",
      "    bst = _train_internal(params, dtrain,\n",
      "  File \"D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\xgboost\\training.py\", line 81, in _train_internal\n",
      "    bst.update(dtrain, i, obj)\n",
      "  File \"D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\xgboost\\core.py\", line 1680, in update\n",
      "    _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n",
      "  File \"D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\xgboost\\core.py\", line 218, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 1.4 for Parameter colsample_bytree exceed bound [0,1]\n",
      "colsample_bytree: Subsample ratio of columns, resample on each tree construction.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.57890574 0.60332469 0.60427743        nan 0.5726411  0.58415647\n",
      "        nan 0.56943836 0.59082337 0.57101843 0.59693419 0.58920857\n",
      " 0.57569573 0.56912861        nan 0.57863564 0.58603778 0.5856402\n",
      " 0.58630732 0.5880956 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:01:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=2,\n",
       "                   estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                          colsample_bylevel=None,\n",
       "                                          colsample_bynode=None,\n",
       "                                          colsample_bytree=None,\n",
       "                                          enable_categorical=False, eta=0.05,\n",
       "                                          eval_metric='rmse', gamma=0,\n",
       "                                          gpu_id=None, importance_type=None,\n",
       "                                          interaction_constraints=None,\n",
       "                                          learning_rate=None,\n",
       "                                          max_delta_step=None, max_depth=None,\n",
       "                                          min_child_weight=None, missing=...\n",
       "                                          reg_alpha=None, reg_lambda=None,\n",
       "                                          scale_pos_weight=None, subsample=None,\n",
       "                                          tree_method=None, ...),\n",
       "                   n_iter=20, n_jobs=-1,\n",
       "                   param_distributions={'colsample_bytree': [0.55, 0.7, 0.8,\n",
       "                                                             0.85, 0.9, 1,\n",
       "                                                             1.4],\n",
       "                                        'learning_rate': [0.07, 0.08, 0.1, 0.12,\n",
       "                                                          0.15],\n",
       "                                        'max_depth': [2, 3, 4, 5, 6, 8],\n",
       "                                        'min_child_weight': [3, 4, 5, 6],\n",
       "                                        'n_estimators': [200, 220, 240, 300],\n",
       "                                        'subsample': [0.7, 0.8, 0.9, 1]},\n",
       "                   verbose=10)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbModel = XGBRegressor(eta=0.05, nthread = -1, objective='reg:linear',\n",
    "    eval_metric='rmse', gamma=0)\n",
    "\n",
    "# define parameter distributions to use for tuning\n",
    "paramDist = {\n",
    "    \"colsample_bytree\": [0.55, 0.7, 0.8, 0.85, 0.9, 1, 1.4],\n",
    "    \"learning_rate\": [0.07, 0.08, 0.1, 0.12, 0.15],\n",
    "    \"max_depth\": [2, 3, 4, 5, 6, 8],\n",
    "    \"n_estimators\": [200, 220, 240, 300],\n",
    "    \"subsample\": [0.7, 0.8, 0.9, 1],\n",
    "    \"min_child_weight\": [3, 4, 5, 6]\n",
    "}\n",
    "\n",
    "# use RandomizedSearchCV to determine best values for hyperparameters\n",
    "randomSearchModel = RandomizedSearchCV(xgbModel, param_distributions = paramDist, verbose = 10, n_jobs = -1, cv = 2, n_iter = 20)\n",
    "randomSearchModel.fit(xtrainnxgb, ytrainnxgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1463ea04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\pandas\\core\\indexing.py:1745: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_params = {\n",
    "    'eta': 0.05,\n",
    "    'learning_rate': randomSearchModel.best_params_[\"learning_rate\"],\n",
    "    'max_depth': randomSearchModel.best_params_[\"max_depth\"],\n",
    "    'min_child_weight': randomSearchModel.best_params_[\"min_child_weight\"],\n",
    "    'subsample': randomSearchModel.best_params_[\"subsample\"],\n",
    "    'n_estimators': randomSearchModel.best_params_[\"n_estimators\"],\n",
    "    'colsample_bytree': randomSearchModel.best_params_[\"colsample_bytree\"],\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'silent': 1,\n",
    "    'gamma': 0\n",
    "}\n",
    "\n",
    "xtrainnxgb = dfs[\"train\"]\n",
    "xtrainnxgb=xtrainnxgb[(xtrainnxgb.price_doc>1e6) & (xtrainnxgb.price_doc!=2e6) & (xtrainnxgb.price_doc!=3e6)]\n",
    "xtrainnxgb.loc[(xtrainnxgb.product_type=='Investment') & (xtrainnxgb.build_year<2000),'price_doc']*=0.895 \n",
    "xtrainnxgb.loc[xtrainnxgb.product_type!='Investment','price_doc']*=0.96 #Louis/Andy's magic number\n",
    "\n",
    "ytrainnxgb = xtrainnxgb['price_doc'].copy()\n",
    "xtrainnxgb = xtrainnxgb.drop([\"id\", \"timestamp\", \"price_doc\"], axis=1).select_dtypes(include=['number','bool','category'])\n",
    "testnxgb = dfs['test'].drop([\"id\", \"timestamp\"], axis=1).select_dtypes(include=['number','bool','category'])\n",
    "\n",
    "for c in xtrainnxgb.columns:\n",
    "    if xtrainnxgb[c].dtype == 'object':\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(xtrainnxgb[c].values)) \n",
    "        xtrainnxgb[c] = lbl.transform(list(xtrainnxgb[c].values))\n",
    "        #x_train.drop(c,axis=1,inplace=True)\n",
    "        \n",
    "for c in testnxgb.columns:\n",
    "    if testnxgb[c].dtype == 'object':\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(testnxgb[c].values)) \n",
    "        testnxgb[c] = lbl.transform(list(testnxgb[c].values))\n",
    "        #x_test.drop(c,axis=1,inplace=True)        \n",
    "\n",
    "\n",
    "dtrain = xgb.DMatrix(xtrainnxgb, ytrainnxgb)\n",
    "dtest = xgb.DMatrix(testnxgb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "471581cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:07:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:07:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\", \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[23:07:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:07:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\", \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[23:07:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:07:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\", \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:7936025.66667\ttest-rmse:7943398.16667\n",
      "[50]\ttrain-rmse:1633129.00000\ttest-rmse:2296242.25000\n",
      "[100]\ttrain-rmse:1348577.37500\ttest-rmse:2199109.41667\n",
      "[150]\ttrain-rmse:1225926.87500\ttest-rmse:2171942.83333\n",
      "[200]\ttrain-rmse:1139934.00000\ttest-rmse:2161306.91667\n",
      "[250]\ttrain-rmse:1070194.70833\ttest-rmse:2155048.33333\n",
      "[300]\ttrain-rmse:1014621.91667\ttest-rmse:2152100.58333\n",
      "[346]\ttrain-rmse:970130.87500\ttest-rmse:2150877.95833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEDCAYAAAAcI05xAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAArEElEQVR4nO3deXxU9b3/8ddnlsxksi9DCGsAkSUsAQOitFZcELTuXbxqr3aRtvqzV/urVX/1Vv11U+vPa73VtmiV9mqtFtdWaxELolZFNjFsAsomS8KSkD2zfH9/nElIIMsEMjlnks/z8ZjHnDnnzJlPZuB9vvOdc75HjDEopZRyLpfdBSillOqcBrVSSjmcBrVSSjmcBrVSSjmcBrVSSjmcBrVSSjlcwoJaRB4XkXIRKYtz/a+IyHoRWScif0pUXUoplWwkUcdRi8gZQA3wR2PMhC7WHQ08C5xljDkkIgOMMeUJKUwppZJMwlrUxphlwMHW80RklIi8JiIrReQtERkbW3Qd8LAx5lDsuRrSSikV09t91POBG40xpwA/AB6JzT8ZOFlE3hGR90RkTi/XpZRSjuXprRcSkXTgdOAvItI829eqjtHAmcAQ4C0RmWCMqeyt+pRSyql6LaixWu+VxpiSdpbtAt4zxoSAT0VkE1Zwf9CL9SmllCP1WteHMeYwVgh/GUAsk2OLXwRmxebnY3WFfNJbtSmllJMl8vC8p4F3gTEisktEvglcBXxTRD4E1gEXx1b/B3BARNYDS4BbjDEHElWbUkolk4QdnqeUUqpn6JmJSinlcAn5MTE/P98UFRUlYtNKKdUnrVy5cr8xJtjesoQEdVFREStWrEjEppVSqk8Ske0dLdOuD6WUcjgNaqWUcjgNaqWUcri4+qhF5GbgW4ABPgK+boxpSGRhSvV3oVCIXbt20dCg/9X6Er/fz5AhQ/B6vXE/p8ugFpHBwPeA8caYehF5FrgCWHC8hSqlurZr1y4yMjIoKiqi1fg4KokZYzhw4AC7du1ixIgRcT8v3q4PD5AqIh4gAOw+jhqVUt3Q0NBAXl6ehnQfIiLk5eV1+1tSl0FtjPkMuB/YAewBqowxi46rSqVUt2hI9z3H85l2GdQikoM1JscIYBCQJiJXt7PePBFZISIrKioqul2IiUb59Lk72fXBX7v9XKWU6svi6fo4B/jUGFMRG4b0eaxxpdswxsw3xpQaY0qDwXZPrumUuFzkr/0du1e83O3nKqV6XmVlJY888kjXKx7l/PPPp7KysucL6sfiCeodwAwRCYjVZj8b2JCIYqolA3dDZSI2rZTqpo6COhKJdPq8V199lezs7ON6TWMM0Wj0uJ7bl8XTR/0+sBBYhXVongvrklo9rsadibepKhGbVkp102233cbWrVspKSlh2rRpzJo1iyuvvJKJEycCcMkll3DKKadQXFzM/PlHIqGoqIj9+/ezbds2xo0bx3XXXUdxcTGzZ8+mvr7+mNdpXu/6669n6tSpvPXWW4wdO5ZvfetbTJgwgauuuorFixczc+ZMRo8ezfLlywF48803KSkpoaSkhClTplBdXQ3AL3/5S6ZNm8akSZO488472/3b7rrrLq655hpmz55NUVERzz//PD/84Q+ZOHEic+bMIRQKAbBy5Uq+8IUvcMopp3DeeeexZ88eAB599FGmTZvG5MmTufzyy6mrqwPg2muv5Xvf+x6nn346I0eOZOHChT3yWcR1HLUx5k6g/b+4BzV4MvGHNaiVOtrdf13H+t2He3Sb4wdlcueFxR0uv+eeeygrK2PNmjUsXbqUCy64gLKyspbDyh5//HFyc3Opr69n2rRpXH755eTl5bXZxubNm3n66ad59NFH+cpXvsJzzz3H1Vcf8xMXmzZt4oknnuCRRx5h27ZtbNmyhb/85S/Mnz+fadOm8ac//Ym3336bl19+mZ///Oe8+OKL3H///Tz88MPMnDmTmpoa/H4/ixYtYvPmzSxfvhxjDBdddBHLli3jjDPOOOY1t27dypIlS1i/fj2nnXYazz33HPfddx+XXnopr7zyChdccAE33ngjL730EsFgkGeeeYYf/ehHPP7441x22WVcd911ANxxxx38/ve/58YbbwRgz549vP3222zcuJGLLrqIL33pS8f9GTXrzUtxdanRm0Ve3R67y1BKtWP69Oltjv196KGHeOGFFwDYuXMnmzdvPiaoR4wYQUlJCQCnnHIK27Zta3fbw4cPZ8aMGW2e19xyLy4u5uyzz0ZEmDhxYss2Zs6cyfe//32uuuoqLrvsMoYMGcKiRYtYtGgRU6ZMAaCmpobNmze3G9Rz587F6/UyceJEIpEIc+ZY19Rufo1NmzZRVlbGueeeC1hdPoWFhQCUlZVxxx13UFlZSU1NDeedd17Ldi+55BJcLhfjx49n3759cb23XXFUUId9OaTVVNtdhlKO01nLt7ekpaW1TC9dupTFixfz7rvvEggEOPPMM9s9Ntjn87VMu91u6uvr2blzJxdeeCEA3/nOd5gzZ06bbR/9PJfL1fLY5XIRDocBq2vmggsu4NVXX2XGjBksXrwYYwy333473/72t9ts7+GHH+bRRx8FrD701q/hcrnwer0th801v4YxhuLiYt59991j/q5rr72WF198kcmTJ7NgwQKWLl3abu09dWEWR431EfVnk2lqMZGw3aUo1e9lZGS09PseraqqipycHAKBABs3buS9996Le7tDhw5lzZo1rFmzhu985zvHXd/WrVuZOHEit956K6WlpWzcuJHzzjuPxx9/nJqaGgA+++wzysvLueGGG1pec9CgQXFtf8yYMVRUVLQEdSgUYt26dQBUV1dTWFhIKBTiqaeeOu6/IV6OalGTmotLDHU1hwhkdf8QP6VUz8nLy2PmzJlMmDCB1NRUCgoKWpbNmTOH3/72t0yaNIkxY8a06bboLQ8++CBLlizB7XYzfvx45s6di8/nY8OGDZx22mkApKen8+STTzJgwIBubz8lJYWFCxfyve99j6qqKsLhMDfddBPFxcX85Cc/4dRTT2X48OFMnDixwx1aT0nINRNLS0vN8Vw44L0XHmbGh/+Hfdf8i4IR9n/VU8pOGzZsYNy4cXaXoRKgvc9WRFYaY0rbW99RXR+edOuHiLqq/TZXopRSzuGooPZl5ANQf7jc5kqUUso5HBXUqdlWv3So+qDNlSillHM4KqjTs60WdbjmgM2VKKWUczgqqLNzgkSNEK3TFrVSSjVzVFD7fSkcJoCr/pDdpSillGM4KqghNoJeowa1UnY73mFOwTrGuXmgInXiHBfUtTqCnlKO0BtB3Xw6uOqc44K6zpOlI+gp5QCthzm95ZZb2h0+tLa2lgsuuIDJkyczYcIEnnnmGR566CF2797NrFmzmDVr1jHbXbBgAV/+8pe58MILmT17NgsWLOCSSy7hwgsvZMSIEfz617/mgQceYMqUKcyYMYODB63frB566CHGjx/PpEmTuOKKK1pe/xvf+AbTpk1jypQpvPTSS+3+LWeeeSY333wzZ5xxBuPGjeODDz7gsssuY/To0dxxxx0t6z355JNMnz6dkpISvv3tb7eMvf3d736X0tJSiouL2wydWlRUxJ133snUqVOZOHEiGzdu7Jk3/yjOOoUcaPJmEqjbYXcZSjnL32+DvR/17DYHToS593S4uPUwp4sWLWLhwoXHDB9aUVHBoEGDeOWVVwBrDJCsrCweeOABlixZQn5+frvbfvfdd1m7di25ubksWLCAsrIyVq9eTUNDAyeddBL33nsvq1ev5uabb+aPf/wjN910E/fccw+ffvopPp+v5QoyP/vZzzjrrLN4/PHHqaysZPr06ZxzzjnHDPIE1inhy5Yt41e/+hUXX3wxK1euJDc3l1GjRnHzzTdTXl7OM888wzvvvIPX6+X666/nqaee4t///d/52c9+Rm5uLpFIhLPPPpu1a9cyadIkAPLz81m1ahWPPPII999/P4899tgJfjDHclyLOuzLJiOqI+gp5SSthw+dOnUqGzduZPPmzUycOJHFixdz66238tZbb5GVlRXX9s4991xyc3NbHs+aNYuMjAyCwSBZWVkto+u1HtZ00qRJXHXVVTz55JN4PJ6Wuu655x5KSkpaRvDbsaP9ht5FF13Uss3i4mIKCwvx+XyMHDmSnTt38sYbb7By5UqmTZtGSUkJb7zxBp988gkAzz77LFOnTmXKlCmsW7eO9evXt2z3sssuAzofxvVEOa5FHfXnkk4dRELg9tpdjlLO0EnLtzd0NHwoWFdBefXVV7n99tuZPXs2P/7xj9ssf+GFF7j77rsBWlqbxzOs6SuvvMKyZct4+eWX+clPfsK6deswxvDcc88xZsyYNtv7+te/zurVqxk0aFC7w5oe/XrNw5pec801/OIXv2izrU8//ZT777+fDz74gJycHK699to2Q7o2b8vtdiesz91xLWpScwBoqNaTXpSyU+thTjsaPnT37t0EAgGuvvpqfvCDH7Bq1apjnnvppZe2DDFaWtrumENdikaj7Ny5k1mzZnHfffe1GbD/v//7v1vGfV69ejUATzzxBGvWrGkJ6XicffbZLFy4kPJyawiLgwcPsn37dg4fPkxaWhpZWVns27ePv//978f1N5wIx7Wo3WnWwEzVhyrwZw+0uRql+q/Ww5zOnTuXK6+88pjhQ7ds2cItt9zSMvj+b37zGwDmzZvH3LlzKSwsZMmSJSdcSyQS4eqrr6aqqgpjDDfffDPZ2dn853/+JzfddBOTJk3CGENRURF/+9vfjus1xo8fz09/+lNmz55NNBrF6/Xy8MMPM2PGDKZMmUJxcTEjR45k5syZJ/z3dJejhjkFWP76s0x/5zq2X/w8w6ec3cOVKZU8dJjTvqvHhzkVkTEisqbV7bCI3NQz5R7Ll9k8gp52fSilFMTR9WGM2QSUAIiIG/gMeCFRBaVmWUEdqtYxqZVSCrr/Y+LZwFZjzPZEFAOQnmP1S4drNKiVSkTXpLLX8Xym3Q3qK4Cn21sgIvNEZIWIrKioqOh2Ic1ysnNpMm6o1aBW/Zvf7+fAgQMa1n2IMYYDBw7g9/u79by4j/oQkRTgIuD2DgqYD8wH68fEblXRSqrPwz4ycdVrUKv+bciQIezatYsTafgo5/H7/QwZMqRbz+nO4XlzgVXGmH3deoXjUOXKwtOgI+ip/s3r9TJixAi7y1AO0J2uj3+jg26PnlbrycbfpBcPUEopiDOoRSQAnAs8n9hyLPXeHAI6gp5SSgFxdn0YY+qAvATX0iLkyyWjToNaKaXAiWN9AJHU2MBM4Ua7S1FKKds5MqglYJ300ni43OZKlFLKfo4MandGEICagwk/wEQppRzPkUGdkjkAgJqDe22uRCml7OfIoE7NLgCgoUq7PpRSypFBnZ5rjffRpH3USinlzKDOzg0SNi6MDsyklFLODOqsgI9K0qFOg1oppRwZ1B63i0rJwt2gp5ErpZQjgxqgxp2Fr0kHZlJKKccGdb0nm9SQBrVSSjk2qBt9uaRHKu0uQymlbOfYoA77c8kwNRCN2F2KUkrZyrFBbQL5uDCYOr0auVKqf3NsULvTrYGZag/peB9Kqf7NsUHtaR6Y6YCO96GU6t8cG9S+nEIA6it321yJUkrZy7FBnZ47GICmQ3tsrkQppezl2KDOzR9Ao/ESrdauD6VU/xbvxW2zRWShiGwUkQ0iclqiC8tL91NBFtToj4lKqf4trovbAr8CXjPGfElEUoBAAmsCIMXj4qDkEKjToU6VUv1bl0EtIpnAGcC1AMaYJqApsWVZDnvyyGvUrg+lVP8WT9fHSKACeEJEVovIYyKSdvRKIjJPRFaIyIqKiooeKa7eFyQzrCe8KKX6t3iC2gNMBX5jjJkC1AK3Hb2SMWa+MabUGFMaDAZ7pLim1CAZphrCjT2yPaWUSkbxBPUuYJcx5v3Y44VYwZ1wJt26dqLRIz+UUv1Yl0FtjNkL7BSRMbFZZwPrE1pVjDvTOuml7qCe9KKU6r/iPerjRuCp2BEfnwBfT1xJR/hyBgFQvX8XaaN64xWVUsp54gpqY8waoDSxpRwrLc86O7H+wGe9/dJKKeUYjj0zESA7fxARI4Sq9DRypVT/5eigDmYFOEAWVOvZiUqp/svRQZ2d6qXCZOOu06BWSvVfjg5ql0uodOfia+iZE2iUUioZOTqoAWpT8kkP6dmJSqn+y/FB3egPkhGp1IvcKqX6LccHdThQgJso1Gr3h1Kqf3J8UBM7OzFSpWcnKqX6J8cHtSdnKAC1FdvsLUQppWzi+KAO5A8DoKZ8u82VKKWUPRwf1LnBQhqMl9DBnXaXopRStnB8UBdmB9hjcqFql92lKKWULRwf1MEMH3vJw1urPyYqpfonxwe12yUc8gwgUK8XD1BK9U+OD2qAWn+Bde3ESNjuUpRSqtclRVA3pQ3GRRRqtFWtlOp/kiKoybIuIGD0B0WlVD+UFEGdkmud9FJ/YIfNlSilVO9LiqBOG1AEQK2e9KKU6oeSIqiD+UGqTSqNB/SkF6VU/xPXxW1FZBtQDUSAsDGmVy90OzDTzx6TS0D7qJVS/VBcQR0zyxizP2GVdKIg08+7Jo+xetKLUqofSoqujxSPi4OeIIEGPTxPKdX/xBvUBlgkIitFZF57K4jIPBFZISIrKip6fpD/Wv9AMsKHIFTf49tWSiknizeoZxpjpgJzgRtE5IyjVzDGzDfGlBpjSoPBYI8WCVCXPtyaOKRHfiil+pe4gtoYszt2Xw68AExPZFHt1pBdZE0c+rS3X1oppWzVZVCLSJqIZDRPA7OBskQXdjRvcCQAjeVbevullVLKVvEc9VEAvCAizev/yRjzWkKrakdwwCAOmwCRvZvx9faLK6WUjboMamPMJ8DkXqilU0Pz0thuBlB44BO7S1FKqV6VFIfnAQzLDbDdFOCt2mZ3KUop1auSJqhzAl52uwpJr9+t41IrpfqVpAlqEaE2MBQ3EajSMT+UUv1H0gQ1QFgP0VNK9UNJFdTe4EkAmIMa1Eqp/iOpgjp7wFAajZf6vZvtLkUppXpNUgX1sPx0dpgBNFZstbsUpZTqNUkV1ENzAmwzBbgrtetDKdV/JFVQD8kJ8IkpJFC9HaIRu8tRSqlekVRBnZriZq+vCI9pAv1BUSnVTyRVUAPUZo22JsrX21uIUkr1kqQLagmOtSYqNtpbiFJK9ZKkC+rBA/LZEQ0S3rvO7lKUUqpXJF1Qjwqms8kMJbxXuz6UUv1D8gX1gDQ2myGkVH4C4Sa7y1FKqYRLuqAuyrOC2mXCcFBPfFFK9X1JF9R+r5uqDGvMD8o32FuMUkr1gqQLagD3gDFEcOmRH0qpfiEpg3r4gFx2mALMPv1BUSnV98Ud1CLiFpHVIvK3RBYUj1ED0tkQHUp4z0d2l6KUUgnXnRb1fwCO6BQeFUynLFpkXT+xocrucpRSKqHiCmoRGQJcADyW2HLiMyqYxnpTZD3Yq61qpVTfFm+L+kHgh0C0oxVEZJ6IrBCRFRUVFT1RW4dy01LY6Ysd+bFnbUJfSyml7NZlUIvIF4FyY8zKztYzxsw3xpQaY0qDwWCPFdhBTWQPGMohVw7s1aBWSvVt8bSoZwIXicg24M/AWSLyZEKrisPJBemsixZh9nxodylKKZVQXQa1MeZ2Y8wQY0wRcAXwT2PM1QmvrAtjCjJYEx4GFZsg1GB3OUoplTBJeRw1wMkDMyiLjkBMRMemVkr1ad0KamPMUmPMFxNVTHeMKchgnRluPdDuD6VUH5a0Leq8dB/1gaHUu9Jhzxq7y1FKqYRJ2qAGGFuYyTr3WNj+rt2lKKVUwiR1UI8ZmMHSxtGwfxPUJPbYbaWUsktyB3VBBm+HxlkPtr9tbzFKKZUgyR3UAzMoM0WEPQHYpkGtlOqbkjqoRxekExEPu9Ina1ArpfqspA7qQIqHorw0VrmKrYsIaD+1UqoPSuqgBigZms1fq0ZZD7a/Y28xSimVAEkf1FOGZfNW7WCiKemw9Q27y1FKqR6X/EE9NIcwHvbmnw6bXwdj7C5JKaV6VNIH9djCDHweFx94p0P1Hh32VCnV5yR9UHvdLiYNyeKF2nGAwMf/sLskpZTqUUkf1ABThuXwrz1uooOnwsev2V2OUkr1qL4R1EOzaYpE2VvwBfhsFdSU212SUkr1mL4R1MNyAFjumQ4Y2PSqvQUppVQP6hNBPTDLz7DcAH8rz4e8k2DtX+wuSSmlekyfCGqA00fl8f6nB4lM/Ko1QFPlTrtLUkqpHtF3gvqkfKobw2wKzrFmfKStaqVU39B3gnpUHgBLygMwdAasfUZPflFK9Ql9Jqjz032MHZjBv7buh8lftQZp0pNflFJ9QJdBLSJ+EVkuIh+KyDoRubs3Cjsep4/KZ8W2QzScfBG4vLD2WbtLUkqpExZPi7oROMsYMxkoAeaIyIyEVnWcZp6UR2M4yqpy4OTzrH7qSNjuspRS6oR0GdTGUhN76I3dHNn5e+rIPFLcLpZsKofJ/wY1+2DDS3aXpZRSJySuPmoRcYvIGqAceN0Y834768wTkRUisqKiwp4B/NN9Hk4dmcsbG8phzPkQHAtv3gfRiC31KKVUT4grqI0xEWNMCTAEmC4iE9pZZ74xptQYUxoMBnu4zPidO76AT/bXsvVAHXzhh9aPiutftK0epZQ6Ud066sMYUwksBeYkopiecNbYAQC8sWEfjL9EW9VKqaQXz1EfQRHJjk2nAucAGxNc13EbkhNg7MAMFm8oB5cbzrzdalWvXGB3aUopdVziaVEXAktEZC3wAVYf9d8SW9aJOWdcASu3H6KyrgnGXwxFn4d//hTqDtpdmlJKdVs8R32sNcZMMcZMMsZMMMb8394o7EScM76ASNSwdFMFiMDce6GhEpb83O7SlFKq2/rMmYmtTRqcRTDDx+IN+6wZBcUwfR588ChsXmxvcUop1U19MqhdLuGsMQN48+MKmsJRa+Y5d0HBBHj+Oqj6zNb6lFKqO/pkUAPMLi6guiHMso9jx3R7U+HLCyDSBAu/AZGQrfUppVS8+mxQn3FykPx0H8+saDUudf5ouPBXsPM9eP1O+4pTSqlu6LNB7XW7uPyUwfxzYznl1Q1HFkz8Ekz/Nrz3MLz3W/sKVEqpOPXZoAb4SulQIlHD86uO6pOe8wsY+0V47TYdYU8p5Xh9OqhHBdMpHZ7Dsx/sxLS+iIDLDZc/BkWfs35cXPJz7bNWSjlWnw5qgCtPHcYn+2utEfVa86bCVQutUfbevBceOwfKHXvCpVKqH+vzQX3h5EEMzk7lkSVbj13o9cOlv4Wv/BEqd8DvzoDFd0FDVa/XqZRSHenzQe11u7ju8yNYsf0Qyz/t4BTy8RfDDe9b92//F/xqMiy+G6p29W6xSinVjj4f1ABfnTaMvLQUHlm6peOV0gfA5Y/CvKUwfCa88yA8OAmevQa2v6sXylVK2cZjdwG9ITXFzddnFnH/oo9Zt7uK4kFZHa88aApc8RQc2g4fPAar/mCNZ513EoycBaNmwYgzwJfRa/Urpfo3MQloKZaWlpoVK1b0+HZPRFV9iJn3/JMvjAny8JVT439iU611CN/GV2D7vyBUCy6PNX7IwIkwcJJ1avrACeDvZAeglFKdEJGVxpjS9pb1ixY1QFaql6+dNpzfvrmVj/dVc3JBnC3ilDQo/bp1CzfBruWw5Q3YvRo2vQarnzyybvZwK7yDYyGzEDIKIX0gZBRAegG4vYn545RSfVq/aVEDHKpt4nP3/pPPjc7nd19rd8fVPcZA9V7YVwZ718LeMtj7ERzcCiZ67PqBfEgLQiAP0vKs+0C+dZ+a0/bmz7QOIfSkgrvf7E+V6re0RR2Tk5bC9bNO4pf/2MSSTeXMGjPgxDYoYrWcMwth9LlH5kfCULcfqvdA9T6o2WvdV++x5tcesI7Zrtsfu5hBFztLlxe8ASu4vf5W0wHw+I9MNy/z+MDts1rwHh+4U6xpd+tpr7Xd5ml3itWl0+60t+1zXG7rb1dK9Yp+1aIGaApHOf+ht2gIRXj95i+QmuK2t6BoBOorrQsb1B86cmuognADhOpb3eqs+/DR8xqOLAvVQbgRook801I6CffYjuDoaXFZ9y53B4/dsenWjz3gch31OLaOuGPbcB95rkiraZd1Q2LzW09LB/Obp12t1nGB0MH8rqYl/vUldgBWm8cd1HX039LZ8ubPq3VNbWpTTqEt6lZSPC5+dskEvjr/PR7652ZunTPW3oJcbqsbJC2vZ7drjDWka6TJ6luPtL6FrPto+MjjaCg2/6jplvXae06r6TbPO+o5JhqbF7Hmm0hsurPHYYhG2z5urztJ9YA4Qr31ui2TXc2XjteTox53+rzubvt4n9cDNQXy4Jv/oKf1u6AGOHVkHl86ZQiPLvuES0oGM2ZgHzzUTsTq9vD4wGd3MT3EmFZhHj0S8CZq3aIRa17zNCa2LPatsWXadD3d8jg2L+5purl+Z8vaqyva9m85et3m96K5O63l+RzZfnv3bdZtZ1nL8pYHHcw/el5765mjHrc3rxvrxP28zmrvoZr8mSRCl0EtIkOBPwIDgSgw3xjzq4RU04v+z/nj+OfGcr7/7Bqev/50fB6bu0BU10SsH1b1x1XVz8RzZmIY+N/GmHHADOAGERmf2LISLzcthXsum8i63Ye56+X1JKKvXimlekI8VyHfY4xZFZuuBjYAgxNdWG+YXTyQ7545iqeX7+C3b35idzlKKdWubn2HFJEiYArwfjvL5gHzAIYNG9YTtfWKW2aP4bND9dz72kYGZfu5uKRP7IOUUn1I3IMyiUg68BxwkzHm8NHLjTHzjTGlxpjSYDDYkzUmlMsl/PLLk5g+Ipdb/rKW19fvs7skpZRqI66gFhEvVkg/ZYx5PrEl9T6fx838r53CuEGZzPufFTzxzqfaZ62Ucowug1pEBPg9sMEY80DiS7JHdiCFP183g3PHFXD3X9dzw59Wcai2ye6ylFIqrhb1TOBrwFkisiZ2Oz/BddkiNcXNb64+hVvnjOX19fs478FlLPu4wu6ylFL9XL87hTxeZZ9VcdMza9hSXsO1pxdx29yx+L16rLVSKjE6O4W8X1zh5XhMGJzF3278HNeeXsSCf21jzoPL+OuHu4lGte9aKdW7NKg74fe6ueuiYv7nm9Pxedzc+PRqLvz127yzZb/dpSml+hEN6jh8fnSQV//j8/zXVydTVR/iqsfe5yu/e5fXyvZoC1splXDaR91NDaEIf/jXNv60fAfbD9QxMNPPhZMLubhkMMWDMhEdOlIpdRw666PWoD5OkajhtbK9vLD6M978uJxQxDAiP40zRucz86R8ZozKI9Ovl95SSsVHgzrBKuua+HvZXv5etpflnx6gIRTFJTB5aDafOymfGSPzKBmaTZpPR31TSrVPg7oXNYYjrN5RyTtb9vPOlv18uKuKSNTgdgnjCjMoHZ7L1OE5lA7PYVB2qt3lKqUcQoPaRtUNIVZuP8TK7YdYse0Qa3ZWUh+KAFCY5Wfq8BwmD8li4uBsJgzOJEO7S5Tql/RSXDbK8Hs5c8wAzoxdSDcUibJxTzUrtx9kxfZDrN5RyStr97SsPzI/jXGDMhlfmMn4QZkUF2YSzPDpj5RK9WPaonaA/TWNfPRZFWW7qvjosyo27D3MzoP1Lcvz0lIYPyiTEflpDMlJZVhugGG5aQzLC5Cu/d5K9Qnaona4/HQfs8YMYFas1Q1QVR9i457DrN9zmA2x+w9XV3K4IdzmublpKQzNDTA8NxAL8ADD8qz7gkw/bpe2xJVKdhrUDpWV6uXUkXmcOrLt1cmr6kPsPFjHjta3A3Ws2VnJKx/tIdLqBBy3Swim+xiY5Wdgpp+BWX4KMv0MzPJRkOFnQKaP/HQfWale7VpRysE0qJNMVqqXrMFZTBicdcyyUCTKnsoGdhysY/vBWvZUNrD3cAP7DjewpaKGd7bsp7oxfMzzPC4hLz2F/HQfeek+8tJSyEtLITc9hfw0H7lpKS3Lc9NSCKS4NdiV6kUa1H2I1+2yuj3yAnyO/HbXqW0Mt4R3RXUjFdWNHKht4kBNI/trrPtPKmo4UNPUcnTK0fxeF3lpPvLSU6wQj03np6cQzPARTPeTHfCSleol0+8l3e/RLhilToAGdT+T5vMwKpjOqGB6l+vWNYU5UNPEgdomDtZaQX4wFupWuDexv6aRj/dWs7+2iaZwtN3tiECm30tOwEt2wAr37ICXnEAKOQEvWYEUMv0eMv1eMlOb762Q93td2npX/Z4GtepQIMVDINfD0NxAl+saY6huDLM/1kqvqg9xuCHM4foQlfUhquqaOFgXorKuifLqBjbtreZQXRN1Te232pt53dLSMs/we0j3e0j3eUj3xR77jsxreRybl+HzkuZzk+734PPoWOIqeWlQqx4hIlZL2O9lZByt9WYNoQiHG0Icrg9T3RCiqj5EdUO4ZZ51b4V+VX2I2sYw+6vrqGm01q9pDBPPAIZet5Du85DmOyrMYzuAjOYWfWxems9Dms99JOxj66d6tX9e9T4NamUrv9eN3+tmQMbxPd8YQ30oQk1DmOrGMNUNYWpj9zWNYWoaQtQ2RVrm1zZa69U2hjlY28T2A3VUx3YKTZH2u25acwltwj4t1pJPS/EQ8LkJpLit6RQPgRR3y7zmx2k+a93m8A+keEjx6GjDqnMa1CqpiUgsBD0M6Hr1TjW37ptDvaYxTG1jpNV0ONaSPzLdfNtb1UBdU4S6pjB1TREaO+ivb0+K20War22IH5n2kO5zE2jeMaQcmfZ7Xfg9bnxeN6let/U4tuNrXubSH3H7hC6DWkQeB74IlBtjJiS+JKXscaKt+9bCkSj1oUgsvK2wr2uKUNt0pGXfshNoClPXaodQF/sGsLeqwVov9vzwcVykIsXjwu85EuDNge5rDvTYstZB74tNpx4V+m12Aq2mm9fzefSH30SJp0W9APg18MfElqJU3+Fxu8hwu3pskC1jDI3haEvo1zaFaQhFqW+K0BCO0BiK0BCK0hCK0BCKUN88HY7QGJuujy1rXq+qPkR5O/O7823gaL6Ogr+d+dayVjsET+sdQMc7BOtbhKtf7Ri6DGpjzDIRKeqFWpRSHRCRltDKTUtJ6GtFo4amyJGdQNsdwJHgb72sPhb0ja1Cv2UHELbWqaxrYm/r+bFlHR3W2RURYi39VsHfwQ6hzXTsOT6P9c0ixe2KBb+bFI81v/neurlbzbPW6e3zArSPWinVhssl+F1WqPWGaNT6ttAmwENRa0dwzM4i2mqHcWQn0OZbRGxncqC2qc1OpHl+PD8ad8XtElLcLrxuIaVVkAfTfTz7ndN64F1pq8eCWkTmAfMAhg0b1lObVUr1cS6XkJriJjWld3YMkaihIRShKRylMdaibwxbXT6NraaPWR4Lees+QihiaApb85pi66X5EvM39FhQG2PmA/PBGua0p7arlFI9ye2S2HHydlcSPz2AUymlHK7LoBaRp4F3gTEisktEvpn4spRSSjWL56iPf+uNQpRSSrVPuz6UUsrhNKiVUsrhNKiVUsrhNKiVUsrhNKiVUsrhxJiePzdFRCqA7cf59Hxgfw+W01uStW7Q2u2SrLUna93g7NqHG2OC7S1ISFCfCBFZYYwptbuO7krWukFrt0uy1p6sdUPy1q5dH0op5XAa1Eop5XBODOr5dhdwnJK1btDa7ZKstSdr3ZCktTuuj1oppVRbTmxRK6WUakWDWimlHM4xQS0ic0Rkk4hsEZHb7K6nKyKyTUQ+EpE1IrIiNi9XRF4Xkc2x+xy76wTrSvIiUi4iZa3mdViriNwe+xw2ich59lTdYd13ichnsfd9jYic32qZI+qO1TJURJaIyAYRWSci/xGb7+j3vZO6Hf++i4hfRJaLyIex2u+OzXf0ex4XY4ztN8ANbAVGAinAh8B4u+vqouZtQP5R8+4DbotN3wbca3edsVrOAKYCZV3VCoyPvf8+YETsc3E7qO67gB+0s65j6o7VUwhMjU1nAB/HanT0+95J3Y5/3wEB0mPTXuB9YIbT3/N4bk5pUU8HthhjPjHGNAF/Bi62uabjcTHwh9j0H4BL7CvlCGPMMuDgUbM7qvVi4M/GmEZjzKfAFqzPp9d1UHdHHFM3gDFmjzFmVWy6GtgADMbh73sndXfEEXUDGEtN7KE3djM4/D2Ph1OCejCws9XjXXT+j8MJDLBIRFbGLuwLUGCM2QPWP3hggG3Vda2jWpPhs/hfIrI21jXS/DXWsXWLSBEwBauFlzTv+1F1QxK87yLiFpE1QDnwujEmqd7zjjglqKWdeU4/bnCmMWYqMBe4QUTOsLugHuL0z+I3wCigBNgD/L/YfEfWLSLpwHPATcaYw52t2s482+pvp+6keN+NMRFjTAkwBJguIhM6Wd1RtXfGKUG9Cxja6vEQYLdNtcTFGLM7dl8OvID1lWmfiBQCxO7L7auwSx3V6ujPwhizL/afMQo8ypGvqo6rW0S8WGH3lDHm+dhsx7/v7dWdTO87gDGmElgKzCEJ3vOuOCWoPwBGi8gIEUkBrgBetrmmDolImohkNE8Ds4EyrJqvia12DfCSPRXGpaNaXwauEBGfiIwARgPLbaivXc3/4WIuxXrfwWF1i4gAvwc2GGMeaLXI0e97R3Unw/suIkERyY5NpwLnABtx+HseF7t/zWz1i+35WL8wbwV+ZHc9XdQ6EuvX4g+Bdc31AnnAG8Dm2H2u3bXG6noa6+tqCKsV8c3OagV+FPscNgFzHVb3/wAfAWux/qMVOq3uWC2fw/oavRZYE7ud7/T3vZO6Hf++A5OA1bEay4Afx+Y7+j2P56ankCullMM5petDKaVUBzSolVLK4TSolVLK4TSolVLK4TSolVLK4TSolVLK4TSolVLK4f4/h/VxjAmq0gwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv_output = xgb.cv(xgb_params, dtrain, num_boost_round=1000, early_stopping_rounds=20,\n",
    "    verbose_eval=50, show_stdv=False)\n",
    "cv_output[['train-rmse-mean', 'test-rmse-mean']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "28aa4b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:08:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:08:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\", \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_boost_rounds = len(cv_output)\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round= num_boost_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "916d3ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "testId = dfs['test']['id']\n",
    "y_predict = model.predict(dtest)\n",
    "output = pd.DataFrame({'id': testId, 'price_doc': y_predict})\n",
    "output.to_csv(\"./output_models/output_naivexgb.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f0ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d80b3d8",
   "metadata": {},
   "source": [
    "## Process raw data for testing with LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7e0d127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_process(train, test):\n",
    "    #Remove the bad prices as suggested by Radar\n",
    "    train=train[(train.price_doc>1e6) & (train.price_doc!=2e6) & (train.price_doc!=3e6)]\n",
    "    train.loc[(train.product_type=='Investment') & (train.build_year<=2000),'price_doc']*=0.895 \n",
    "    train.loc[train.product_type!='Investment','price_doc']*=0.96\n",
    "\n",
    "  \n",
    "    id_test = test.id\n",
    "    times=pd.concat([train.timestamp,test.timestamp])\n",
    "    num_train=train.shape[0]\n",
    "    y_train = train[\"price_doc\"]\n",
    "    train.drop(['price_doc'],inplace=True,axis=1)\n",
    "    da=pd.concat([train,test])\n",
    "    da['na_count']=da.isnull().sum(axis=1)\n",
    "    df_cat=None\n",
    "    to_remove=[]\n",
    "    for c in da.columns:\n",
    "        if da[c].dtype=='object':\n",
    "            oh=pd.get_dummies(da[c],prefix=c)\n",
    "            if df_cat is None:\n",
    "                df_cat=oh\n",
    "            else:\n",
    "                df_cat=pd.concat([df_cat,oh],axis=1)\n",
    "            to_remove.append(c)\n",
    "    da.drop(to_remove,inplace=True,axis=1)\n",
    "\n",
    "    #Remove rare features,prevent overfitting\n",
    "    to_remove=[]\n",
    "    if df_cat is not None:\n",
    "        sums=df_cat.sum(axis=0)\n",
    "        to_remove=sums[sums<200].index.values\n",
    "        df_cat=df_cat.loc[:,df_cat.columns.difference(to_remove)]\n",
    "        da = pd.concat([da, df_cat], axis=1)\n",
    "    x_train=da[:num_train].drop(['timestamp','id'],axis=1)\n",
    "    x_test=da[num_train:].drop(['timestamp','id'],axis=1)\n",
    "    #Log transformation, boxcox works better.\n",
    "    y_train=np.log(y_train)\n",
    "    \n",
    "    return x_train, x_test, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "680c5b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\pandas\\core\\indexing.py:1745: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\pandas\\core\\frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013619 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "lgbm_train = pd.read_csv('../data/train.zip',parse_dates=['timestamp'])\n",
    "lgbm_test = pd.read_csv('../data/test.zip',parse_dates=['timestamp'])\n",
    "lgbmx_train, lgbmx_test, lgbmy_train = lgbm_process(lgbm_train, lgbm_test)\n",
    "id_test =  lgbm_test.id\n",
    "lgbm_pred = lgbm_predict(lgbmx_train, lgbmx_test, lgbmy_train)\n",
    "lgbm_output = pd.DataFrame({'id':id_test,'price_doc':lgbm_pred})\n",
    "lgbm_output.to_csv('./output_models/lgbm_raw.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e0baf7",
   "metadata": {},
   "source": [
    "**Display Result Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "499297cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>Train MSE</th>\n",
       "      <th>Test MSE</th>\n",
       "      <th>Train RMSE</th>\n",
       "      <th>Test RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.1225</td>\n",
       "      <td>0.2243</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.2449</td>\n",
       "      <td>0.4838</td>\n",
       "      <td>0.4949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.0815</td>\n",
       "      <td>0.2212</td>\n",
       "      <td>0.2855</td>\n",
       "      <td>0.4703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>SGD</td>\n",
       "      <td>84.5541</td>\n",
       "      <td>83.8895</td>\n",
       "      <td>9.1953</td>\n",
       "      <td>9.1591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.5344</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>0.7310</td>\n",
       "      <td>0.7272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resdf = pd.DataFrame(resArr, columns = [\"Model\", \"Train MSE\", \"Test MSE\", \"Train RMSE\", \"Test RMSE\"])\n",
    "display(HTML(resdf.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c55aa37",
   "metadata": {},
   "source": [
    "**SGD Regressor has an appalling performance with extremely high MSE / RMSE scores**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ce0f5e",
   "metadata": {},
   "source": [
    "## Trying a different approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720cfaeb",
   "metadata": {},
   "source": [
    "## Step 1: Split train data into 80-20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4f7c06a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and cross-validation\n",
    "x_tr, x_cv, y_tr, y_cv = train_test_split(xTrain, yTrain, test_size=0.15,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7b1d7a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect code categorical variables:\n",
    "\n",
    "num = xTrain.select_dtypes(exclude=['object'])\n",
    "cat = xTrain.select_dtypes(include=['object']).copy()\n",
    "\n",
    "\n",
    "for c in cat:\n",
    "  labelEnc = preprocessing.LabelEncoder()\n",
    "  labelEnc.fit(x_tr[c])\n",
    "\n",
    "  x_cv[c] = x_cv[c].map(lambda s: '<unknown>' if s not in labelEnc.classes_ else s)\n",
    "  labelEnc.classes_ = np.append(labelEnc.classes_, '<unknown>')\n",
    "\n",
    "  x_tr[c] = labelEnc.transform(x_tr[c])\n",
    "  x_cv[c] = labelEnc.transform(x_cv[c])\n",
    "\n",
    "for c in num:\n",
    "  min = x_tr[c].min()\n",
    "  max = x_tr[c].max()\n",
    "\n",
    "  x_tr[c] = (x_tr[c] - min)/(max-min)\n",
    "  x_cv[c] = (x_cv[c] - min)/(max-min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2ae85e",
   "metadata": {},
   "source": [
    "## Step 2: In train set, split the train set into A1 and A2 50-50 split\n",
    "## Then we use A1 to do sampling with replacement to create a1,a2....ak(k samples).\n",
    "## Now we create 'k' models, and train each of our models with these k samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c618e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1, A2, y_a1, y_a2 = train_test_split(x_tr, y_tr, test_size=0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fbbc3587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12950, 249) (12950, 249)\n"
     ]
    }
   ],
   "source": [
    "A1 = A1.to_numpy()\n",
    "A2 = A2.to_numpy()\n",
    "\n",
    "y_a1 = y_a1.to_numpy()\n",
    "y_a2 = y_a2.to_numpy()\n",
    "\n",
    "print(A1.shape, A2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63be74ae",
   "metadata": {},
   "source": [
    "### Generate random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0a61f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generating_samples(input_, target_):\n",
    "     \n",
    "    s_rows = random.sample(range(len(input_)),int(len(input_)*0.60))  \n",
    "    sample_data = list()\n",
    "    target_data = list()\n",
    "    \n",
    "    for i in s_rows:\n",
    "      sample_data.append(input_[i])\n",
    "        \n",
    "    target_data = target_[s_rows].tolist()\n",
    "    \n",
    "    \n",
    "    \n",
    "    return sample_data, target_data, s_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "746f0535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e098c2e98794cd5975fef2c07f51e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create 200 samples using generating_samples function\n",
    "input_data =[]\n",
    "output_data =[]\n",
    "selected_rows = []\n",
    "\n",
    "f = 200\n",
    "\n",
    "for i in tqdm_notebook(range(f)):\n",
    "    a,b,c = generating_samples(A1, y_a1)\n",
    "    input_data.append(a)\n",
    "    output_data.append(b)\n",
    "    selected_rows.append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e58cece",
   "metadata": {},
   "source": [
    "## Regression Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c840a09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973ec04324a94bfca253063f8daf9252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_all_models = list()\n",
    "\n",
    "for i in tqdm_notebook(range(len(input_data))):\n",
    "    \n",
    "    k = np.array(input_data[i])\n",
    "    j = np.array(output_data[i])\n",
    "      \n",
    "    model = DecisionTreeRegressor(max_depth = None)\n",
    "    model.fit(k,j)\n",
    "    \n",
    "    list_all_models.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aa6ee9",
   "metadata": {},
   "source": [
    "## Step 3: We pass the A2 set to each k models, then we will get k predictions for A2 for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e2ad4a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_predictions = list()\n",
    "\n",
    "for i in list_all_models:\n",
    "  k_predictions.append(i.predict(A2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "deef463c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12950, 200)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_predictions = np.array(k_predictions)\n",
    "k_predictions = k_predictions.T\n",
    "k_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28247ac3",
   "metadata": {},
   "source": [
    "## Step 4: We then use the k predictions to create a new dataset, and for A2, since we know it's target values, we can use the k predictions to train a meta model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0f1a0e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for train is:  0.2198841629563653\n"
     ]
    }
   ],
   "source": [
    "# Using Linear Regression Model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(k_predictions, y_a2)\n",
    "\n",
    "print(\"MSE for train is: \", mean_squared_error(reg.predict(k_predictions), y_a2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6712ca",
   "metadata": {},
   "source": [
    "## Step 5: For the model evaluation, we use the 20% data that we kept as the test set. \n",
    "## Pass that test set to each base model and we will get 'k' predictions. \n",
    "## We then create a new dataset using these k predictions and pass it to our meta model to get the final prediction. We can calculate the models performance score using this final prediction as well as the targets for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c67fe752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance score for test data is:  0.3564683253782919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Testing the Model\n",
    "test_pred = list()\n",
    "\n",
    "for model in list_all_models:\n",
    "  test_pred.append(model.predict(x_cv))\n",
    "\n",
    "test_pred = np.array(test_pred)\n",
    "test_pred = test_pred.T\n",
    "\n",
    "print(\"Model Performance score for test data is: \", reg.score(test_pred, y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9cc22cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23290383289545963\n"
     ]
    }
   ],
   "source": [
    "# Mean Sqaured Error for test data is:\n",
    "\n",
    "mse = mean_squared_error(reg.predict(test_pred), y_cv)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfcc45d",
   "metadata": {},
   "source": [
    "### Predicting the house prices for test.csv file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "75536dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7662, 249)\n"
     ]
    }
   ],
   "source": [
    "x_test = xTest.copy()\n",
    "test_id = x_test['id']\n",
    "x_test.drop(['id','timestamp'],axis=1,inplace=True)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5e569c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the House prices\n",
    "# Preparing test data:\n",
    "\n",
    "num = xTrain.select_dtypes(exclude=['object'])\n",
    "cat = xTrain.select_dtypes(include=['object']).copy()\n",
    "\n",
    "\n",
    "for c in cat:\n",
    "  labelEnc = preprocessing.LabelEncoder()\n",
    "  labelEnc.fit(xTrain[c])\n",
    "\n",
    "  x_test[c] = x_test[c].map(lambda s: '<unknown>' if s not in labelEnc.classes_ else s)\n",
    "  labelEnc.classes_ = np.append(labelEnc.classes_, '<unknown>')\n",
    "\n",
    "  x_test[c] = labelEnc.transform(x_test[c])\n",
    "\n",
    "for c in num:\n",
    "  min = xTrain[c].min()\n",
    "  max = xTrain[c].max()\n",
    "\n",
    "  x_test[c] = (x_test[c] - min)/(max-min)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e62d8678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Predict the House prices \n",
    "# Testing the Model\n",
    "test_pred = list()\n",
    "\n",
    "for model in list_all_models:\n",
    "  test_pred.append(model.predict(x_test))\n",
    "\n",
    "test_pred = np.array(test_pred)\n",
    "test_pred = test_pred.T\n",
    "pred = reg.predict(test_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5fd05ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_sq</th>\n",
       "      <th>life_sq</th>\n",
       "      <th>floor</th>\n",
       "      <th>max_floor</th>\n",
       "      <th>material</th>\n",
       "      <th>num_room</th>\n",
       "      <th>state</th>\n",
       "      <th>product_type</th>\n",
       "      <th>sub_area</th>\n",
       "      <th>area_m</th>\n",
       "      <th>raion_popul</th>\n",
       "      <th>green_zone_part</th>\n",
       "      <th>indust_part</th>\n",
       "      <th>children_preschool</th>\n",
       "      <th>preschool_quota</th>\n",
       "      <th>preschool_education_centers_raion</th>\n",
       "      <th>children_school</th>\n",
       "      <th>school_education_centers_raion</th>\n",
       "      <th>school_education_centers_top_20_raion</th>\n",
       "      <th>hospital_beds_raion</th>\n",
       "      <th>healthcare_centers_raion</th>\n",
       "      <th>university_top_20_raion</th>\n",
       "      <th>sport_objects_raion</th>\n",
       "      <th>additional_education_raion</th>\n",
       "      <th>culture_objects_top_25</th>\n",
       "      <th>shopping_centers_raion</th>\n",
       "      <th>office_raion</th>\n",
       "      <th>thermal_power_plant_raion</th>\n",
       "      <th>incineration_raion</th>\n",
       "      <th>oil_chemistry_raion</th>\n",
       "      <th>radiation_raion</th>\n",
       "      <th>railroad_terminal_raion</th>\n",
       "      <th>big_market_raion</th>\n",
       "      <th>nuclear_reactor_raion</th>\n",
       "      <th>detention_facility_raion</th>\n",
       "      <th>young_all</th>\n",
       "      <th>young_male</th>\n",
       "      <th>young_female</th>\n",
       "      <th>work_all</th>\n",
       "      <th>work_male</th>\n",
       "      <th>work_female</th>\n",
       "      <th>ekder_all</th>\n",
       "      <th>ekder_male</th>\n",
       "      <th>ekder_female</th>\n",
       "      <th>0_6_all</th>\n",
       "      <th>0_6_male</th>\n",
       "      <th>0_6_female</th>\n",
       "      <th>7_14_all</th>\n",
       "      <th>7_14_male</th>\n",
       "      <th>7_14_female</th>\n",
       "      <th>0_17_all</th>\n",
       "      <th>0_17_male</th>\n",
       "      <th>0_17_female</th>\n",
       "      <th>0_13_all</th>\n",
       "      <th>0_13_male</th>\n",
       "      <th>0_13_female</th>\n",
       "      <th>raion_build_count_with_material_info</th>\n",
       "      <th>build_count_brick</th>\n",
       "      <th>build_count_monolith</th>\n",
       "      <th>raion_build_count_with_builddate_info</th>\n",
       "      <th>build_count_before_1920</th>\n",
       "      <th>build_count_1946-1970</th>\n",
       "      <th>metro_min_avto</th>\n",
       "      <th>metro_km_avto</th>\n",
       "      <th>metro_min_walk</th>\n",
       "      <th>metro_km_walk</th>\n",
       "      <th>kindergarten_km</th>\n",
       "      <th>school_km</th>\n",
       "      <th>park_km</th>\n",
       "      <th>green_zone_km</th>\n",
       "      <th>industrial_km</th>\n",
       "      <th>water_treatment_km</th>\n",
       "      <th>incineration_km</th>\n",
       "      <th>railroad_station_walk_km</th>\n",
       "      <th>railroad_station_walk_min</th>\n",
       "      <th>...</th>\n",
       "      <th>sport_count_1500</th>\n",
       "      <th>market_count_1500</th>\n",
       "      <th>green_part_2000</th>\n",
       "      <th>prom_part_2000</th>\n",
       "      <th>office_count_2000</th>\n",
       "      <th>office_sqm_2000</th>\n",
       "      <th>trc_count_2000</th>\n",
       "      <th>trc_sqm_2000</th>\n",
       "      <th>cafe_count_2000</th>\n",
       "      <th>cafe_sum_2000_min_price_avg</th>\n",
       "      <th>cafe_sum_2000_max_price_avg</th>\n",
       "      <th>cafe_avg_price_2000</th>\n",
       "      <th>cafe_count_2000_na_price</th>\n",
       "      <th>cafe_count_2000_price_500</th>\n",
       "      <th>cafe_count_2000_price_1000</th>\n",
       "      <th>cafe_count_2000_price_1500</th>\n",
       "      <th>cafe_count_2000_price_2500</th>\n",
       "      <th>cafe_count_2000_price_4000</th>\n",
       "      <th>cafe_count_2000_price_high</th>\n",
       "      <th>big_church_count_2000</th>\n",
       "      <th>church_count_2000</th>\n",
       "      <th>mosque_count_2000</th>\n",
       "      <th>leisure_count_2000</th>\n",
       "      <th>sport_count_2000</th>\n",
       "      <th>market_count_2000</th>\n",
       "      <th>green_part_3000</th>\n",
       "      <th>office_count_3000</th>\n",
       "      <th>office_sqm_3000</th>\n",
       "      <th>trc_count_3000</th>\n",
       "      <th>trc_sqm_3000</th>\n",
       "      <th>cafe_count_3000</th>\n",
       "      <th>cafe_count_3000_na_price</th>\n",
       "      <th>cafe_count_3000_price_500</th>\n",
       "      <th>cafe_count_3000_price_1000</th>\n",
       "      <th>cafe_count_3000_price_1500</th>\n",
       "      <th>cafe_count_3000_price_2500</th>\n",
       "      <th>cafe_count_3000_price_4000</th>\n",
       "      <th>cafe_count_3000_price_high</th>\n",
       "      <th>big_church_count_3000</th>\n",
       "      <th>church_count_3000</th>\n",
       "      <th>mosque_count_3000</th>\n",
       "      <th>leisure_count_3000</th>\n",
       "      <th>sport_count_3000</th>\n",
       "      <th>market_count_3000</th>\n",
       "      <th>green_part_5000</th>\n",
       "      <th>prom_part_5000</th>\n",
       "      <th>office_count_5000</th>\n",
       "      <th>office_sqm_5000</th>\n",
       "      <th>trc_count_5000</th>\n",
       "      <th>trc_sqm_5000</th>\n",
       "      <th>cafe_count_5000</th>\n",
       "      <th>cafe_count_5000_na_price</th>\n",
       "      <th>cafe_count_5000_price_500</th>\n",
       "      <th>cafe_count_5000_price_1000</th>\n",
       "      <th>cafe_count_5000_price_1500</th>\n",
       "      <th>cafe_count_5000_price_2500</th>\n",
       "      <th>cafe_count_5000_price_4000</th>\n",
       "      <th>cafe_count_5000_price_high</th>\n",
       "      <th>big_church_count_5000</th>\n",
       "      <th>church_count_5000</th>\n",
       "      <th>mosque_count_5000</th>\n",
       "      <th>leisure_count_5000</th>\n",
       "      <th>sport_count_5000</th>\n",
       "      <th>market_count_5000</th>\n",
       "      <th>year</th>\n",
       "      <th>year_month</th>\n",
       "      <th>living_area_ratio</th>\n",
       "      <th>non_living_area</th>\n",
       "      <th>non_living_area_ratio</th>\n",
       "      <th>room_area_avg</th>\n",
       "      <th>relative_floor</th>\n",
       "      <th>sub_area_building_height_avg</th>\n",
       "      <th>sub_area_kremlin_dist_avg</th>\n",
       "      <th>sales_year_month</th>\n",
       "      <th>price_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30471</th>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0690</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0.1180</td>\n",
       "      <td>0.7174</td>\n",
       "      <td>0.1598</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.7300</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8462</td>\n",
       "      <td>0.7784</td>\n",
       "      <td>0.9286</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.2462</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4483</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1739</td>\n",
       "      <td>0.0284</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7549</td>\n",
       "      <td>0.7727</td>\n",
       "      <td>0.7360</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.7399</td>\n",
       "      <td>0.7597</td>\n",
       "      <td>0.4517</td>\n",
       "      <td>0.3794</td>\n",
       "      <td>0.4886</td>\n",
       "      <td>0.7300</td>\n",
       "      <td>0.7443</td>\n",
       "      <td>0.7145</td>\n",
       "      <td>0.7784</td>\n",
       "      <td>0.8013</td>\n",
       "      <td>0.7544</td>\n",
       "      <td>0.7581</td>\n",
       "      <td>0.7767</td>\n",
       "      <td>0.7383</td>\n",
       "      <td>0.7505</td>\n",
       "      <td>0.7700</td>\n",
       "      <td>0.7297</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3690</td>\n",
       "      <td>0.9134</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0916</td>\n",
       "      <td>0.5195</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0124</td>\n",
       "      <td>0.0124</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0158</td>\n",
       "      <td>0.0432</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0858</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>0.1774</td>\n",
       "      <td>0.1943</td>\n",
       "      <td>0.1943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1081</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2014</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.3493</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0926</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1951</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>0.0275</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2510</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0667</td>\n",
       "      <td>0.0652</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0642</td>\n",
       "      <td>0.0476</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.5839</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.5248</td>\n",
       "      <td>0.3324</td>\n",
       "      <td>0.2309</td>\n",
       "      <td>5,462,395.0703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30472</th>\n",
       "      <td>0.0147</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.1039</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1111</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>0.1150</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2462</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0435</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.1673</td>\n",
       "      <td>0.1024</td>\n",
       "      <td>0.0472</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1645</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.0460</td>\n",
       "      <td>0.0581</td>\n",
       "      <td>0.0581</td>\n",
       "      <td>0.0410</td>\n",
       "      <td>0.0281</td>\n",
       "      <td>0.0929</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.3334</td>\n",
       "      <td>0.3132</td>\n",
       "      <td>0.2205</td>\n",
       "      <td>0.2205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6541</td>\n",
       "      <td>0.0724</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.2449</td>\n",
       "      <td>0.2619</td>\n",
       "      <td>0.2554</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5317</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0305</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4946</td>\n",
       "      <td>0.2642</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.0504</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0440</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>0.0476</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.5839</td>\n",
       "      <td>0.9937</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>0.7023</td>\n",
       "      <td>0.3214</td>\n",
       "      <td>0.2309</td>\n",
       "      <td>8,067,024.0976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30473</th>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0390</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.0556</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>0.5584</td>\n",
       "      <td>0.0746</td>\n",
       "      <td>0.4327</td>\n",
       "      <td>0.3268</td>\n",
       "      <td>0.1872</td>\n",
       "      <td>0.5385</td>\n",
       "      <td>0.3378</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3192</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4483</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0870</td>\n",
       "      <td>0.0496</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3331</td>\n",
       "      <td>0.3246</td>\n",
       "      <td>0.3422</td>\n",
       "      <td>0.5647</td>\n",
       "      <td>0.5570</td>\n",
       "      <td>0.5722</td>\n",
       "      <td>0.5869</td>\n",
       "      <td>0.4967</td>\n",
       "      <td>0.6329</td>\n",
       "      <td>0.3268</td>\n",
       "      <td>0.3151</td>\n",
       "      <td>0.3394</td>\n",
       "      <td>0.3378</td>\n",
       "      <td>0.3339</td>\n",
       "      <td>0.3419</td>\n",
       "      <td>0.3374</td>\n",
       "      <td>0.3301</td>\n",
       "      <td>0.3451</td>\n",
       "      <td>0.3315</td>\n",
       "      <td>0.3233</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.3825</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.3335</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5172</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0530</td>\n",
       "      <td>0.2929</td>\n",
       "      <td>0.0641</td>\n",
       "      <td>0.2425</td>\n",
       "      <td>0.1711</td>\n",
       "      <td>0.1462</td>\n",
       "      <td>0.1462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3243</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.5431</td>\n",
       "      <td>0.1873</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0811</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0251</td>\n",
       "      <td>0.1786</td>\n",
       "      <td>0.1975</td>\n",
       "      <td>0.1903</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0727</td>\n",
       "      <td>0.2593</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.6180</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0352</td>\n",
       "      <td>0.0909</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>0.0204</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.0431</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0588</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.3072</td>\n",
       "      <td>0.4720</td>\n",
       "      <td>0.0342</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.2167</td>\n",
       "      <td>0.2234</td>\n",
       "      <td>0.0677</td>\n",
       "      <td>0.0287</td>\n",
       "      <td>0.0815</td>\n",
       "      <td>0.0988</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0662</td>\n",
       "      <td>0.0840</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0943</td>\n",
       "      <td>0.3257</td>\n",
       "      <td>0.5238</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.5837</td>\n",
       "      <td>0.9935</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.2586</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>0.2309</td>\n",
       "      <td>5,029,510.1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30474</th>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.2208</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0556</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>1</td>\n",
       "      <td>105</td>\n",
       "      <td>0.0952</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.3062</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2462</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0163</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0184</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0158</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0163</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>0.1673</td>\n",
       "      <td>0.1024</td>\n",
       "      <td>0.0472</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1645</td>\n",
       "      <td>0.1291</td>\n",
       "      <td>0.0806</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>0.1096</td>\n",
       "      <td>0.0747</td>\n",
       "      <td>0.1185</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>0.1012</td>\n",
       "      <td>0.2470</td>\n",
       "      <td>0.4165</td>\n",
       "      <td>0.4165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4163</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0278</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4689</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2882</td>\n",
       "      <td>0.0511</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.5846</td>\n",
       "      <td>0.9939</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.3331</td>\n",
       "      <td>0.3632</td>\n",
       "      <td>0.2309</td>\n",
       "      <td>6,521,690.2788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30475</th>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.2208</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>0.1150</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2462</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0435</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.1673</td>\n",
       "      <td>0.1024</td>\n",
       "      <td>0.0472</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1645</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0291</td>\n",
       "      <td>0.0291</td>\n",
       "      <td>0.0309</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.3489</td>\n",
       "      <td>0.3243</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5127</td>\n",
       "      <td>0.0556</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.2449</td>\n",
       "      <td>0.2619</td>\n",
       "      <td>0.2554</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0556</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5607</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4462</td>\n",
       "      <td>0.2381</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0439</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0480</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0505</td>\n",
       "      <td>0.0476</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.5825</td>\n",
       "      <td>0.9894</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.7023</td>\n",
       "      <td>0.3214</td>\n",
       "      <td>0.2309</td>\n",
       "      <td>5,373,426.2699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30476</th>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.3549</td>\n",
       "      <td>0.4017</td>\n",
       "      <td>0.4572</td>\n",
       "      <td>0.1957</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.3846</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>0.3571</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6207</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2609</td>\n",
       "      <td>0.0426</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1966</td>\n",
       "      <td>0.1947</td>\n",
       "      <td>0.1986</td>\n",
       "      <td>0.3976</td>\n",
       "      <td>0.4815</td>\n",
       "      <td>0.3160</td>\n",
       "      <td>0.2743</td>\n",
       "      <td>0.2373</td>\n",
       "      <td>0.2933</td>\n",
       "      <td>0.1957</td>\n",
       "      <td>0.1918</td>\n",
       "      <td>0.1999</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>0.1947</td>\n",
       "      <td>0.1987</td>\n",
       "      <td>0.1975</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.2006</td>\n",
       "      <td>0.1963</td>\n",
       "      <td>0.1928</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1750</td>\n",
       "      <td>0.2560</td>\n",
       "      <td>0.0551</td>\n",
       "      <td>0.1751</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.2118</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>0.0283</td>\n",
       "      <td>0.0352</td>\n",
       "      <td>0.0352</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.0226</td>\n",
       "      <td>0.0362</td>\n",
       "      <td>0.1994</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>0.2104</td>\n",
       "      <td>0.1154</td>\n",
       "      <td>0.1154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1892</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1578</td>\n",
       "      <td>0.6733</td>\n",
       "      <td>0.1120</td>\n",
       "      <td>0.5540</td>\n",
       "      <td>0.2703</td>\n",
       "      <td>0.2629</td>\n",
       "      <td>0.0404</td>\n",
       "      <td>0.3189</td>\n",
       "      <td>0.3294</td>\n",
       "      <td>0.3253</td>\n",
       "      <td>0.0429</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0573</td>\n",
       "      <td>0.0421</td>\n",
       "      <td>0.0412</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0182</td>\n",
       "      <td>0.2963</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.1055</td>\n",
       "      <td>0.5254</td>\n",
       "      <td>0.2424</td>\n",
       "      <td>0.4152</td>\n",
       "      <td>0.0865</td>\n",
       "      <td>0.1092</td>\n",
       "      <td>0.0757</td>\n",
       "      <td>0.0884</td>\n",
       "      <td>0.0650</td>\n",
       "      <td>0.1165</td>\n",
       "      <td>0.0885</td>\n",
       "      <td>0.0435</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0549</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0471</td>\n",
       "      <td>0.3600</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1685</td>\n",
       "      <td>0.4102</td>\n",
       "      <td>0.2142</td>\n",
       "      <td>0.3911</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.4965</td>\n",
       "      <td>0.2291</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>0.1785</td>\n",
       "      <td>0.2083</td>\n",
       "      <td>0.2231</td>\n",
       "      <td>0.2626</td>\n",
       "      <td>0.3878</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1523</td>\n",
       "      <td>0.1680</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.1226</td>\n",
       "      <td>0.5642</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.5839</td>\n",
       "      <td>0.9937</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.5676</td>\n",
       "      <td>0.4846</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>0.2309</td>\n",
       "      <td>9,228,396.5130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30477</th>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.1948</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.4546</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.7112</td>\n",
       "      <td>0.4287</td>\n",
       "      <td>0.3315</td>\n",
       "      <td>0.3846</td>\n",
       "      <td>0.3646</td>\n",
       "      <td>0.3571</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1547</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2174</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3948</td>\n",
       "      <td>0.3776</td>\n",
       "      <td>0.4131</td>\n",
       "      <td>0.4616</td>\n",
       "      <td>0.4733</td>\n",
       "      <td>0.4503</td>\n",
       "      <td>0.3843</td>\n",
       "      <td>0.3151</td>\n",
       "      <td>0.4197</td>\n",
       "      <td>0.4287</td>\n",
       "      <td>0.3999</td>\n",
       "      <td>0.4597</td>\n",
       "      <td>0.3646</td>\n",
       "      <td>0.3560</td>\n",
       "      <td>0.3736</td>\n",
       "      <td>0.3909</td>\n",
       "      <td>0.3755</td>\n",
       "      <td>0.4072</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.3798</td>\n",
       "      <td>0.4185</td>\n",
       "      <td>0.4762</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>0.1260</td>\n",
       "      <td>0.4759</td>\n",
       "      <td>0.0620</td>\n",
       "      <td>0.3290</td>\n",
       "      <td>0.0544</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0344</td>\n",
       "      <td>0.0344</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.1818</td>\n",
       "      <td>0.0449</td>\n",
       "      <td>0.3345</td>\n",
       "      <td>0.2631</td>\n",
       "      <td>0.0920</td>\n",
       "      <td>0.0920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0811</td>\n",
       "      <td>0.2857</td>\n",
       "      <td>0.2884</td>\n",
       "      <td>0.4772</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>0.1351</td>\n",
       "      <td>0.0471</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.1929</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1973</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0182</td>\n",
       "      <td>0.0741</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.3412</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>0.1061</td>\n",
       "      <td>0.1109</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.3140</td>\n",
       "      <td>0.4268</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>0.0307</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.1032</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.0169</td>\n",
       "      <td>0.0216</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0186</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0331</td>\n",
       "      <td>0.0560</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0283</td>\n",
       "      <td>0.0780</td>\n",
       "      <td>0.0952</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.5839</td>\n",
       "      <td>0.9937</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.6719</td>\n",
       "      <td>0.2395</td>\n",
       "      <td>0.2309</td>\n",
       "      <td>4,762,607.5111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30478</th>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1121</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3568</td>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "      <td>0.3171</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.3928</td>\n",
       "      <td>0.1383</td>\n",
       "      <td>0.0253</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2462</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0249</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.0248</td>\n",
       "      <td>0.0281</td>\n",
       "      <td>0.0301</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0267</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.0253</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0246</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.0253</td>\n",
       "      <td>0.0255</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.0248</td>\n",
       "      <td>0.0248</td>\n",
       "      <td>0.0247</td>\n",
       "      <td>0.1673</td>\n",
       "      <td>0.1024</td>\n",
       "      <td>0.0472</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1645</td>\n",
       "      <td>0.1008</td>\n",
       "      <td>0.0687</td>\n",
       "      <td>0.0868</td>\n",
       "      <td>0.0868</td>\n",
       "      <td>0.1025</td>\n",
       "      <td>0.0658</td>\n",
       "      <td>0.0680</td>\n",
       "      <td>0.0565</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1435</td>\n",
       "      <td>0.1695</td>\n",
       "      <td>0.4931</td>\n",
       "      <td>0.4931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4942</td>\n",
       "      <td>0.1061</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.1071</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.1438</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3860</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4051</td>\n",
       "      <td>0.3929</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0732</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0480</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0275</td>\n",
       "      <td>0.1429</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.5839</td>\n",
       "      <td>0.9937</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.4895</td>\n",
       "      <td>0.2961</td>\n",
       "      <td>0.2309</td>\n",
       "      <td>4,516,483.6337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30479</th>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.1169</td>\n",
       "      <td>0.0948</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.0556</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.0755</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0695</td>\n",
       "      <td>0.0691</td>\n",
       "      <td>0.0927</td>\n",
       "      <td>0.0769</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0714</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0412</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0435</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.0654</td>\n",
       "      <td>0.0642</td>\n",
       "      <td>0.0741</td>\n",
       "      <td>0.0771</td>\n",
       "      <td>0.0711</td>\n",
       "      <td>0.0717</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.0795</td>\n",
       "      <td>0.0691</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>0.0686</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0618</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>0.0647</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.0650</td>\n",
       "      <td>0.0659</td>\n",
       "      <td>0.0641</td>\n",
       "      <td>0.7333</td>\n",
       "      <td>0.3825</td>\n",
       "      <td>0.1417</td>\n",
       "      <td>0.7338</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.1032</td>\n",
       "      <td>0.0699</td>\n",
       "      <td>0.0883</td>\n",
       "      <td>0.0883</td>\n",
       "      <td>0.0442</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.1502</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.1121</td>\n",
       "      <td>0.3460</td>\n",
       "      <td>0.3474</td>\n",
       "      <td>0.0932</td>\n",
       "      <td>0.0932</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4290</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.1747</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5162</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0122</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5666</td>\n",
       "      <td>0.1249</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.0304</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0280</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0321</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.5838</td>\n",
       "      <td>0.9934</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.3672</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>0.2309</td>\n",
       "      <td>4,893,371.0928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30480</th>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0909</td>\n",
       "      <td>0.1810</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>0.1150</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2462</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0435</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.1673</td>\n",
       "      <td>0.1024</td>\n",
       "      <td>0.0472</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1645</td>\n",
       "      <td>0.1679</td>\n",
       "      <td>0.1228</td>\n",
       "      <td>0.0718</td>\n",
       "      <td>0.0718</td>\n",
       "      <td>0.0847</td>\n",
       "      <td>0.0456</td>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1166</td>\n",
       "      <td>0.3946</td>\n",
       "      <td>0.3706</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6516</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2073</td>\n",
       "      <td>0.2207</td>\n",
       "      <td>0.2151</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5238</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3716</td>\n",
       "      <td>0.0480</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0321</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.5825</td>\n",
       "      <td>0.9894</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.7023</td>\n",
       "      <td>0.3214</td>\n",
       "      <td>0.2309</td>\n",
       "      <td>5,369,138.3727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       full_sq  life_sq  floor  max_floor  material  num_room  state  \\\n",
       "30471   0.0071   0.0028 0.0260     0.0690    0.0000    0.0000 0.6667   \n",
       "30472   0.0147   0.0040 0.1039     0.1379    0.0000    0.1111 0.0000   \n",
       "30473   0.0074   0.0034 0.0390     0.0345    0.2000    0.0556 0.3333   \n",
       "30474   0.0116   0.0048 0.2208     0.1379    0.0000    0.0556 0.6667   \n",
       "30475   0.0073   0.0053 0.2208     0.1379    0.0000    0.0000 0.0000   \n",
       "30476   0.0089   0.0040 0.2727     0.0000    0.0000    0.0000 0.0000   \n",
       "30477   0.0071   0.0040 0.1948     0.1379    0.0000    0.0000 0.0000   \n",
       "30478   0.0079   0.0040 0.0649     0.1121    0.0000    0.0000 0.3568   \n",
       "30479   0.0083   0.0038 0.1169     0.0948    0.8000    0.0556 0.3333   \n",
       "30480   0.0079   0.0058 0.0909     0.1810    0.0000    0.0000 0.0000   \n",
       "\n",
       "       product_type  sub_area  area_m  raion_popul  green_zone_part  \\\n",
       "30471             0        38  0.1180       0.7174           0.1598   \n",
       "30472             1       103  0.1150       0.0059           0.5810   \n",
       "30473             0        84  0.0386       0.5584           0.0746   \n",
       "30474             1       105  0.0952       0.0187           0.3062   \n",
       "30475             1       103  0.1150       0.0059           0.5810   \n",
       "30476             1        24  0.0370       0.3549           0.4017   \n",
       "30477             1       124  0.0453       0.4546           0.0963   \n",
       "30478             1       102  0.3171       0.0286           0.3928   \n",
       "30479             0       136  0.0649       0.0755           0.0584   \n",
       "30480             1       103  0.1150       0.0059           0.5810   \n",
       "\n",
       "       indust_part  children_preschool  preschool_quota  \\\n",
       "30471       0.0788              0.7300           1.0000   \n",
       "30472       0.0136              0.0052           0.2416   \n",
       "30473       0.4327              0.3268           0.1872   \n",
       "30474       0.0338              0.0165           0.2416   \n",
       "30475       0.0136              0.0052           0.2416   \n",
       "30476       0.4572              0.1957           0.1276   \n",
       "30477       0.7112              0.4287           0.3315   \n",
       "30478       0.1383              0.0253           0.2416   \n",
       "30479       0.0695              0.0691           0.0927   \n",
       "30480       0.0136              0.0052           0.2416   \n",
       "\n",
       "       preschool_education_centers_raion  children_school  \\\n",
       "30471                             0.8462           0.7784   \n",
       "30472                             0.0000           0.0051   \n",
       "30473                             0.5385           0.3378   \n",
       "30474                             0.0000           0.0159   \n",
       "30475                             0.0000           0.0051   \n",
       "30476                             0.3846           0.1967   \n",
       "30477                             0.3846           0.3646   \n",
       "30478                             0.0000           0.0244   \n",
       "30479                             0.0769           0.0608   \n",
       "30480                             0.0000           0.0051   \n",
       "\n",
       "       school_education_centers_raion  school_education_centers_top_20_raion  \\\n",
       "30471                          0.9286                                 0.5000   \n",
       "30472                          0.0000                                 0.0000   \n",
       "30473                          0.5000                                 0.0000   \n",
       "30474                          0.0000                                 0.0000   \n",
       "30475                          0.0000                                 0.0000   \n",
       "30476                          0.3571                                 0.0000   \n",
       "30477                          0.3571                                 0.0000   \n",
       "30478                          0.0000                                 0.0000   \n",
       "30479                          0.0714                                 0.0000   \n",
       "30480                          0.0000                                 0.0000   \n",
       "\n",
       "       hospital_beds_raion  healthcare_centers_raion  university_top_20_raion  \\\n",
       "30471               0.2462                    0.1667                   0.0000   \n",
       "30472               0.2462                    0.0000                   0.0000   \n",
       "30473               0.3192                    0.5000                   0.0000   \n",
       "30474               0.2462                    0.0000                   0.0000   \n",
       "30475               0.2462                    0.0000                   0.0000   \n",
       "30476               0.1454                    0.5000                   0.0000   \n",
       "30477               0.1547                    0.3333                   0.0000   \n",
       "30478               0.2462                    0.0000                   0.0000   \n",
       "30479               0.0412                    0.1667                   0.0000   \n",
       "30480               0.2462                    0.0000                   0.0000   \n",
       "\n",
       "       sport_objects_raion  additional_education_raion  \\\n",
       "30471               0.4483                      0.2500   \n",
       "30472               0.0000                      0.0000   \n",
       "30473               0.4483                      0.0000   \n",
       "30474               0.0000                      0.1250   \n",
       "30475               0.0000                      0.0000   \n",
       "30476               0.6207                      0.0625   \n",
       "30477               0.1379                      0.1875   \n",
       "30478               0.0345                      0.0000   \n",
       "30479               0.0000                      0.0000   \n",
       "30480               0.0000                      0.0000   \n",
       "\n",
       "       culture_objects_top_25  shopping_centers_raion  office_raion  \\\n",
       "30471                       0                  0.1739        0.0284   \n",
       "30472                       0                  0.0435        0.0000   \n",
       "30473                       0                  0.0870        0.0496   \n",
       "30474                       0                  0.0000        0.0000   \n",
       "30475                       0                  0.0435        0.0000   \n",
       "30476                       0                  0.2609        0.0426   \n",
       "30477                       0                  0.2174        0.0071   \n",
       "30478                       0                  0.0000        0.0071   \n",
       "30479                       0                  0.0435        0.0071   \n",
       "30480                       0                  0.0435        0.0000   \n",
       "\n",
       "       thermal_power_plant_raion  incineration_raion  oil_chemistry_raion  \\\n",
       "30471                          0                   0                    0   \n",
       "30472                          0                   0                    0   \n",
       "30473                          1                   0                    1   \n",
       "30474                          0                   0                    0   \n",
       "30475                          0                   0                    0   \n",
       "30476                          0                   0                    0   \n",
       "30477                          0                   0                    0   \n",
       "30478                          0                   0                    0   \n",
       "30479                          0                   0                    0   \n",
       "30480                          0                   0                    0   \n",
       "\n",
       "       radiation_raion  railroad_terminal_raion  big_market_raion  \\\n",
       "30471                0                        0                 0   \n",
       "30472                0                        0                 0   \n",
       "30473                1                        0                 0   \n",
       "30474                0                        0                 0   \n",
       "30475                0                        0                 0   \n",
       "30476                1                        0                 0   \n",
       "30477                0                        0                 0   \n",
       "30478                0                        0                 1   \n",
       "30479                0                        0                 0   \n",
       "30480                0                        0                 0   \n",
       "\n",
       "       nuclear_reactor_raion  detention_facility_raion  young_all  young_male  \\\n",
       "30471                      0                         0     0.7549      0.7727   \n",
       "30472                      0                         0     0.0052      0.0052   \n",
       "30473                      0                         0     0.3331      0.3246   \n",
       "30474                      0                         0     0.0163      0.0164   \n",
       "30475                      0                         0     0.0052      0.0052   \n",
       "30476                      0                         0     0.1966      0.1947   \n",
       "30477                      0                         0     0.3948      0.3776   \n",
       "30478                      0                         0     0.0249      0.0250   \n",
       "30479                      0                         0     0.0649      0.0654   \n",
       "30480                      0                         0     0.0052      0.0052   \n",
       "\n",
       "       young_female  work_all  work_male  work_female  ekder_all  ekder_male  \\\n",
       "30471        0.7360    0.7500     0.7399       0.7597     0.4517      0.3794   \n",
       "30472        0.0051    0.0058     0.0063       0.0054     0.0055      0.0046   \n",
       "30473        0.3422    0.5647     0.5570       0.5722     0.5869      0.4967   \n",
       "30474        0.0162    0.0184     0.0197       0.0171     0.0174      0.0146   \n",
       "30475        0.0051    0.0058     0.0063       0.0054     0.0055      0.0046   \n",
       "30476        0.1986    0.3976     0.4815       0.3160     0.2743      0.2373   \n",
       "30477        0.4131    0.4616     0.4733       0.4503     0.3843      0.3151   \n",
       "30478        0.0248    0.0281     0.0301       0.0262     0.0267      0.0223   \n",
       "30479        0.0642    0.0741     0.0771       0.0711     0.0717      0.0564   \n",
       "30480        0.0051    0.0058     0.0063       0.0054     0.0055      0.0046   \n",
       "\n",
       "       ekder_female  0_6_all  0_6_male  0_6_female  7_14_all  7_14_male  \\\n",
       "30471        0.4886   0.7300    0.7443      0.7145    0.7784     0.8013   \n",
       "30472        0.0060   0.0052    0.0053      0.0052    0.0051     0.0051   \n",
       "30473        0.6329   0.3268    0.3151      0.3394    0.3378     0.3339   \n",
       "30474        0.0189   0.0165    0.0165      0.0165    0.0159     0.0160   \n",
       "30475        0.0060   0.0052    0.0053      0.0052    0.0051     0.0051   \n",
       "30476        0.2933   0.1957    0.1918      0.1999    0.1967     0.1947   \n",
       "30477        0.4197   0.4287    0.3999      0.4597    0.3646     0.3560   \n",
       "30478        0.0289   0.0253    0.0252      0.0252    0.0244     0.0246   \n",
       "30479        0.0795   0.0691    0.0694      0.0686    0.0608     0.0618   \n",
       "30480        0.0060   0.0052    0.0053      0.0052    0.0051     0.0051   \n",
       "\n",
       "       7_14_female  0_17_all  0_17_male  0_17_female  0_13_all  0_13_male  \\\n",
       "30471       0.7544    0.7581     0.7767       0.7383    0.7505     0.7700   \n",
       "30472       0.0050    0.0053     0.0053       0.0052    0.0052     0.0052   \n",
       "30473       0.3419    0.3374     0.3301       0.3451    0.3315     0.3233   \n",
       "30474       0.0158    0.0165     0.0166       0.0163    0.0162     0.0162   \n",
       "30475       0.0050    0.0053     0.0053       0.0052    0.0052     0.0052   \n",
       "30476       0.1987    0.1975     0.1945       0.2006    0.1963     0.1928   \n",
       "30477       0.3736    0.3909     0.3755       0.4072    0.3985     0.3798   \n",
       "30478       0.0241    0.0253     0.0255       0.0250    0.0248     0.0248   \n",
       "30479       0.0596    0.0647     0.0645       0.0649    0.0650     0.0659   \n",
       "30480       0.0050    0.0053     0.0053       0.0052    0.0052     0.0052   \n",
       "\n",
       "       0_13_female  raion_build_count_with_material_info  build_count_brick  \\\n",
       "30471       0.7297                                1.0000             0.3690   \n",
       "30472       0.0051                                0.1673             0.1024   \n",
       "30473       0.3403                                0.3333             0.3825   \n",
       "30474       0.0161                                0.1673             0.1024   \n",
       "30475       0.0051                                0.1673             0.1024   \n",
       "30476       0.2000                                0.1750             0.2560   \n",
       "30477       0.4185                                0.4762             0.1898   \n",
       "30478       0.0247                                0.1673             0.1024   \n",
       "30479       0.0641                                0.7333             0.3825   \n",
       "30480       0.0051                                0.1673             0.1024   \n",
       "\n",
       "       build_count_monolith  raion_build_count_with_builddate_info  \\\n",
       "30471                0.9134                                 1.0000   \n",
       "30472                0.0472                                 0.1674   \n",
       "30473                0.0236                                 0.3335   \n",
       "30474                0.0472                                 0.1674   \n",
       "30475                0.0472                                 0.1674   \n",
       "30476                0.0551                                 0.1751   \n",
       "30477                0.1260                                 0.4759   \n",
       "30478                0.0472                                 0.1674   \n",
       "30479                0.1417                                 0.7338   \n",
       "30480                0.0472                                 0.1674   \n",
       "\n",
       "       build_count_before_1920  build_count_1946-1970  metro_min_avto  \\\n",
       "30471                   0.0916                 0.5195          0.0205   \n",
       "30472                   0.0000                 0.1645          0.0689   \n",
       "30473                   0.0000                 0.5172          0.0258   \n",
       "30474                   0.0000                 0.1645          0.1291   \n",
       "30475                   0.0000                 0.1645          0.0350   \n",
       "30476                   0.0027                 0.2118          0.0550   \n",
       "30477                   0.0620                 0.3290          0.0544   \n",
       "30478                   0.0000                 0.1645          0.1008   \n",
       "30479                   0.0027                 1.0000          0.1032   \n",
       "30480                   0.0000                 0.1645          0.1679   \n",
       "\n",
       "       metro_km_avto  metro_min_walk  metro_km_walk  kindergarten_km  \\\n",
       "30471         0.0098          0.0124         0.0124           0.0027   \n",
       "30472         0.0460          0.0581         0.0581           0.0410   \n",
       "30473         0.0150          0.0189         0.0189           0.0022   \n",
       "30474         0.0806          0.0964         0.0964           0.1096   \n",
       "30475         0.0230          0.0291         0.0291           0.0309   \n",
       "30476         0.0283          0.0352         0.0352           0.0250   \n",
       "30477         0.0272          0.0344         0.0344           0.0079   \n",
       "30478         0.0687          0.0868         0.0868           0.1025   \n",
       "30479         0.0699          0.0883         0.0883           0.0442   \n",
       "30480         0.1228          0.0718         0.0718           0.0847   \n",
       "\n",
       "       school_km  park_km  green_zone_km  industrial_km  water_treatment_km  \\\n",
       "30471     0.0158   0.0432         0.0310         0.0858              0.0147   \n",
       "30472     0.0281   0.0929         0.0000         0.0528              0.3334   \n",
       "30473     0.0041   0.0530         0.2929         0.0641              0.2425   \n",
       "30474     0.0747   0.1185         0.0128         0.0332              0.1012   \n",
       "30475     0.0260   0.0964         0.2155         0.0252              0.3489   \n",
       "30476     0.0226   0.0362         0.1994         0.0000              0.3048   \n",
       "30477     0.0009   0.0131         0.1818         0.0449              0.3345   \n",
       "30478     0.0658   0.0680         0.0565         0.0060              0.1435   \n",
       "30479     0.0178   0.1502         0.0197         0.1121              0.3460   \n",
       "30480     0.0456   0.1453         0.0000         0.1166              0.3946   \n",
       "\n",
       "       incineration_km  railroad_station_walk_km  railroad_station_walk_min  \\\n",
       "30471           0.1774                    0.1943                     0.1943   \n",
       "30472           0.3132                    0.2205                     0.2205   \n",
       "30473           0.1711                    0.1462                     0.1462   \n",
       "30474           0.2470                    0.4165                     0.4165   \n",
       "30475           0.3243                    0.1506                     0.1506   \n",
       "30476           0.2104                    0.1154                     0.1154   \n",
       "30477           0.2631                    0.0920                     0.0920   \n",
       "30478           0.1695                    0.4931                     0.4931   \n",
       "30479           0.3474                    0.0932                     0.0932   \n",
       "30480           0.3706                    0.0995                     0.0995   \n",
       "\n",
       "       ...  sport_count_1500  market_count_1500  green_part_2000  \\\n",
       "30471  ...            0.1081             0.0000           0.2014   \n",
       "30472  ...            0.0270             0.0000           0.6541   \n",
       "30473  ...            0.3243             0.4286           0.5431   \n",
       "30474  ...            0.0000             0.0000           0.4163   \n",
       "30475  ...            0.0000             0.0000           0.5127   \n",
       "30476  ...            0.1892             0.0000           0.1578   \n",
       "30477  ...            0.0811             0.2857           0.2884   \n",
       "30478  ...            0.0270             0.0000           0.4942   \n",
       "30479  ...            0.0000             0.0000           0.4290   \n",
       "30480  ...            0.0000             0.0000           0.6516   \n",
       "\n",
       "       prom_part_2000  office_count_2000  office_sqm_2000  trc_count_2000  \\\n",
       "30471          0.0210             0.0000           0.0000          0.0000   \n",
       "30472          0.0724             0.0000           0.0000          0.0270   \n",
       "30473          0.1873             0.0240           0.0223          0.0811   \n",
       "30474          0.0533             0.0000           0.0000          0.0000   \n",
       "30475          0.0556             0.0000           0.0000          0.0541   \n",
       "30476          0.6733             0.1120           0.5540          0.2703   \n",
       "30477          0.4772             0.0080           0.0219          0.1351   \n",
       "30478          0.1061             0.0000           0.0000          0.0000   \n",
       "30479          0.0025             0.0000           0.0000          0.0270   \n",
       "30480          0.0036             0.0000           0.0000          0.0270   \n",
       "\n",
       "       trc_sqm_2000  cafe_count_2000  cafe_sum_2000_min_price_avg  \\\n",
       "30471        0.0000           0.0027                       0.3750   \n",
       "30472        0.0020           0.0063                       0.2449   \n",
       "30473        0.0058           0.0251                       0.1786   \n",
       "30474        0.0000           0.0009                       0.0000   \n",
       "30475        0.0090           0.0063                       0.2449   \n",
       "30476        0.2629           0.0404                       0.3189   \n",
       "30477        0.0471           0.0063                       0.1929   \n",
       "30478        0.0000           0.0009                       0.1071   \n",
       "30479        0.0069           0.0018                       0.1875   \n",
       "30480        0.0069           0.0000                       0.2073   \n",
       "\n",
       "       cafe_sum_2000_max_price_avg  cafe_avg_price_2000  \\\n",
       "30471                       0.3333               0.3493   \n",
       "30472                       0.2619               0.2554   \n",
       "30473                       0.1975               0.1903   \n",
       "30474                       0.0000               0.0000   \n",
       "30475                       0.2619               0.2554   \n",
       "30476                       0.3294               0.3253   \n",
       "30477                       0.2000               0.1973   \n",
       "30478                       0.1667               0.1438   \n",
       "30479                       0.1667               0.1747   \n",
       "30480                       0.2207               0.2151   \n",
       "\n",
       "       cafe_count_2000_na_price  cafe_count_2000_price_500  \\\n",
       "30471                    0.0000                     0.0000   \n",
       "30472                    0.0000                     0.0036   \n",
       "30473                    0.0143                     0.0252   \n",
       "30474                    0.0000                     0.0036   \n",
       "30475                    0.0000                     0.0036   \n",
       "30476                    0.0429                     0.0252   \n",
       "30477                    0.0286                     0.0036   \n",
       "30478                    0.0000                     0.0000   \n",
       "30479                    0.0000                     0.0036   \n",
       "30480                    0.0000                     0.0000   \n",
       "\n",
       "       cafe_count_2000_price_1000  cafe_count_2000_price_1500  \\\n",
       "30471                      0.0000                      0.0115   \n",
       "30472                      0.0115                      0.0077   \n",
       "30473                      0.0458                      0.0230   \n",
       "30474                      0.0000                      0.0000   \n",
       "30475                      0.0115                      0.0077   \n",
       "30476                      0.0573                      0.0421   \n",
       "30477                      0.0076                      0.0077   \n",
       "30478                      0.0038                      0.0000   \n",
       "30479                      0.0000                      0.0038   \n",
       "30480                      0.0000                      0.0000   \n",
       "\n",
       "       cafe_count_2000_price_2500  cafe_count_2000_price_4000  \\\n",
       "30471                      0.0000                      0.0000   \n",
       "30472                      0.0059                      0.0000   \n",
       "30473                      0.0118                      0.0000   \n",
       "30474                      0.0000                      0.0000   \n",
       "30475                      0.0059                      0.0000   \n",
       "30476                      0.0412                      0.0123   \n",
       "30477                      0.0000                      0.0000   \n",
       "30478                      0.0000                      0.0000   \n",
       "30479                      0.0000                      0.0000   \n",
       "30480                      0.0000                      0.0000   \n",
       "\n",
       "       cafe_count_2000_price_high  big_church_count_2000  church_count_2000  \\\n",
       "30471                      0.0000                 0.0143             0.0185   \n",
       "30472                      0.0000                 0.0143             0.0185   \n",
       "30473                      0.0000                 0.0286             0.0185   \n",
       "30474                      0.0000                 0.0000             0.0278   \n",
       "30475                      0.0000                 0.0143             0.0185   \n",
       "30476                      0.0625                 0.0143             0.0185   \n",
       "30477                      0.0000                 0.0000             0.0000   \n",
       "30478                      0.0000                 0.0000             0.0185   \n",
       "30479                      0.0000                 0.0000             0.0093   \n",
       "30480                      0.0000                 0.0000             0.0185   \n",
       "\n",
       "       mosque_count_2000  leisure_count_2000  sport_count_2000  \\\n",
       "30471             1.0000              0.0000            0.0926   \n",
       "30472             0.0000              0.0000            0.0185   \n",
       "30473             0.0000              0.0727            0.2593   \n",
       "30474             0.0000              0.0000            0.0000   \n",
       "30475             0.0000              0.0000            0.0556   \n",
       "30476             0.0000              0.0182            0.2963   \n",
       "30477             0.0000              0.0182            0.0741   \n",
       "30478             0.0000              0.0000            0.0185   \n",
       "30479             0.0000              0.0000            0.0000   \n",
       "30480             0.0000              0.0000            0.0000   \n",
       "\n",
       "       market_count_2000  green_part_3000  office_count_3000  office_sqm_3000  \\\n",
       "30471             0.0000           0.1951             0.0000           0.0000   \n",
       "30472             0.0000           0.5317             0.0000           0.0000   \n",
       "30473             0.5000           0.6180             0.0162           0.0352   \n",
       "30474             0.0000           0.4689             0.0000           0.0000   \n",
       "30475             0.0000           0.5607             0.0000           0.0000   \n",
       "30476             0.0000           0.1811             0.1055           0.5254   \n",
       "30477             0.2500           0.3412             0.0061           0.0300   \n",
       "30478             0.0000           0.3860             0.0020           0.0139   \n",
       "30479             0.0000           0.5162             0.0000           0.0000   \n",
       "30480             0.0000           0.5238             0.0000           0.0000   \n",
       "\n",
       "       trc_count_3000  trc_sqm_3000  cafe_count_3000  \\\n",
       "30471          0.0455        0.0275           0.0066   \n",
       "30472          0.0303        0.0083           0.0055   \n",
       "30473          0.0909        0.0147           0.0204   \n",
       "30474          0.0000        0.0000           0.0006   \n",
       "30475          0.0303        0.0083           0.0050   \n",
       "30476          0.2424        0.4152           0.0865   \n",
       "30477          0.1061        0.1109           0.0066   \n",
       "30478          0.0000        0.0000           0.0022   \n",
       "30479          0.0152        0.0064           0.0028   \n",
       "30480          0.0152        0.0064           0.0022   \n",
       "\n",
       "       cafe_count_3000_na_price  cafe_count_3000_price_500  \\\n",
       "30471                    0.0084                     0.0045   \n",
       "30472                    0.0000                     0.0022   \n",
       "30473                    0.0084                     0.0178   \n",
       "30474                    0.0000                     0.0022   \n",
       "30475                    0.0000                     0.0022   \n",
       "30476                    0.1092                     0.0757   \n",
       "30477                    0.0252                     0.0045   \n",
       "30478                    0.0000                     0.0000   \n",
       "30479                    0.0000                     0.0022   \n",
       "30480                    0.0000                     0.0022   \n",
       "\n",
       "       cafe_count_3000_price_1000  cafe_count_3000_price_1500  \\\n",
       "30471                      0.0045                      0.0157   \n",
       "30472                      0.0136                      0.0045   \n",
       "30473                      0.0431                      0.0157   \n",
       "30474                      0.0000                      0.0000   \n",
       "30475                      0.0113                      0.0045   \n",
       "30476                      0.0884                      0.0650   \n",
       "30477                      0.0113                      0.0045   \n",
       "30478                      0.0023                      0.0022   \n",
       "30479                      0.0045                      0.0022   \n",
       "30480                      0.0023                      0.0022   \n",
       "\n",
       "       cafe_count_3000_price_2500  cafe_count_3000_price_4000  \\\n",
       "30471                      0.0000                      0.0000   \n",
       "30472                      0.0038                      0.0000   \n",
       "30473                      0.0075                      0.0000   \n",
       "30474                      0.0000                      0.0000   \n",
       "30475                      0.0038                      0.0000   \n",
       "30476                      0.1165                      0.0885   \n",
       "30477                      0.0000                      0.0000   \n",
       "30478                      0.0038                      0.0088   \n",
       "30479                      0.0038                      0.0000   \n",
       "30480                      0.0038                      0.0000   \n",
       "\n",
       "       cafe_count_3000_price_high  big_church_count_3000  church_count_3000  \\\n",
       "30471                      0.0000                 0.0098             0.0183   \n",
       "30472                      0.0000                 0.0098             0.0305   \n",
       "30473                      0.0000                 0.0196             0.0183   \n",
       "30474                      0.0000                 0.0000             0.0244   \n",
       "30475                      0.0000                 0.0098             0.0244   \n",
       "30476                      0.0435                 0.0294             0.0549   \n",
       "30477                      0.0000                 0.0000             0.0183   \n",
       "30478                      0.0000                 0.0000             0.0244   \n",
       "30479                      0.0000                 0.0000             0.0122   \n",
       "30480                      0.0000                 0.0000             0.0183   \n",
       "\n",
       "       mosque_count_3000  leisure_count_3000  sport_count_3000  \\\n",
       "30471             0.5000              0.0000            0.0700   \n",
       "30472             0.0000              0.0000            0.0700   \n",
       "30473             0.0000              0.0588            0.2200   \n",
       "30474             0.0000              0.0000            0.0000   \n",
       "30475             0.0000              0.0000            0.0600   \n",
       "30476             0.0000              0.0471            0.3600   \n",
       "30477             0.0000              0.0118            0.0500   \n",
       "30478             0.0000              0.0000            0.0100   \n",
       "30479             0.0000              0.0000            0.0000   \n",
       "30480             0.0000              0.0000            0.0000   \n",
       "\n",
       "       market_count_3000  green_part_5000  prom_part_5000  office_count_5000  \\\n",
       "30471             0.0000           0.2510          0.1580             0.0013   \n",
       "30472             0.0000           0.4946          0.2642             0.0025   \n",
       "30473             0.4000           0.3072          0.4720             0.0342   \n",
       "30474             0.0000           0.2882          0.0511             0.0000   \n",
       "30475             0.0000           0.4462          0.2381             0.0013   \n",
       "30476             0.2000           0.1685          0.4102             0.2142   \n",
       "30477             0.2000           0.3140          0.4268             0.0139   \n",
       "30478             0.0000           0.4051          0.3929             0.0038   \n",
       "30479             0.0000           0.5666          0.1249             0.0013   \n",
       "30480             0.0000           0.3716          0.0480             0.0000   \n",
       "\n",
       "       office_sqm_5000  trc_count_5000  trc_sqm_5000  cafe_count_5000  \\\n",
       "30471           0.0030          0.0667        0.0652           0.0072   \n",
       "30472           0.0140          0.0500        0.0504           0.0076   \n",
       "30473           0.0337          0.2167        0.2234           0.0677   \n",
       "30474           0.0000          0.0000        0.0000           0.0019   \n",
       "30475           0.0092          0.0333        0.0439           0.0076   \n",
       "30476           0.3911          0.3333        0.4965           0.2291   \n",
       "30477           0.0307          0.1250        0.1032           0.0197   \n",
       "30478           0.0086          0.0333        0.0732           0.0098   \n",
       "30479           0.0092          0.0250        0.0304           0.0049   \n",
       "30480           0.0000          0.0167        0.0048           0.0068   \n",
       "\n",
       "       cafe_count_5000_na_price  cafe_count_5000_price_500  \\\n",
       "30471                    0.0115                     0.0077   \n",
       "30472                    0.0115                     0.0062   \n",
       "30473                    0.0287                     0.0815   \n",
       "30474                    0.0000                     0.0015   \n",
       "30475                    0.0057                     0.0062   \n",
       "30476                    0.2529                     0.1785   \n",
       "30477                    0.0345                     0.0169   \n",
       "30478                    0.0057                     0.0154   \n",
       "30479                    0.0057                     0.0031   \n",
       "30480                    0.0000                     0.0031   \n",
       "\n",
       "       cafe_count_5000_price_1000  cafe_count_5000_price_1500  \\\n",
       "30471                      0.0062                      0.0125   \n",
       "30472                      0.0123                      0.0062   \n",
       "30473                      0.0988                      0.0655   \n",
       "30474                      0.0000                      0.0016   \n",
       "30475                      0.0123                      0.0078   \n",
       "30476                      0.2083                      0.2231   \n",
       "30477                      0.0216                      0.0187   \n",
       "30478                      0.0093                      0.0078   \n",
       "30479                      0.0093                      0.0047   \n",
       "30480                      0.0123                      0.0078   \n",
       "\n",
       "       cafe_count_5000_price_2500  cafe_count_5000_price_4000  \\\n",
       "30471                      0.0000                      0.0000   \n",
       "30472                      0.0027                      0.0068   \n",
       "30473                      0.0292                      0.0272   \n",
       "30474                      0.0027                      0.0136   \n",
       "30475                      0.0027                      0.0068   \n",
       "30476                      0.2626                      0.3878   \n",
       "30477                      0.0186                      0.0136   \n",
       "30478                      0.0053                      0.0136   \n",
       "30479                      0.0027                      0.0000   \n",
       "30480                      0.0053                      0.0068   \n",
       "\n",
       "       cafe_count_5000_price_high  big_church_count_5000  church_count_5000  \\\n",
       "30471                      0.0000                 0.0066             0.0400   \n",
       "30472                      0.0000                 0.0132             0.0440   \n",
       "30473                      0.0000                 0.0662             0.0840   \n",
       "30474                      0.0000                 0.0000             0.0400   \n",
       "30475                      0.0000                 0.0132             0.0480   \n",
       "30476                      0.4000                 0.1523             0.1680   \n",
       "30477                      0.0000                 0.0331             0.0560   \n",
       "30478                      0.0000                 0.0199             0.0480   \n",
       "30479                      0.0000                 0.0066             0.0280   \n",
       "30480                      0.0000                 0.0132             0.0360   \n",
       "\n",
       "       mosque_count_5000  leisure_count_5000  sport_count_5000  \\\n",
       "30471             0.5000              0.0000            0.0642   \n",
       "30472             0.0000              0.0094            0.0550   \n",
       "30473             0.0000              0.0943            0.3257   \n",
       "30474             0.0000              0.0000            0.0092   \n",
       "30475             0.0000              0.0094            0.0505   \n",
       "30476             0.5000              0.1226            0.5642   \n",
       "30477             0.0000              0.0283            0.0780   \n",
       "30478             0.0000              0.0000            0.0275   \n",
       "30479             0.0000              0.0000            0.0321   \n",
       "30480             0.0000              0.0000            0.0321   \n",
       "\n",
       "       market_count_5000   year  year_month  living_area_ratio  \\\n",
       "30471             0.0476 1.0000          47             0.0056   \n",
       "30472             0.0476 1.0000          47             0.0063   \n",
       "30473             0.5238 1.0000          47             0.0065   \n",
       "30474             0.0000 1.0000          47             0.0061   \n",
       "30475             0.0476 1.0000          47             0.0106   \n",
       "30476             0.3333 1.0000          47             0.0063   \n",
       "30477             0.0952 1.0000          47             0.0063   \n",
       "30478             0.1429 1.0000          47             0.0063   \n",
       "30479             0.0000 1.0000          47             0.0066   \n",
       "30480             0.0000 1.0000          47             0.0106   \n",
       "\n",
       "       non_living_area  non_living_area_ratio  room_area_avg  relative_floor  \\\n",
       "30471           0.5839                 0.9944         0.0083          0.0060   \n",
       "30472           0.5839                 0.9937         0.0074          0.0127   \n",
       "30473           0.5837                 0.9935         0.0050          0.0162   \n",
       "30474           0.5846                 0.9939         0.0072          0.0270   \n",
       "30475           0.5825                 0.9894         0.0160          0.0270   \n",
       "30476           0.5839                 0.9937         0.0074          0.5676   \n",
       "30477           0.5839                 0.9937         0.0074          0.0238   \n",
       "30478           0.5839                 0.9937         0.0074          0.0160   \n",
       "30479           0.5838                 0.9934         0.0057          0.0203   \n",
       "30480           0.5825                 0.9894         0.0174          0.0086   \n",
       "\n",
       "       sub_area_building_height_avg  sub_area_kremlin_dist_avg  \\\n",
       "30471                        0.5248                     0.3324   \n",
       "30472                        0.7023                     0.3214   \n",
       "30473                        0.2586                     0.1357   \n",
       "30474                        0.3331                     0.3632   \n",
       "30475                        0.7023                     0.3214   \n",
       "30476                        0.4846                     0.0964   \n",
       "30477                        0.6719                     0.2395   \n",
       "30478                        0.4895                     0.2961   \n",
       "30479                        0.3672                     0.3590   \n",
       "30480                        0.7023                     0.3214   \n",
       "\n",
       "       sales_year_month      price_doc  \n",
       "30471            0.2309 5,462,395.0703  \n",
       "30472            0.2309 8,067,024.0976  \n",
       "30473            0.2309 5,029,510.1960  \n",
       "30474            0.2309 6,521,690.2788  \n",
       "30475            0.2309 5,373,426.2699  \n",
       "30476            0.2309 9,228,396.5130  \n",
       "30477            0.2309 4,762,607.5111  \n",
       "30478            0.2309 4,516,483.6337  \n",
       "30479            0.2309 4,893,371.0928  \n",
       "30480            0.2309 5,369,138.3727  \n",
       "\n",
       "[10 rows x 250 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test['price_doc'] = np.expm1(pred)\n",
    "x_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "88d0a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the test data with predicted price into the csv file\n",
    "x_test['id'] = test_id\n",
    "x_test[['id','price_doc']].to_csv('./output_models/output_custom.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da8196e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "1d649c4a-48b1-4ddf-9c6c-61154fc3eb69",
    "3fc7bc7b-a329-4d0a-8378-ecd214a94a5c",
    "f43233fd-d92c-45c3-a983-6760d17f8af2",
    "3db5652b-0e4c-4483-9ae4-c12cfc02cad9",
    "eacd3859-1644-49e7-b823-ecaa1a1e1309",
    "9fd74c3a-7d03-4ef1-88f5-512543ea2743",
    "ad7d8de9-ce0e-49ff-9182-d43884be8de9",
    "76333052-72ea-49d9-9bae-dfc14cb16a97",
    "b2c02a57-93c2-48a1-811f-a0a098007051",
    "cb448266-8c1d-43f4-a546-cd1da680cc5d",
    "e7611d91-b2bb-463f-afec-d61387c96aa9",
    "f6467dc8-edbf-42f4-a5da-2b943280d570",
    "a249ef17-e4c0-4dc6-9c37-d99bddfe4397",
    "dc7fd1ae-6b64-477d-a07d-2c39691ee4c9",
    "dfcbad98-798b-4726-9945-65f0621e9a6c",
    "13a68d3a-924b-418f-aa40-7e2d781d7cd4",
    "12015471-eacf-408c-823a-99a86a89a3f5",
    "81987137-ad1d-4c38-b4fd-5afa0d179f45",
    "5a556eaa-ce64-42cd-bc3b-012dd80e6abb"
   ],
   "name": "ExploratoryDataAnalysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
