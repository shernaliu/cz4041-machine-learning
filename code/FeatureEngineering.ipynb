{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kk43419t4dzT",
    "outputId": "2bf00b6f-d7f6-4a48-a78e-05bdb6b9d7d0"
   },
   "outputs": [],
   "source": [
    "# Google Drive mount for colab\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "# After mounting, /drive/MyDrive/ should appear on the left in Files tab\n",
    "# Go to your own Google Drive, create a /cz4041/ folder, and upload the zip and csv files there\n",
    "# It should appear in the files tab under /drive/MyDrive/cz4041/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1b36853f-29b3-48d1-a364-19db616a4801"
   },
   "source": [
    "# Data Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aea06625-2d66-4ff1-92cc-6063222bb130"
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import pprint\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# pandas display options\n",
    "pd.set_option('display.max_columns', 150)\n",
    "pd.set_option('display.max_rows', 300)\n",
    "pd.set_option('float_format', '{:,.4f}'.format) # All float will be displayed in 4 d.p. with comma to separate thousands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "c9a97915-f943-48ad-9cda-30118a703fd0"
   },
   "outputs": [],
   "source": [
    "# datasets paths\n",
    "path = \"../data\"\n",
    "#path = \"/content/drive/MyDrive/cz4041/\" # path to Google Drive, for colab\n",
    "macro = os.path.join(path, \"macro.csv\")\n",
    "train = os.path.join(path, \"train.zip\")\n",
    "test = os.path.join(path,  \"test.zip\")\n",
    "\n",
    "# place all datasets paths in a datasets dict\n",
    "datasets = {}\n",
    "datasets['macro'] = macro\n",
    "datasets['train'] = train\n",
    "datasets['test'] = test\n",
    "\n",
    "# load dataframes into dfs dict\n",
    "dfs = {}\n",
    "for dataset_name, path in datasets.items():\n",
    "    df = pd.read_csv(path)\n",
    "    dfs[dataset_name] = df\n",
    "\n",
    "# assign to own df variables when you want to use them individually\n",
    "df_macro = dfs['macro']\n",
    "df_train = dfs['train']\n",
    "df_test = dfs['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0c7eca0-bddb-4322-967b-837c1694db7d",
    "tags": []
   },
   "source": [
    "## Overview of datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "329adc68-c5ea-4656-a7dc-26dccbff516d"
   },
   "source": [
    "**Dataset size & Number of distinct datatypes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "393d7704-805e-49e9-afd5-c17434b341ea",
    "outputId": "9029b006-6c55-4c4c-ad01-7d3ac56956af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Dataset size - macro: (2484, 100) ======\n",
      "Number of distinct datatypes: \n",
      "<class 'numpy.dtype[float64]'>    94\n",
      "<class 'numpy.dtype[object_]'>     4\n",
      "<class 'numpy.dtype[int64]'>       2\n",
      "dtype: int64\n",
      "====== Dataset size - train: (30471, 292) ======\n",
      "Number of distinct datatypes: \n",
      "<class 'numpy.dtype[int64]'>      157\n",
      "<class 'numpy.dtype[float64]'>    119\n",
      "<class 'numpy.dtype[object_]'>     16\n",
      "dtype: int64\n",
      "====== Dataset size - test: (7662, 291) ======\n",
      "Number of distinct datatypes: \n",
      "<class 'numpy.dtype[int64]'>      159\n",
      "<class 'numpy.dtype[float64]'>    116\n",
      "<class 'numpy.dtype[object_]'>     16\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, df in dfs.items():\n",
    "    print(\"====== Dataset size - {}: {} ======\".format(dataset_name , df.shape))\n",
    "    print(\"Number of distinct datatypes: \\n{}\".format(df.dtypes.map(type).value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare dataset for datacleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0d607c8a-6fa0-4aa6-9fb5-06b317a94dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38133, 291)\n"
     ]
    }
   ],
   "source": [
    "# Copy train price out to facilitate train & test split later\n",
    "trainPrice = dfs[\"train\"][[\"id\", \"price_doc\"]].copy()\n",
    "\n",
    "# Concat train dataset (minus price_doc) and test dataset for data cleaning\n",
    "trainNoPrice = dfs[\"train\"].drop(\"price_doc\", axis = 1)\n",
    "\n",
    "mergeData = pd.concat([trainNoPrice, dfs[\"test\"]])\n",
    "print(mergeData.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since we are predicting price, we will want to drop columns that have low correlation value to 'price_doc'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_year                     0.0022\n",
      "kitch_sq                       0.0287\n",
      "school_quota                   0.0140\n",
      "culture_objects_top_25_raion   0.0443\n",
      "full_all                       0.0253\n",
      "male_f                         0.0264\n",
      "female_f                       0.0243\n",
      "16_29_all                      0.0223\n",
      "16_29_male                     0.0231\n",
      "16_29_female                   0.0216\n",
      "build_count_block              0.0315\n",
      "build_count_wood               0.0425\n",
      "build_count_frame              0.0303\n",
      "build_count_panel              0.0201\n",
      "build_count_foam               0.0107\n",
      "build_count_slag               0.0240\n",
      "build_count_mix                0.0330\n",
      "build_count_1921-1945          0.0203\n",
      "build_count_1971-1995          0.0097\n",
      "build_count_after_1995         0.0259\n",
      "cemetery_km                    0.0249\n",
      "ID_railroad_station_walk       0.0218\n",
      "water_km                       0.0266\n",
      "mkad_km                        0.0206\n",
      "big_market_km                  0.0483\n",
      "prom_part_500                  0.0090\n",
      "trc_sqm_500                    0.0004\n",
      "cafe_sum_500_min_price_avg     0.0364\n",
      "cafe_sum_500_max_price_avg     0.0379\n",
      "cafe_avg_price_500             0.0374\n",
      "cafe_count_500_na_price        0.0492\n",
      "big_church_count_500           0.0262\n",
      "church_count_500               0.0147\n",
      "mosque_count_500               0.0185\n",
      "market_count_500               0.0404\n",
      "trc_sqm_1000                   0.0416\n",
      "cafe_count_1000_price_4000     0.0363\n",
      "prom_part_3000                 0.0227\n",
      "cafe_sum_3000_min_price_avg    0.0051\n",
      "cafe_sum_3000_max_price_avg    0.0022\n",
      "cafe_avg_price_3000            0.0033\n",
      "cafe_sum_5000_min_price_avg    0.0322\n",
      "cafe_sum_5000_max_price_avg    0.0333\n",
      "cafe_avg_price_5000            0.0329\n",
      "Name: price_doc, dtype: float64 , 44\n"
     ]
    }
   ],
   "source": [
    "# get the correlation to 'price_doc' in train dataset\n",
    "trainCorr = abs(dfs[\"train\"].corr()[\"price_doc\"])\n",
    "\n",
    "# pick out features that have less than 5% correlation to be dropped\n",
    "threshold = trainCorr <= 0.05\n",
    "lowCorrFeats = trainCorr[threshold]\n",
    "print(lowCorrFeats, \",\", len(lowCorrFeats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38133, 247)\n"
     ]
    }
   ],
   "source": [
    "# drop\n",
    "mergeData.drop(list(lowCorrFeats.index), axis = 1, inplace = True)\n",
    "print(mergeData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38133, 241)\n"
     ]
    }
   ],
   "source": [
    "# Drop columns with 'ID' in name as they do not provide much value\n",
    "\n",
    "IDfeats = [feat for feat in mergeData.columns if \"ID\" in feat]\n",
    "mergeData.drop(IDfeats, axis = 1, inplace = True)\n",
    "print(mergeData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace data in the following columns\n",
    "mergeData.state.replace({33:3},inplace=True)\n",
    "mergeData[\"material\"].replace(to_replace = 3, value = 1, inplace = True)\n",
    "mergeData[\"full_sq\"].replace(to_replace = 0, value = np.nan, inplace = True)\n",
    "mergeData[\"max_floor\"].replace(to_replace = 0, value = np.nan, inplace = True)\n",
    "mergeData[\"num_room\"].replace(to_replace = 0, value = np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38133, 251)\n"
     ]
    }
   ],
   "source": [
    "# create 'year' and 'year_month' features from 'timestamp'\n",
    "mergeData[\"year\"] = mergeData[\"timestamp\"].apply(lambda x: int(x[0:4]))\n",
    "mergeData[\"year_month\"] = mergeData[\"timestamp\"].apply(lambda x: x[0:7])\\\n",
    "\n",
    "# create 'living_area_ratio', 'non_living_area' and 'non_living_area_ratio' from 'life_sq' and 'full_sq'\n",
    "mergeData[\"living_area_ratio\"] = mergeData[\"life_sq\"] / mergeData[\"full_sq\"]\n",
    "mergeData[\"non_living_area\"] = mergeData[\"full_sq\"] - mergeData[\"life_sq\"]\n",
    "mergeData[\"non_living_area_ratio\"] = mergeData[\"non_living_area\"] / mergeData[\"full_sq\"]\n",
    "\n",
    "# create 'room_area_avg' from 'life_sq' and 'num_room'\n",
    "mergeData[\"room_area_avg\"] = mergeData[\"life_sq\"] / mergeData[\"num_room\"]\n",
    "\n",
    "# create 'relative_floor' from 'floor' and 'max_floor'\n",
    "mergeData[\"relative_floor\"] = mergeData[\"floor\"] / mergeData[\"max_floor\"]\n",
    "\n",
    "# create 'sub_area_building_height_avg' from 'sub_area' and 'max_floor'\n",
    "sub_area_building_avg = mergeData.groupby('sub_area').agg({'max_floor':np.mean}).reset_index().rename(columns={'max_floor':'sub_area_building_height_avg'})\n",
    "mergeData = pd.merge(mergeData, sub_area_building_avg, on = ['sub_area'], how = 'left')\n",
    "\n",
    "# create 'sub_area_kremlin_dist_avg' from 'sub_area' and 'kremlin_km'\n",
    "kremlin_dist = mergeData.groupby('sub_area').agg({'kremlin_km':np.nanmean}).reset_index().rename(columns={'kremlin_km':'sub_area_kremlin_dist_avg'})\n",
    "mergeData = pd.merge(mergeData, kremlin_dist, on = ['sub_area'], how = 'left')\n",
    "\n",
    "# create 'sales_year_month' from 'year_month'\n",
    "sales_year_month = mergeData.groupby('year_month').size().reset_index().rename(columns={0:'sales_year_month'})\n",
    "mergeData = pd.merge(mergeData, sales_year_month, on = ['year_month'], how = 'left')\n",
    "\n",
    "print(mergeData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hospital_beds_raion                      17859\n",
      "room_area_avg                            14897\n",
      "state                                    14253\n",
      "relative_floor                           10355\n",
      "max_floor                                10355\n",
      "num_room                                  9586\n",
      "material                                  9572\n",
      "preschool_quota                           8284\n",
      "cafe_sum_1000_min_price_avg               7746\n",
      "cafe_avg_price_1000                       7746\n",
      "cafe_sum_1000_max_price_avg               7746\n",
      "non_living_area_ratio                     7562\n",
      "non_living_area                           7562\n",
      "living_area_ratio                         7562\n",
      "life_sq                                   7559\n",
      "raion_build_count_with_material_info      6209\n",
      "build_count_brick                         6209\n",
      "build_count_before_1920                   6209\n",
      "raion_build_count_with_builddate_info     6209\n",
      "build_count_monolith                      6209\n",
      "build_count_1946-1970                     6209\n",
      "cafe_avg_price_1500                       5020\n",
      "cafe_sum_1500_min_price_avg               5020\n",
      "cafe_sum_1500_max_price_avg               5020\n",
      "cafe_sum_2000_max_price_avg               2149\n",
      "cafe_sum_2000_min_price_avg               2149\n",
      "cafe_avg_price_2000                       2149\n",
      "prom_part_5000                             270\n",
      "floor                                      167\n",
      "metro_min_walk                              59\n",
      "railroad_station_walk_km                    59\n",
      "railroad_station_walk_min                   59\n",
      "metro_km_walk                               59\n",
      "product_type                                33\n",
      "green_part_2000                             19\n",
      "full_sq                                      3\n",
      "dtype: int64\n",
      "\r\n",
      "Missing Values Count: 36\n"
     ]
    }
   ],
   "source": [
    "# find out columns that have missing values\n",
    "missing_vals = ((mergeData.isna().sum()))\n",
    "missing_vals.sort_values(ascending = False, inplace = True)\n",
    "print(missing_vals[missing_vals > 0])\n",
    "print(\"\\r\\nMissing Values Count: \" + str(len(missing_vals[missing_vals > 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hospital_beds_raion                     0.4683\n",
      "room_area_avg                           0.3907\n",
      "state                                   0.3738\n",
      "relative_floor                          0.2715\n",
      "max_floor                               0.2715\n",
      "num_room                                0.2514\n",
      "material                                0.2510\n",
      "preschool_quota                         0.2172\n",
      "cafe_sum_1000_min_price_avg             0.2031\n",
      "cafe_avg_price_1000                     0.2031\n",
      "cafe_sum_1000_max_price_avg             0.2031\n",
      "non_living_area_ratio                   0.1983\n",
      "non_living_area                         0.1983\n",
      "living_area_ratio                       0.1983\n",
      "life_sq                                 0.1982\n",
      "raion_build_count_with_material_info    0.1628\n",
      "build_count_brick                       0.1628\n",
      "build_count_before_1920                 0.1628\n",
      "raion_build_count_with_builddate_info   0.1628\n",
      "build_count_monolith                    0.1628\n",
      "build_count_1946-1970                   0.1628\n",
      "cafe_avg_price_1500                     0.1316\n",
      "cafe_sum_1500_min_price_avg             0.1316\n",
      "cafe_sum_1500_max_price_avg             0.1316\n",
      "cafe_sum_2000_max_price_avg             0.0564\n",
      "cafe_sum_2000_min_price_avg             0.0564\n",
      "cafe_avg_price_2000                     0.0564\n",
      "prom_part_5000                          0.0071\n",
      "floor                                   0.0044\n",
      "metro_min_walk                          0.0015\n",
      "railroad_station_walk_km                0.0015\n",
      "railroad_station_walk_min               0.0015\n",
      "metro_km_walk                           0.0015\n",
      "product_type                            0.0009\n",
      "green_part_2000                         0.0005\n",
      "full_sq                                 0.0001\n",
      "dtype: float64\n",
      "\r\n",
      "Missing Values Count: 36\n"
     ]
    }
   ],
   "source": [
    "# get the missing values in percentage\n",
    "missing_vals_pct = ((mergeData.isna().sum()) / len(mergeData))\n",
    "missing_vals_pct.sort_values(ascending = False, inplace = True)\n",
    "print(missing_vals_pct[missing_vals_pct > 0])\n",
    "print(\"\\r\\nMissing Values Count: \" + str(len(missing_vals_pct[missing_vals_pct > 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Median Imputation for missing values lesser or equal to 30%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['relative_floor', 'max_floor', 'num_room', 'material',\n",
      "       'preschool_quota', 'cafe_sum_1000_min_price_avg', 'cafe_avg_price_1000',\n",
      "       'cafe_sum_1000_max_price_avg', 'non_living_area_ratio',\n",
      "       'non_living_area', 'living_area_ratio', 'life_sq',\n",
      "       'raion_build_count_with_material_info', 'build_count_brick',\n",
      "       'build_count_before_1920', 'raion_build_count_with_builddate_info',\n",
      "       'build_count_monolith', 'build_count_1946-1970', 'cafe_avg_price_1500',\n",
      "       'cafe_sum_1500_min_price_avg', 'cafe_sum_1500_max_price_avg',\n",
      "       'cafe_sum_2000_max_price_avg', 'cafe_sum_2000_min_price_avg',\n",
      "       'cafe_avg_price_2000', 'prom_part_5000', 'floor', 'metro_min_walk',\n",
      "       'railroad_station_walk_km', 'railroad_station_walk_min',\n",
      "       'metro_km_walk', 'product_type', 'green_part_2000', 'full_sq'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(missing_vals_pct[(missing_vals_pct > 0) & (missing_vals_pct <= 0.3000)].index)\n",
    "\n",
    "for feat in missing_vals_pct[(missing_vals_pct > 0) & (missing_vals_pct <= 0.3000)].index:\n",
    "    try:\n",
    "        mergeData[feat].fillna(mergeData[feat].median(), inplace = True)\n",
    "    except:\n",
    "        mergeData[feat].fillna(mergeData[feat].mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hospital_beds_raion   0.4683\n",
      "room_area_avg         0.3907\n",
      "state                 0.3738\n",
      "dtype: float64\n",
      "\n",
      "Missing Values Count: 3\n"
     ]
    }
   ],
   "source": [
    "# check remaining missing values in percentage\n",
    "missing_vals_pct = ((mergeData.isna().sum()) / len(mergeData))\n",
    "missing_vals_pct.sort_values(ascending = False, inplace = True)\n",
    "print(missing_vals_pct[missing_vals_pct > 0])\n",
    "print(\"\\r\\nMissing Values Count: \" + str(len(missing_vals_pct[missing_vals_pct > 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN Imputer for remaining missing values greater than 30%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa4bdfdd9cd4ef2a128f6f128dbbbed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# sklearn KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "\n",
    "missingCol = list(missing_vals_pct[missing_vals_pct > 0].index)\n",
    "\n",
    "for i in tqdm_notebook(missingCol):\n",
    "    mergeData[i] = imputer.fit_transform(mergeData[[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: float64)\n",
      "\r\n",
      "Missing Values Count: 0\n",
      "\r\n",
      "(38133, 251)\n"
     ]
    }
   ],
   "source": [
    "# check for any remaining missing values\n",
    "missing_vals_pct = ((mergeData.isna().sum()) / len(mergeData))\n",
    "missing_vals_pct.sort_values(ascending = False, inplace = True)\n",
    "print(missing_vals_pct[missing_vals_pct > 0])\n",
    "print(\"\\r\\nMissing Values Count: \" + str(len(missing_vals_pct[missing_vals_pct > 0])))\n",
    "print(\"\\r\\n\" + str(mergeData.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Train and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 'trainPrice' dataset from earlier to split 'mergeData' back into train and test datasets\n",
    "xTrain = mergeData[mergeData[\"id\"].isin(trainPrice[\"id\"])]\n",
    "xTrain = pd.merge(xTrain, trainPrice, on = [\"id\"], how = \"inner\")\n",
    "\n",
    "xTest = mergeData[~mergeData[\"id\"].isin(trainPrice[\"id\"])]\n",
    "\n",
    "yTrain = xTrain[\"price_doc\"].apply(lambda j: np.log1p(j))\n",
    "xTrain.drop(columns = [\"id\", \"timestamp\", \"price_doc\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and cross-validation\n",
    "x_tr, x_cv, y_tr, y_cv = train_test_split(xTrain, yTrain, test_size=0.15,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-cd4917d1c0dd>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_cv[cat] = x_cv[cat].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
      "<ipython-input-19-cd4917d1c0dd>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_tr[cat] = le.transform(x_tr[cat])\n",
      "<ipython-input-19-cd4917d1c0dd>:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_cv[cat] = le.transform(x_cv[cat])\n",
      "<ipython-input-19-cd4917d1c0dd>:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_tr[num] = (x_tr[num] - min)/(max-min)\n",
      "<ipython-input-19-cd4917d1c0dd>:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_cv[num] = (x_cv[num] - min)/(max-min)\n"
     ]
    }
   ],
   "source": [
    "# process categorical ('object') and numerical (non 'object') type data using label encoder\n",
    "\n",
    "categoricals = xTrain.select_dtypes(include = [\"object\"]).copy()\n",
    "numericals = xTrain.select_dtypes(exclude = [\"object\"])\n",
    "\n",
    "# categoricals = xTrain.dtypes[(xTrain.dtypes == object)].copy()\n",
    "# numericals = xTrain.dtypes[(xTrain.dtypes != object)].copy()\n",
    "\n",
    "# categoricals\n",
    "for cat in categoricals:\n",
    "  le = preprocessing.LabelEncoder()\n",
    "  le.fit(x_tr[cat])\n",
    "\n",
    "  x_cv[cat] = x_cv[cat].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
    "  le.classes_ = np.append(le.classes_, '<unknown>')\n",
    "\n",
    "  x_tr[cat] = le.transform(x_tr[cat])\n",
    "  x_cv[cat] = le.transform(x_cv[cat])\n",
    "\n",
    "# numericals\n",
    "for num in numericals:\n",
    "  min = x_tr[num].min()\n",
    "  max = x_tr[num].max()\n",
    "  x_tr[num] = (x_tr[num] - min)/(max-min)\n",
    "  x_cv[num] = (x_cv[num] - min)/(max-min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# check any null values\n",
    "print(x_tr.isnull().values.any())\n",
    "print(x_cv.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result array\n",
    "resArr = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (after feature selection): (25900, 27)\n",
      "Test (after feature selection) : (4571, 27)\n"
     ]
    }
   ],
   "source": [
    "# Select best features\n",
    "sel = SelectFromModel(RandomForestRegressor(n_jobs = -1, max_depth = 10))\n",
    "sel.fit(x_tr, y_tr)\n",
    "\n",
    "trainFiltered = sel.transform(x_tr)\n",
    "testFiltered = sel.transform(x_cv)\n",
    "\n",
    "print(\"Train (after feature selection): \" + str(trainFiltered.shape))\n",
    "print(\"Test (after feature selection) : \" + str(testFiltered.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameter distributions to use for tuning\n",
    "paramDist = {\n",
    "    'n_estimators': [50, 100, 150, 200, 250, 300],\n",
    "    'max_depth': [10, 11, 12, 13, 14, 15]\n",
    "}\n",
    "\n",
    "# use RandomizedSearchCV to determine best values for parameters\n",
    "randomSearchModel = RandomizedSearchCV(RandomForestRegressor(), param_distributions = paramDist, verbose = 10, n_jobs = -1, cv = 2, n_iter = 15)\n",
    "randomSearchModel.fit(trainFiltered, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Parameter values: \")\n",
    "print(randomSearchModel.best_params_)\n",
    "print(\"\\r\\nBest Score: \")\n",
    "print(randomSearchModel.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the Model with the best hyperparameters\n",
    "rfrModel = RandomForestRegressor(\n",
    "    n_estimators = randomSearchModel.best_params_['n_estimators'],\n",
    "    max_depth = randomSearchModel.best_params_['max_depth'],\n",
    "    random_state = 42,\n",
    "    n_jobs = -1\n",
    ")\n",
    "rfrModel.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on train data\n",
    "yPred = rfrModel.predict(x_tr)\n",
    "trainMse = mean_squared_error(y_tr, yPred)\n",
    "print(\"Predict Train\")\n",
    "print(\"MSE : %.4f\" % trainMse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict on test data\n",
    "yPred = rfrModel.predict(x_cv)\n",
    "testMse = mean_squared_error(y_cv, yPred)\n",
    "print(\"Predict Test\")\n",
    "print(\"MSE : %.4f\" % testMse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to result array\n",
    "resArr.append([\"Random Forest\", str(\"%.4f\" % trainMse), str(\"%.4f\" % testMse), \"-\", \"-\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Predict on test.csv test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do prep: process.. run pred\n",
    "testRF = xTest.copy()\n",
    "test_id = testRF['id']\n",
    "testRF.drop(['id', 'timestamp'], axis = 1, inplace = True)\n",
    "\n",
    "# process datatype\n",
    "categoricals = xTrain.select_dtypes(include = [\"object\"]).copy()\n",
    "numericals = xTrain.select_dtypes(exclude = [\"object\"])\n",
    "\n",
    "for c in categoricals:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(xTrain[c])\n",
    "\n",
    "    testRF[c] = testRF[c].map(lambda s: \"<unknown>\" if s not in le.classes_ else s)\n",
    "    le.classes_ = np.append(le.classes_, \"<unknown>\")\n",
    "\n",
    "    testRF[c] = le.transform(testRF[c])\n",
    "\n",
    "for n in numericals:\n",
    "    min = xTrain[n].min()\n",
    "    max = xTrain[n].max()\n",
    "\n",
    "    testRF[n] = (testRF[n] - min) / (max - min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using random forest model\n",
    "test_pred = rfrModel.predict(testRF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# populate price_doc column with predicted prices\n",
    "testRF['price_doc'] = np.expm1(test_pred)\n",
    "testRF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check output dir\n",
    "if not os.path.exists(\"./output_models\"):\n",
    "    os.mkdir(\"./output_models\")\n",
    "\n",
    "# write id and predicted price_doc columns to csv\n",
    "testRF['id'] = test_id\n",
    "testRF[['id', 'price_doc']].to_csv('./output_models/output_random_forest.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtModel = DecisionTreeRegressor()\n",
    "\n",
    "# Define 'max_depth' params to use for tuning GridSearchCV\n",
    "paramDepth ={'max_depth' : [5, 10, 11, 12, 13, 14, 15, 20, 25, 30, 35, 40, 45, 50]}\n",
    "\n",
    "gridSearchModel = GridSearchCV(dtModel, param_grid = paramDepth, verbose = 10, cv = 3, n_jobs = -1)\n",
    "gridSearchModel.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Parameter values: \")\n",
    "print(gridSearchModel.best_params_)\n",
    "print(\"\\r\\nBest Score: \")\n",
    "print(gridSearchModel.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the Model with the best hyperparameters\n",
    "dtModel = DecisionTreeRegressor(\n",
    "    max_depth = gridSearchModel.best_params_['max_depth'],\n",
    "    random_state = 42\n",
    ")\n",
    "dtModel.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on train data\n",
    "yPred = dtModel.predict(x_tr)\n",
    "trainMse = mean_squared_error(y_tr, yPred)\n",
    "print(\"Predict Train\")\n",
    "print(\"MSE : %.4f\" % trainMse)\n",
    "print(\"RMSE: %.4f\" % sqrt(trainMse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on cross validation data\n",
    "yPred = dtModel.predict(x_cv)\n",
    "testMse = mean_squared_error(y_cv, yPred)\n",
    "print(\"Predict Cross Validation\")\n",
    "print(\"MSE : %.4f\" % testMse)\n",
    "print(\"RMSE: %.4f\" % sqrt(testMse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to result array\n",
    "resArr.append([\"Decision Tree\", str(\"%.4f\" % trainMse), str(\"%.4f\" % testMse), str(\"%.4f\" % sqrt(trainMse)), str(\"%.4f\" % sqrt(testMse))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test.csv test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same prep: process.. run pred\n",
    "testDT = xTest.copy()\n",
    "testId = testDT[\"id\"]\n",
    "testDT.drop([\"id\", \"timestamp\"], axis = 1, inplace = True)\n",
    "\n",
    "# process datatype\n",
    "categoricals = xTrain.select_dtypes(include = [\"object\"]).copy()\n",
    "numericals = xTrain.select_dtypes(exclude = [\"object\"])\n",
    "\n",
    "for c in categoricals:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(xTrain[c])\n",
    "\n",
    "    testDT[c] = testDT[c].map(lambda s: \"<unknown>\" if s not in le.classes_ else s)\n",
    "    le.classes_ = np.append(le.classes_, \"<unknown>\")\n",
    "\n",
    "    testDT[c] = le.transform(testDT[c])\n",
    "\n",
    "for n in numericals:\n",
    "    min = xTrain[n].min()\n",
    "    max = xTrain[n].max()\n",
    "\n",
    "    testDT[n] = (testDT[n] - min) / (max - min)\n",
    "\n",
    "# predict using decision tree model\n",
    "testPred = dtModel.predict(testDT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output prediction to csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate price_doc column with predicted prices\n",
    "testDT[\"price_doc\"] = np.expm1(testPred)\n",
    "testDT[\"id\"] = testId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check output dir\n",
    "if not os.path.exists(\"./output_models\"):\n",
    "    os.mkdir(\"./output_models\")\n",
    "\n",
    "# write id and predicted price_doc columns to csv\n",
    "testDT[[\"id\", \"price_doc\"]].to_csv(\"./output_models/output_decisiontree.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. XGBoost (eXtreme Gradient Boost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tune Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 15 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   35.7s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  30 | elapsed:  3.1min remaining:   20.3s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=2,\n",
       "                   estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                          colsample_bylevel=None,\n",
       "                                          colsample_bynode=None,\n",
       "                                          colsample_bytree=None,\n",
       "                                          enable_categorical=False, gamma=None,\n",
       "                                          gpu_id=None, importance_type=None,\n",
       "                                          interaction_constraints=None,\n",
       "                                          learning_rate=None,\n",
       "                                          max_delta_step=None, max_depth=None,\n",
       "                                          min_child_weight=None, missing=nan,\n",
       "                                          monotone_constraints=...\n",
       "                                          reg_alpha=None, reg_lambda=None,\n",
       "                                          scale_pos_weight=None, subsample=None,\n",
       "                                          tree_method=None,\n",
       "                                          validate_parameters=None,\n",
       "                                          verbosity=None),\n",
       "                   n_iter=15, n_jobs=-1,\n",
       "                   param_distributions={'colsample_bytree': [0.1, 0.25, 0.3,\n",
       "                                                             0.5, 0.75, 0.8,\n",
       "                                                             1],\n",
       "                                        'learning_rate': [0.01, 0.03, 0.05,\n",
       "                                                          0.07, 0.1, 0.13,\n",
       "                                                          0.15],\n",
       "                                        'max_depth': [10, 11, 12, 13, 14, 15],\n",
       "                                        'n_estimators': [50, 100, 150, 200],\n",
       "                                        'subsample': [0.1, 0.3, 0.5, 0.7, 1]},\n",
       "                   verbose=10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbModel = XGBRegressor(nthread = -1)\n",
    "\n",
    "# define parameter distributions to use for tuning\n",
    "paramDist = {\n",
    "    \"colsample_bytree\": [0.1, 0.25, 0.3, 0.5, 0.75, 0.8, 1],\n",
    "    \"learning_rate\": [0.01, 0.03, 0.05, 0.07, 0.1, 0.13, 0.15],\n",
    "    \"max_depth\": [10, 11, 12, 13, 14, 15],\n",
    "    \"n_estimators\": [50, 100, 150, 200],\n",
    "    \"subsample\": [0.1, 0.3, 0.5, 0.7, 1]\n",
    "}\n",
    "\n",
    "# use RandomizedSearchCV to determine best values for hyperparameters\n",
    "randomSearchModel = RandomizedSearchCV(xgbModel, param_distributions = paramDist, verbose = 10, n_jobs = -1, cv = 2, n_iter = 15)\n",
    "randomSearchModel.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Best Hyperparamter values: \")\n",
    "print(randomSearchModel.best_params_)\n",
    "print(\"\\r\\nBest Score: \")\n",
    "print(randomSearchModel.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use best Hyperparameters in model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tune Model\n",
    "xgbModel = XGBRegressor(\n",
    "    colsample_bytree = randomSearchModel.best_params_[\"colsample_bytree\"],\n",
    "    learning_rate = randomSearchModel.best_params_[\"learning_rate\"],\n",
    "    max_depth = randomSearchModel.best_params_[\"max_depth\"],\n",
    "    n_estimators = randomSearchModel.best_params_[\"n_estimators\"],\n",
    "    subsample = randomSearchModel.best_params_[\"subsample\"]\n",
    ")\n",
    "# fit model\n",
    "xgbModel.fit(x_tr, y_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# predict on train data\n",
    "yPred = xgbModel.predict(x_tr)\n",
    "trainMse = mean_squared_error(y_tr, yPred)\n",
    "print(\"Predict Train\")\n",
    "print(\"MSE : %.4f\" % trainMse)\n",
    "print(\"RMSE: %.4f\" % sqrt(trainMse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# predict on cross validation data\n",
    "yPred = xgbModel.predict(x_cv)\n",
    "testMse = mean_squared_error(y_cv, yPred)\n",
    "print(\"Predict Cross Validation\")\n",
    "print(\"MSE : %.4f\" % testMse)\n",
    "print(\"RMSE: %.4f\" % sqrt(testMse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to result array\n",
    "resArr.append([\"XGBoost\", str(\"%.4f\" % trainMse), str(\"%.4f\" % testMse), str(\"%.4f\" % sqrt(trainMse)), str(\"%.4f\" % sqrt(testMse))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test.csv test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same prep: process.. run pred\n",
    "testXGB = xTest.copy()\n",
    "testId = testXGB[\"id\"]\n",
    "testXGB.drop([\"id\", \"timestamp\"], axis = 1, inplace = True)\n",
    "\n",
    "# process datatype\n",
    "categoricals = xTrain.select_dtypes(include = [\"object\"]).copy()\n",
    "numericals = xTrain.select_dtypes(exclude = [\"object\"])\n",
    "\n",
    "for c in categoricals:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(xTrain[c])\n",
    "\n",
    "    testXGB[c] = testXGB[c].map(lambda s: \"<unknown>\" if s not in le.classes_ else s)\n",
    "    le.classes_ = np.append(le.classes_, \"<unknown>\")\n",
    "\n",
    "    testXGB[c] = le.transform(testXGB[c])\n",
    "\n",
    "for n in numericals:\n",
    "    min = xTrain[n].min()\n",
    "    max = xTrain[n].max()\n",
    "\n",
    "    testXGB[n] = (testXGB[n] - min) / (max - min)\n",
    "\n",
    "# predict using xgb model\n",
    "testPred = xgbModel.predict(testXGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output prediction to csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate price_doc column with predicted prices\n",
    "testXGB[\"price_doc\"] = np.expm1(testPred)\n",
    "testXGB[\"id\"] = testId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check output dir\n",
    "if not os.path.exists(\"./output_models\"):\n",
    "    os.mkdir(\"./output_models\")\n",
    "\n",
    "# write id and predicted price_doc columns to csv\n",
    "testXGB[[\"id\", \"price_doc\"]].to_csv(\"./output_models/output_xgboost.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SGD (Stochastic Gradient Descent) Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tune Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sgdModel = SGDRegressor()\n",
    "\n",
    "# define parameter distributions to use for tuning\n",
    "paramDist = {\n",
    "    \"alpha\": [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4],\n",
    "    \"learning_rate\": ['optimal'],\n",
    "    \"loss\": [\"squared_loss\"],\n",
    "    \"max_iter\": [500, 1000, 1500, 2000],\n",
    "    \"penalty\": [\"l2\"]\n",
    "}\n",
    "\n",
    "# use RandomizedSearchCV to determine best values for hyperparameters\n",
    "randomSearchModel = RandomizedSearchCV(sgdModel, param_distributions = paramDist, verbose = 10, n_jobs = -1, cv=3, n_iter = 15)\n",
    "randomSearchModel.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Best Hyperparamter values: \")\n",
    "print(randomSearchModel.best_params_)\n",
    "print(\"\\r\\nBest Score: \")\n",
    "print(randomSearchModel.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use best Hyperparameters in model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tune Model\n",
    "sgdModel = SGDRegressor(\n",
    "    alpha = randomSearchModel.best_params_[\"alpha\"],\n",
    "    learning_rate = \"optimal\",\n",
    "    loss = \"squared_loss\",\n",
    "    max_iter = randomSearchModel.best_params_[\"max_iter\"],\n",
    "    penalty = \"l2\",\n",
    ")\n",
    "# fit model\n",
    "sgdModel.fit(x_tr, y_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# predict on train data\n",
    "yPred = sgdModel.predict(x_tr)\n",
    "trainMse = mean_squared_error(y_tr, yPred)\n",
    "print(\"Predict Train\")\n",
    "print(\"MSE : %.4f\" % trainMse)\n",
    "print(\"RMSE: %.4f\" % sqrt(trainMse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# predict on cross validation data\n",
    "yPred = sgdModel.predict(x_cv)\n",
    "testMse = mean_squared_error(y_cv, yPred)\n",
    "print(\"Predict Cross Validation\")\n",
    "print(\"MSE : %.4f\" % testMse)\n",
    "print(\"RMSE: %.4f\" % sqrt(testMse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to result array\n",
    "resArr.append([\"SGD\", str(\"%.4f\" % trainMse), str(\"%.4f\" % testMse), str(\"%.4f\" % sqrt(trainMse)), str(\"%.4f\" % sqrt(testMse))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test.csv test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same prep: process.. run pred\n",
    "testSGD = xTest.copy()\n",
    "testId = testSGD[\"id\"]\n",
    "testSGD.drop([\"id\", \"timestamp\"], axis = 1, inplace = True)\n",
    "\n",
    "# process datatype\n",
    "categoricals = xTrain.select_dtypes(include = [\"object\"]).copy()\n",
    "numericals = xTrain.select_dtypes(exclude = [\"object\"])\n",
    "\n",
    "for c in categoricals:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(xTrain[c])\n",
    "\n",
    "    testSGD[c] = testSGD[c].map(lambda s: \"<unknown>\" if s not in le.classes_ else s)\n",
    "    le.classes_ = np.append(le.classes_, \"<unknown>\")\n",
    "\n",
    "    testSGD[c] = le.transform(testSGD[c])\n",
    "\n",
    "for n in numericals:\n",
    "    min = xTrain[n].min()\n",
    "    max = xTrain[n].max()\n",
    "\n",
    "    testSGD[n] = (testSGD[n] - min) / (max - min)\n",
    "\n",
    "# predict using sgd model\n",
    "testPred = sgdModel.predict(testSGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output prediction to csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate price_doc column with predicted prices\n",
    "testSGD[\"price_doc\"] = np.expm1(testPred)\n",
    "testSGD[\"id\"] = testId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check output dir\n",
    "if not os.path.exists(\"./output_models\"):\n",
    "    os.mkdir(\"./output_models\")\n",
    "\n",
    "# write id and predicted price_doc columns to csv\n",
    "testSGD[[\"id\", \"price_doc\"]].to_csv(\"./output_models/output_sgd.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. AdaBoost (Adaptive Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tune 'n_estimators' parameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adbModel = AdaBoostRegressor()\n",
    "\n",
    "# define 'n_estimators' distributions to use for tuning\n",
    "paramDist = { \"n_estimators\": [50, 100, 150, 200, 250, 300] }\n",
    "\n",
    "# use RandomizedSearchCV to determine best value for 'n_estimators'\n",
    "randomSearchModel = RandomizedSearchCV(adbModel, param_distributions = paramDist, verbose = 10, n_jobs = -1, cv = 3)\n",
    "randomSearchModel.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best 'n_estimators' value: \")\n",
    "print(randomSearchModel.best_params_[\"n_estimators\"])\n",
    "print(\"\\r\\nBest Score: \")\n",
    "print(randomSearchModel.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use best 'n_estimators' value in model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune Model\n",
    "adbModel = AdaBoostRegressor(n_estimators = randomSearchModel.best_params_[\"n_estimators\"])\n",
    "# fit model\n",
    "adbModel.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on train data\n",
    "yPred = adbModel.predict(x_tr)\n",
    "trainMse = mean_squared_error(y_tr, yPred)\n",
    "print(\"Predict Train\")\n",
    "print(\"MSE : %.4f\" % trainMse)\n",
    "print(\"RMSE: %.4f\" % sqrt(trainMse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on cross validation data\n",
    "yPred = adbModel.predict(x_cv)\n",
    "testMse = mean_squared_error(y_cv, yPred)\n",
    "print(\"Predict Cross Validation\")\n",
    "print(\"MSE : %.4f\" % testMse)\n",
    "print(\"RMSE: %.4f\" % sqrt(testMse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to result array\n",
    "resArr.append([\"AdaBoost\", str(\"%.4f\" % trainMse), str(\"%.4f\" % testMse), str(\"%.4f\" % sqrt(trainMse)), str(\"%.4f\" % sqrt(testMse))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test.csv test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same prep: process.. run pred\n",
    "testAdB = xTest.copy()\n",
    "testId = testAdB[\"id\"]\n",
    "testAdB.drop([\"id\", \"timestamp\"], axis = 1, inplace = True)\n",
    "\n",
    "# process datatype\n",
    "categoricals = xTrain.select_dtypes(include = [\"object\"]).copy()\n",
    "numericals = xTrain.select_dtypes(exclude = [\"object\"])\n",
    "\n",
    "for c in categoricals:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(xTrain[c])\n",
    "\n",
    "    testAdB[c] = testAdB[c].map(lambda s: \"<unknown>\" if s not in le.classes_ else s)\n",
    "    le.classes_ = np.append(le.classes_, \"<unknown>\")\n",
    "\n",
    "    testAdB[c] = le.transform(testAdB[c])\n",
    "\n",
    "for n in numericals:\n",
    "    min = xTrain[n].min()\n",
    "    max = xTrain[n].max()\n",
    "\n",
    "    testAdB[n] = (testAdB[n] - min) / (max - min)\n",
    "\n",
    "# predict using adb model\n",
    "testPred = adbModel.predict(testAdB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output prediction to csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate price_doc column with predicted prices\n",
    "testAdB[\"price_doc\"] = np.expm1(testPred)\n",
    "testAdB[\"id\"] = testId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check output dir\n",
    "if not os.path.exists(\"./output_models\"):\n",
    "    os.mkdir(\"./output_models\")\n",
    "\n",
    "# write id and predicted price_doc columns to csv\n",
    "testAdB[[\"id\", \"price_doc\"]].to_csv(\"./output_models/output_adaboost.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test naive xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\envs\\cz4041\\lib\\site-packages\\sklearn\\model_selection\\_search.py:278: UserWarning: The total space of parameters 12 is smaller than n_iter=15. Running 12 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n"
     ]
    }
   ],
   "source": [
    "#xgb1 = XGBClassifier( \n",
    "#    learning_rate =0.1, \n",
    "#    n_estimators=70, \n",
    "#    max_depth=5,\n",
    "#    min_child_weight=1,\n",
    "#    gamma=0, \n",
    "#    subsample=0.8, \n",
    "#    colsample_bytree=0.8,\n",
    "#    objective='binary:logistic', \n",
    "#    nthread=4, \n",
    "#    scale_pos_weight=1, \n",
    "#    seed=27)\n",
    "\n",
    "xgb1 = XGBClassifier(nthread=4)\n",
    "\n",
    "# define parameter distributions to use for tuning\n",
    "paramDist = {\n",
    "    \"colsample_bytree\": [0.1, 0.25, 0.3, 0.5, 0.75, 0.8, 1],\n",
    "    \"learning_rate\": [0.01, 0.03, 0.05, 0.07, 0.1, 0.13, 0.15],\n",
    "    \"n_estimators\": [50, 100, 150, 200],\n",
    "    \"subsample\": [0.1, 0.3, 0.5, 0.7, 1],\n",
    "    \"max_depth\": range(3,10,2),\n",
    "    \"min_child_weight\": range(1,6,2)\n",
    "}\n",
    "#    \"n_estimators\": range(70,140,20),\n",
    "#    \"subsample\": [0.4, 0.6, 0.8, 1]\n",
    "# use RandomizedSearchCV to determine best values for hyperparameters\n",
    "randomSearchModel = RandomizedSearchCV(xgb1, param_distributions = paramDist, verbose = 10, n_jobs = -1, cv = 2, n_iter = 15)\n",
    "randomSearchModel.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 20 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   16.0s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   28.0s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done  38 out of  40 | elapsed:  2.3min remaining:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:58:13] WARNING: c:\\ci\\xgboost-split_1638290375667\\work\\src\\objective\\regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=2,\n",
       "                   estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                          colsample_bylevel=None,\n",
       "                                          colsample_bynode=None,\n",
       "                                          colsample_bytree=None,\n",
       "                                          enable_categorical=False, eta=0.05,\n",
       "                                          eval_metric='rmse', gamma=None,\n",
       "                                          gpu_id=None, importance_type=None,\n",
       "                                          interaction_constraints=None,\n",
       "                                          learning_rate=None,\n",
       "                                          max_delta_step=None, max_depth=None,\n",
       "                                          min_child_weight=None, missi...\n",
       "                                          predictor=None, random_state=None,\n",
       "                                          reg_alpha=None, reg_lambda=None,\n",
       "                                          scale_pos_weight=None, subsample=None,\n",
       "                                          tree_method=None, ...),\n",
       "                   n_iter=20, n_jobs=-1,\n",
       "                   param_distributions={'colsample_bytree': [0.8, 0.85, 0.9, 1,\n",
       "                                                             1.2, 1.4],\n",
       "                                        'learning_rate': [0.08, 0.1, 0.12,\n",
       "                                                          0.15],\n",
       "                                        'max_depth': [2, 3, 4, 5, 6],\n",
       "                                        'min_child_weight': [4, 5, 6, 7, 8],\n",
       "                                        'n_estimators': [200, 220, 240, 300],\n",
       "                                        'subsample': [0.8, 0.9, 1, 1.1, 1.2]},\n",
       "                   verbose=10)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbModel = XGBRegressor(eta=0.05, nthread = -1, objective='reg:linear',\n",
    "    eval_metric='rmse')\n",
    "\n",
    "# define parameter distributions to use for tuning\n",
    "paramDist = {\n",
    "    \"colsample_bytree\": [0.8, 0.85, 0.9, 1, 1.2, 1.4],\n",
    "    \"learning_rate\": [0.08, 0.1, 0.12, 0.15],\n",
    "    \"max_depth\": [2, 3, 4, 5, 6],\n",
    "    \"n_estimators\": [200, 220, 240, 300],\n",
    "    \"subsample\": [0.8, 0.9, 1, 1.1, 1.2],\n",
    "    \"min_child_weight\": [4, 5, 6, 7, 8]\n",
    "}\n",
    "\n",
    "# use RandomizedSearchCV to determine best values for hyperparameters\n",
    "randomSearchModel = RandomizedSearchCV(xgbModel, param_distributions = paramDist, verbose = 10, n_jobs = -1, cv = 2, n_iter = 30)\n",
    "randomSearchModel.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n",
      "0.08\n",
      "4\n",
      "220\n",
      "0.9\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#print(randomSearchModel.best_params_[\"eta\"])\n",
    "print(randomSearchModel.best_params_[\"colsample_bytree\"])\n",
    "print(randomSearchModel.best_params_[\"learning_rate\"])\n",
    "print(randomSearchModel.best_params_[\"max_depth\"])\n",
    "print(randomSearchModel.best_params_[\"n_estimators\"])\n",
    "print(randomSearchModel.best_params_[\"subsample\"])\n",
    "print(randomSearchModel.best_params_[\"min_child_weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "#xgb_params = {\n",
    "#    'eta': 0.05,\n",
    "#    'max_depth': 5,\n",
    "#    'subsample': 0.7,\n",
    "#    'colsample_bytree': 0.7,\n",
    "#    'objective': 'reg:linear',\n",
    "#    'eval_metric': 'rmse',\n",
    "#    'silent': 1\n",
    "#}\n",
    "\n",
    "xgb_params = {\n",
    "    'eta': 0.05,\n",
    "    'learning_rate': randomSearchModel.best_params_[\"learning_rate\"],\n",
    "    'max_depth': randomSearchModel.best_params_[\"max_depth\"],\n",
    "    'min_child_weight': randomSearchModel.best_params_[\"min_child_weight\"],\n",
    "    'subsample': randomSearchModel.best_params_[\"subsample\"],\n",
    "    'n_estimators': randomSearchModel.best_params_[\"n_estimators\"],\n",
    "    'colsample_bytree': randomSearchModel.best_params_[\"colsample_bytree\"],\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "\n",
    "xtrainnxgb = dfs[\"train\"].drop([\"id\", \"timestamp\", \"price_doc\"], axis=1)\n",
    "ytrainnxgb = dfs['train']['price_doc'].copy()\n",
    "testnxgb = dfs['test'].drop([\"id\", \"timestamp\"], axis=1)\n",
    "#testId = testnxgb[\"id\"]\n",
    "#testnxgb.drop([\"id\", \"timestamp\"], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "for c in xtrainnxgb.columns:\n",
    "    if xtrainnxgb[c].dtype == 'object':\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(xtrainnxgb[c].values)) \n",
    "        xtrainnxgb[c] = lbl.transform(list(xtrainnxgb[c].values))\n",
    "        #x_train.drop(c,axis=1,inplace=True)\n",
    "        \n",
    "for c in testnxgb.columns:\n",
    "    if testnxgb[c].dtype == 'object':\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(testnxgb[c].values)) \n",
    "        testnxgb[c] = lbl.transform(list(testnxgb[c].values))\n",
    "        #x_test.drop(c,axis=1,inplace=True)        \n",
    "\n",
    "\n",
    "# # process datatype\n",
    "# categoricals = xTrain.select_dtypes(include = [\"object\"]).copy()\n",
    "# numericals = xTrain.select_dtypes(exclude = [\"object\"])\n",
    "\n",
    "# for c in categoricals:\n",
    "#     le = preprocessing.LabelEncoder()\n",
    "#     le.fit(xTrain[c])\n",
    "\n",
    "#     testnxgb[c] = testnxgb[c].map(lambda s: \"<unknown>\" if s not in le.classes_ else s)\n",
    "#     le.classes_ = np.append(le.classes_, \"<unknown>\")\n",
    "\n",
    "#     testnxgb[c] = le.transform(testnxgb[c])\n",
    "\n",
    "# for n in numericals:\n",
    "#     min = xTrain[n].min()\n",
    "#     max = xTrain[n].max()\n",
    "\n",
    "#     testnxgb[n] = (testnxgb[n] - min) / (max - min)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dtrain = xgb.DMatrix(xtrainnxgb, ytrainnxgb)\n",
    "dtest = xgb.DMatrix(testnxgb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:59:27] WARNING: c:\\ci\\xgboost-split_1638290375667\\work\\src\\objective\\regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:59:27] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"n_estimators\", \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[23:59:27] WARNING: c:\\ci\\xgboost-split_1638290375667\\work\\src\\objective\\regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:59:27] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"n_estimators\", \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[23:59:27] WARNING: c:\\ci\\xgboost-split_1638290375667\\work\\src\\objective\\regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:59:27] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"n_estimators\", \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:7989052.33333\ttest-rmse:7993758.83333\n",
      "[50]\ttrain-rmse:2465392.08333\ttest-rmse:2802676.50000\n",
      "[100]\ttrain-rmse:2266506.16667\ttest-rmse:2706651.50000\n",
      "[150]\ttrain-rmse:2164141.16667\ttest-rmse:2680222.75000\n",
      "[200]\ttrain-rmse:2083102.66667\ttest-rmse:2668209.08333\n",
      "[250]\ttrain-rmse:2013847.87500\ttest-rmse:2659209.25000\n",
      "[300]\ttrain-rmse:1956670.50000\ttest-rmse:2654175.08333\n",
      "[350]\ttrain-rmse:1903573.04167\ttest-rmse:2650343.08333\n",
      "[374]\ttrain-rmse:1878252.12500\ttest-rmse:2651769.08333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv_output = xgb.cv(xgb_params, dtrain, num_boost_round=1000, early_stopping_rounds=20,\n",
    "    verbose_eval=50, show_stdv=False)\n",
    "cv_output[['train-rmse-mean', 'test-rmse-mean']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00:38] WARNING: c:\\ci\\xgboost-split_1638290375667\\work\\src\\objective\\regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:00:38] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"n_estimators\", \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_boost_rounds = len(cv_output)\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round= num_boost_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(dtest)\n",
    "output = pd.DataFrame({'id': testId, 'price_doc': y_predict})\n",
    "output.to_csv(\"./output_models/naivexgb_6.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30471</th>\n",
       "      <td>30474</td>\n",
       "      <td>5,738,062.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30472</th>\n",
       "      <td>30475</td>\n",
       "      <td>8,185,186.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30473</th>\n",
       "      <td>30476</td>\n",
       "      <td>5,421,624.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30474</th>\n",
       "      <td>30477</td>\n",
       "      <td>6,189,545.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30475</th>\n",
       "      <td>30478</td>\n",
       "      <td>5,101,315.5000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id      price_doc\n",
       "30471  30474 5,738,062.0000\n",
       "30472  30475 8,185,186.0000\n",
       "30473  30476 5,421,624.0000\n",
       "30474  30477 6,189,545.0000\n",
       "30475  30478 5,101,315.5000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display Result Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resdf = pd.DataFrame(resArr, columns = [\"Model\", \"Train MSE\", \"Test MSE\", \"Train RMSE\", \"Test RMSE\"])\n",
    "display(HTML(resdf.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SGD Regressor has an appalling performance with extremely high MSE / RMSE scores**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying a different approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Split train data into 80-20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and cross-validation\n",
    "x_tr, x_cv, y_tr, y_cv = train_test_split(xTrain, yTrain, test_size=0.15,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect code categorical variables:\n",
    "\n",
    "num = xTrain.select_dtypes(exclude=['object'])\n",
    "cat = xTrain.select_dtypes(include=['object']).copy()\n",
    "\n",
    "\n",
    "for c in cat:\n",
    "  labelEnc = preprocessing.LabelEncoder()\n",
    "  labelEnc.fit(x_tr[c])\n",
    "\n",
    "  x_cv[c] = x_cv[c].map(lambda s: '<unknown>' if s not in labelEnc.classes_ else s)\n",
    "  labelEnc.classes_ = np.append(labelEnc.classes_, '<unknown>')\n",
    "\n",
    "  x_tr[c] = labelEnc.transform(x_tr[c])\n",
    "  x_cv[c] = labelEnc.transform(x_cv[c])\n",
    "\n",
    "for c in num:\n",
    "  min = x_tr[c].min()\n",
    "  max = x_tr[c].max()\n",
    "\n",
    "  x_tr[c] = (x_tr[c] - min)/(max-min)\n",
    "  x_cv[c] = (x_cv[c] - min)/(max-min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: In train set, split the train set into A1 and A2 50-50 split\n",
    "## Then we use A1 to do sampling with replacement to create a1,a2....ak(k samples).\n",
    "## Now we create 'k' models, and train each of our models with these k samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1, A2, y_a1, y_a2 = train_test_split(x_tr, y_tr, test_size=0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = A1.to_numpy()\n",
    "A2 = A2.to_numpy()\n",
    "\n",
    "y_a1 = y_a1.to_numpy()\n",
    "y_a2 = y_a2.to_numpy()\n",
    "\n",
    "print(A1.shape, A2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generating_samples(input_, target_):\n",
    "     \n",
    "    s_rows = random.sample(range(len(input_)),int(len(input_)*0.60))  \n",
    "    sample_data = list()\n",
    "    target_data = list()\n",
    "    \n",
    "    for i in s_rows:\n",
    "      sample_data.append(input_[i])\n",
    "        \n",
    "    target_data = target_[s_rows].tolist()\n",
    "    \n",
    "    \n",
    "    \n",
    "    return sample_data, target_data, s_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 200 samples using generating_samples function\n",
    "input_data =[]\n",
    "output_data =[]\n",
    "selected_rows = []\n",
    "\n",
    "f = 200\n",
    "\n",
    "for i in tqdm_notebook(range(f)):\n",
    "    a,b,c = generating_samples(A1, y_a1)\n",
    "    input_data.append(a)\n",
    "    output_data.append(b)\n",
    "    selected_rows.append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all_models = list()\n",
    "\n",
    "for i in tqdm_notebook(range(len(input_data))):\n",
    "    \n",
    "    k = np.array(input_data[i])\n",
    "    j = np.array(output_data[i])\n",
    "      \n",
    "    model = DecisionTreeRegressor(max_depth = None)\n",
    "    model.fit(k,j)\n",
    "    \n",
    "    list_all_models.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: We pass the A2 set to each k models, then we will get k predictions for A2 for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_predictions = list()\n",
    "\n",
    "for i in list_all_models:\n",
    "  k_predictions.append(i.predict(A2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_predictions = np.array(k_predictions)\n",
    "k_predictions = k_predictions.T\n",
    "k_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: We then use the k predictions to create a new dataset, and for A2, since we know it's target values, we can use the k predictions to train a meta model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Linear Regression Model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(k_predictions, y_a2)\n",
    "\n",
    "print(\"MSE for train is: \", mean_squared_error(reg.predict(k_predictions), y_a2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: For the model evaluation, we use the 20% data that we kept as the test set. \n",
    "## Pass that test set to each base model and we will get 'k' predictions. \n",
    "## We then create a new dataset using these k predictions and pass it to our meta model to get the final prediction. We can calculate the models performance score using this final prediction as well as the targets for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the Model\n",
    "test_pred = list()\n",
    "\n",
    "for model in list_all_models:\n",
    "  test_pred.append(model.predict(x_cv))\n",
    "\n",
    "test_pred = np.array(test_pred)\n",
    "test_pred = test_pred.T\n",
    "\n",
    "print(\"Model Performance score for test data is: \", reg.score(test_pred, y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Sqaured Error for test data is:\n",
    "\n",
    "mse = mean_squared_error(reg.predict(test_pred), y_cv)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the house prices for test.csv file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = xTest.copy()\n",
    "test_id = x_test['id']\n",
    "x_test.drop(['id','timestamp'],axis=1,inplace=True)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the House prices\n",
    "# Preparing test data:\n",
    "\n",
    "num = xTrain.select_dtypes(exclude=['object'])\n",
    "cat = xTrain.select_dtypes(include=['object']).copy()\n",
    "\n",
    "\n",
    "for c in cat:\n",
    "  labelEnc = preprocessing.LabelEncoder()\n",
    "  labelEnc.fit(xTrain[c])\n",
    "\n",
    "  x_test[c] = x_test[c].map(lambda s: '<unknown>' if s not in labelEnc.classes_ else s)\n",
    "  labelEnc.classes_ = np.append(labelEnc.classes_, '<unknown>')\n",
    "\n",
    "  x_test[c] = labelEnc.transform(x_test[c])\n",
    "\n",
    "for c in num:\n",
    "  min = xTrain[c].min()\n",
    "  max = xTrain[c].max()\n",
    "\n",
    "  x_test[c] = (x_test[c] - min)/(max-min)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the House prices \n",
    "# Testing the Model\n",
    "test_pred = list()\n",
    "\n",
    "for model in list_all_models:\n",
    "  test_pred.append(model.predict(x_test))\n",
    "\n",
    "test_pred = np.array(test_pred)\n",
    "test_pred = test_pred.T\n",
    "pred = reg.predict(test_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test['price_doc'] = np.expm1(pred)\n",
    "x_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the test data with predicted price into the csv file\n",
    "x_test['id'] = test_id\n",
    "x_test[['id','price_doc']].to_csv('output_file.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "1d649c4a-48b1-4ddf-9c6c-61154fc3eb69",
    "3fc7bc7b-a329-4d0a-8378-ecd214a94a5c",
    "f43233fd-d92c-45c3-a983-6760d17f8af2",
    "3db5652b-0e4c-4483-9ae4-c12cfc02cad9",
    "eacd3859-1644-49e7-b823-ecaa1a1e1309",
    "9fd74c3a-7d03-4ef1-88f5-512543ea2743",
    "ad7d8de9-ce0e-49ff-9182-d43884be8de9",
    "76333052-72ea-49d9-9bae-dfc14cb16a97",
    "b2c02a57-93c2-48a1-811f-a0a098007051",
    "cb448266-8c1d-43f4-a546-cd1da680cc5d",
    "e7611d91-b2bb-463f-afec-d61387c96aa9",
    "f6467dc8-edbf-42f4-a5da-2b943280d570",
    "a249ef17-e4c0-4dc6-9c37-d99bddfe4397",
    "dc7fd1ae-6b64-477d-a07d-2c39691ee4c9",
    "dfcbad98-798b-4726-9945-65f0621e9a6c",
    "13a68d3a-924b-418f-aa40-7e2d781d7cd4",
    "12015471-eacf-408c-823a-99a86a89a3f5",
    "81987137-ad1d-4c38-b4fd-5afa0d179f45",
    "5a556eaa-ce64-42cd-bc3b-012dd80e6abb"
   ],
   "name": "ExploratoryDataAnalysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
